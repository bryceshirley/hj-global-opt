{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVLjGnWsR7bq"
   },
   "source": [
    "## Understanding the Geometry of MAD with Interative Plot Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moreau_envelope_visualizer import HJMoreauAdaptiveDescentVisualizer\n",
    "\n",
    "visualizer = HJMoreauAdaptiveDescentVisualizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_time_evolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QuadraticOptimization:\n",
    "    def __init__(self, beta, alpha, max_iterations):\n",
    "        # Conditions from Visualizer\n",
    "        coeffs = visualizer.fit_quadratic_to_moreau_envelope()\n",
    "        self.a, self.b, self.c = coeffs\n",
    "        x_0 = visualizer.x_0_input.value\n",
    "        self.T  = visualizer.fixed_T_input.value\n",
    "        self.x_vals = visualizer.x_values\n",
    "        self.u_vals = visualizer.u_values\n",
    "\n",
    "        # Adjustable parameters\n",
    "        self.alpha = alpha\n",
    "        self.max_iterations = max_iterations\n",
    "        self.beta = beta\n",
    "\n",
    "        # Variables for GD and Accelerated GD\n",
    "        self.k = 0  # iteration counter\n",
    "        self.x_k = x_0  # for gradient descent\n",
    "        self.acc_x_k = x_0  # for accelerated gradient descent\n",
    "        self.acc_x_k_minus_1 = x_0  # previous value for NAG\n",
    "\n",
    "        # Lists to store function values and errors\n",
    "        self.gd_errors = []\n",
    "        self.nag_errors = []\n",
    "        self.gd_values = []\n",
    "        self.nag_values = []\n",
    "        self.gd_points = []  # x values for gradient descent path\n",
    "        self.nag_points = []  # x values for NAG path\n",
    "        self.gk_hist = [(self.x_k,self.gradient(self.x_k))]\n",
    "\n",
    "    def quadratic_function(self, x):\n",
    "        \"\"\"Quadratic function f(x) = ax^2 + bx + c.\"\"\"\n",
    "        return self.a * x ** 2 + self.b * x + self.c\n",
    "\n",
    "    def gradient(self, x):\n",
    "        \"\"\"Gradient of the quadratic function: f'(x) = 2ax + b.\"\"\"\n",
    "        return 2 * self.a * x + self.b\n",
    "\n",
    "    def gradient_descent(self, x):\n",
    "        \"\"\"Performs one step of gradient descent.\"\"\"\n",
    "        grad = self.gradient(x)\n",
    "        return x - self.alpha * self.T * grad\n",
    "\n",
    "    def run_optimization(self):\n",
    "        for _ in range(self.max_iterations):\n",
    "            # Perform standard Gradient Descent\n",
    "            x_k_plus_1 = self.gradient_descent(self.x_k)\n",
    "            self.gd_points.append(x_k_plus_1)\n",
    "\n",
    "            # Store function value and error for GD\n",
    "            gd_value = self.quadratic_function(x_k_plus_1)\n",
    "            self.gd_values.append(gd_value)\n",
    "            self.gd_errors.append(np.abs(x_k_plus_1 - self.x_k))\n",
    "            \n",
    "            # Update x_k for next iteration\n",
    "            self.x_k = x_k_plus_1\n",
    "\n",
    "            # Perform Accelerated Gradient Descent (NAG)\n",
    "            if self.k > 1:\n",
    "                y_k = self.acc_x_k + self.beta * (self.acc_x_k - self.acc_x_k_minus_1)\n",
    "                acc_x_k_plus_1 = self.gradient_descent(y_k)\n",
    "            else:\n",
    "                acc_x_k_plus_1 = x_k_plus_1\n",
    "\n",
    "            self.nag_points.append(acc_x_k_plus_1)\n",
    "\n",
    "            # Store function value and error for NAG\n",
    "            nag_value = self.quadratic_function(acc_x_k_plus_1)\n",
    "            self.nag_values.append(nag_value)\n",
    "            self.nag_errors.append(np.abs(acc_x_k_plus_1 - self.acc_x_k))\n",
    "\n",
    "            # Update accelerated variables for next iteration\n",
    "            self.acc_x_k_minus_1 = self.acc_x_k\n",
    "            self.acc_x_k = acc_x_k_plus_1\n",
    "\n",
    "            # Update iteration counter\n",
    "            self.gk_hist.append((self.x_k,self.gradient(self.x_k)))\n",
    "            self.k += 1\n",
    "\n",
    "    def plot_results(self):\n",
    "        \"\"\"Plot error convergence, function value, and the quadratic function.\"\"\"\n",
    "        iterations = np.arange(1, self.max_iterations + 1)\n",
    "\n",
    "        # Plot error convergence using semilogy\n",
    "        plt.figure(figsize=(16, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.semilogy(iterations, self.gd_errors, label='Gradient Descent', marker='o')\n",
    "        plt.semilogy(iterations, self.nag_errors, label='Accelerated GD (NAG)', marker='x')\n",
    "        plt.title('Error Convergence (Semilogy)')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Error (log scale)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot function value using semilogy\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(iterations, self.gd_values, label='Gradient Descent', marker='o')\n",
    "        plt.plot(iterations, self.nag_values, label='Accelerated GD (NAG)', marker='x')\n",
    "        plt.title('Function Value Convergence (Semilogy)')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Function Value (log scale)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot the quadratic function and the optimization points\n",
    "        y_vals = self.quadratic_function(self.x_vals)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.x_vals, y_vals, label='Quadratic Least Squares Function', color='blue')\n",
    "        plt.plot(self.x_vals, self.u_vals, label='Moreau Envelope', color='green')\n",
    "        plt.scatter(self.gd_points, [self.quadratic_function(x) for x in self.gd_points], color='black', label='GD Path', zorder=5)\n",
    "        plt.scatter(self.nag_points, [self.quadratic_function(x) for x in self.nag_points], color='red', label='NAG Path', zorder=5)\n",
    "        plt.title('Quadratic Function and Optimization Paths')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('f(x)')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage with a quadratic function f(x) = 2x^2 - 3x + 1\n",
    "if __name__ == \"__main__\":\n",
    "    alpha = 0.11\n",
    "    beta = 0.5\n",
    "    max_iterations = 100\n",
    "\n",
    "    optimizer = QuadraticOptimization(beta, alpha, max_iterations)\n",
    "    optimizer.run_optimization()\n",
    "    optimizer.plot_results()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
