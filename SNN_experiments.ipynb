{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from hj_mad_ls import HJ_MD_LS\n",
    "\n",
    "from hj_mad_cd import HJ_MD_CD\n",
    "\n",
    "# Hyperparameters\n",
    "n_neurons = 10  # Number of neurons in the shallow network\n",
    "num_epochs = 10\n",
    "batch_size = 4\n",
    "\n",
    "# Generate data\n",
    "no_of_samples = 500\n",
    "noise_level = 1e-3\n",
    "x = np.linspace(0, 2 * np.pi, no_of_samples)\n",
    "y = np.sin(x) + noise_level * np.random.randn(*x.shape)  # Add noise to the samples\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the shallow neural network\n",
    "class ShallowNet(nn.Module):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(ShallowNet, self).__init__()\n",
    "        self.linear = nn.Linear(1, n_neurons)\n",
    "        self.coeffs = nn.Parameter(torch.randn(n_neurons, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)      # Compute w_j * x + b_j\n",
    "        x = self.relu(x)        # Apply ReLU activation\n",
    "        x = x @ self.coeffs     # Compute sum of c_j * ReLU(...)\n",
    "        return x\n",
    "\n",
    "    def set_parameters(self, new_parameters):\n",
    "        \"\"\"\n",
    "        Set model parameters from the flattened parameter vector.\n",
    "        This method updates the model's parameters from a single flattened tensor.\n",
    "\n",
    "        Args:\n",
    "            new_parameters: A flattened tensor containing all parameters.\n",
    "        \"\"\"\n",
    "        offset = 0\n",
    "        for param in self.parameters():\n",
    "            param_size = param.numel()  # Total number of elements in the parameter\n",
    "            # Extract the relevant slice and reshape it to the parameter's original shape\n",
    "            if len(new_parameters.shape) == 1:\n",
    "                new_param_values = new_parameters[offset:offset + param_size]\n",
    "            else:\n",
    "                new_param_values = new_parameters[0, offset:offset + param_size]  # Access batch dimension\n",
    "            new_param_values = new_param_values.view_as(param)  # Match the original shape\n",
    "            param.data.copy_(new_param_values)  # Update parameter\n",
    "            offset += param_size\n",
    "\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the function f (e.g., a loss function based on the model's predictions)\n",
    "def loss_function(x, model, inputs, targets, plot=False):\n",
    "    \"\"\"\n",
    "    Computes the loss based on the given model parameters.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape (n_samples, n_features) representing flattened parameters.\n",
    "        model: The neural network model.\n",
    "        inputs: Input batch.\n",
    "        targets: Ground truth values.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (n_samples,) containing the loss for each sample.\n",
    "    \"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    \n",
    "    # Initialize list to store losses\n",
    "    sample_losses = []\n",
    "\n",
    "    # Compute losses for each sample in the batch\n",
    "    for i in range(n_samples):\n",
    "        # Extract parameters for the current sample\n",
    "        model_params = x[i, :]  # Take the i-th sample\n",
    "        model.set_parameters(model_params)  # Update model parameters\n",
    "\n",
    "        # Forward pass and loss computation\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        sample_losses.append(loss.item())\n",
    "        \n",
    "    # Convert the list of losses to a tensor\n",
    "    return torch.tensor(sample_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = ShallowNet(n_neurons)\n",
    "\n",
    "# # HJ_MAD hyperparameters\n",
    "# delta = 5e-11#4.87e-11          # Small delta -> Better Approximation of the Moreau Envelope Provided Large T or Close to Global Minimum\n",
    "# t =   5e+7 #3.69            # Large T -> Better Exploration (Moreau Envelope closer to approximate quadratic)\n",
    "# int_samples = int(500)  # Large N -> Better Expectations for Larger T -> Better Exploration (But more expensive)\n",
    "# max_iters = int(2)   # Maximum number of iterations\n",
    "# loss_tol = 9e-5         # Stopping criterion for the loss\n",
    "# sat_tol = 1e-3          # Stopping criterion for the saturation\n",
    "# beta=0.0                # Set beta to 0.0 to turn off \n",
    "# momentum=0.0            # Set momentum to 0.0 to turn off\n",
    "# distribution=\"Gaussian\" # Set sampling distribution \n",
    "\n",
    "# # Use HJ_MAD to optimize the model parameters\n",
    "# HJ_MAD_alg = HJ_MD_LS(delta=delta, t=t,distribution=distribution,momentum=momentum,\n",
    "#                       int_samples=int_samples, max_iters=max_iters, f_tol=loss_tol,\n",
    "#                       verbose=False, adaptive_time=False,adaptive_delta=True,\n",
    "#                       line_search=False)\n",
    "\n",
    "\n",
    "# # Training loop: Iterate through the epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Iterate through the dataset in batches\n",
    "#     for inputs, targets in dataloader:\n",
    "#         # Define f\n",
    "#         def f(x):\n",
    "#             return loss_function(x, model, inputs, targets)\n",
    "        \n",
    "#         # Get the model parameters as a flattened tensor\n",
    "#         parameters = list(model.parameters())\n",
    "#         parameter_vector = torch.cat([param.flatten() for param in parameters])\n",
    "#         parameter_vector = parameter_vector.unsqueeze(0) # shape (1, num_features)\n",
    "\n",
    "#         # Compute Current Loss\n",
    "#         loss_old = f(parameter_vector)  # Compute the loss for the current parameters\n",
    "\n",
    "#         # Run HJ_MAD to optimize the model parameters\n",
    "#         new_parameters, loss, loss_history, delta_hist, tk_hist, iterations = HJ_MAD_alg.run(f, parameter_vector)  # Run HJ_MAD to optimize the model parameters\n",
    "\n",
    "#         HJ_MAD_alg.delta = delta_hist[-1]  # Update delta for the next iteration\n",
    "#         HJ_MAD_alg.t = tk_hist[-1]\n",
    "\n",
    "#         # Update the model parameters with the optimized values\n",
    "#         model.set_parameters(new_parameters)\n",
    "\n",
    "#         # Print the loss before and after optimization\n",
    "#         print(f'Loss before: {loss_old.item():.4e} | Loss after: {loss.item():.4e} | Delta: {delta_hist[-1]}')\n",
    "        \n",
    "#     # Print the loss every 100 epochs for monitoring\n",
    "#     if (epoch + 1) % 100 == 0:\n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# # Visualise the learned function\n",
    "# plt.figure()\n",
    "# plt.plot(x, y, label='True function')\n",
    "# plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define f\n",
    "# def f(x):\n",
    "#     return loss_function(x, model, x_tensor, y_tensor)\n",
    "\n",
    "# def plot(k, xk, fk, deltak, tk, ax):\n",
    "#     model.set_parameters(xk)\n",
    "#     ax.clear()  # Clear previous plot to avoid overlap\n",
    "#     ax.plot(x, y_tensor, label='True function')  # Assuming `x` and `y_tensor` are available\n",
    "#     ax.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')  # Assuming model and x_tensor are available\n",
    "#     ax.legend()\n",
    "#     ax.set_title(f\"Iteration: {k}, Loss: {fk.item():.4e},\\nDelta: {deltak:.2e}, T: {tk:.2e}\")\n",
    "\n",
    "\n",
    "# model = ShallowNet(n_neurons)\n",
    "\n",
    "# # HJ_MAD hyperparameters\n",
    "# delta = 1e-10#5e-11 #1e-12         # Small delta -> Better Approximation of the Moreau Envelope Provided Large T or Close to Global Minimum\n",
    "# #delta = 0.1\n",
    "# #t = 1\n",
    "# t =   5e+7#5e7            # Large T -> Better Exploration (Moreau Envelope closer to approximate quadratic)\n",
    "# int_samples = int(50)#500  # Large N -> Better Expectations for Larger T -> Better Exploration (But more expensive)\n",
    "# max_iters = int(100000)   # Maximum number of iterations\n",
    "# loss_tol = 5e-5         # Stopping criterion for the loss\n",
    "# distribution=\"Gaussian\" # Set sampling distribution \n",
    "\n",
    "# # Get the model parameters as a flattened tensor\n",
    "# parameters = list(model.parameters())\n",
    "# parameter_vector = torch.cat([param.flatten() for param in parameters])\n",
    "# parameter_vector = parameter_vector.unsqueeze(0) # shape (1, num_features)\n",
    "\n",
    "# # Compute Current Loss\n",
    "# loss_old = f(parameter_vector)  # Compute the loss for the current parameters\n",
    "\n",
    "# # Use HJ_MAD to optimize the model parameters\n",
    "# HJ_MAD_alg = HJ_MD_CD(f=f, delta=delta, t=t,distribution=distribution,\n",
    "#                       int_samples=int_samples, max_iters=max_iters, f_tol=loss_tol,\n",
    "#                       verbose=True, adaptive_time=True,adaptive_delta=True,\n",
    "#                       line_search=False, stepsize=1.0)\n",
    "\n",
    "# # Run HJ_MAD to optimize the model parameters\n",
    "# start_time = time.time()\n",
    "# new_parameters, loss, xk_hist, loss_history, delta_hist, tk_hist, iterations = HJ_MAD_alg.run(parameter_vector)  # Run HJ_MAD to optimize the model parameters\n",
    "# elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "\n",
    "# func_evals = 2*iterations * (int_samples)\n",
    "# print(\"\\n=== Optimization Results ===\")\n",
    "# print(f\"Function Evaluations: {func_evals}\")\n",
    "# print(f\"Iterations          : {iterations} (Max: {max_iters})\")\n",
    "# print(f\"Elapsed Time        : {elapsed_time:.2f} seconds\")\n",
    "# print(f\"Final Loss          : {f(new_parameters).item():.5e}\")\n",
    "# print(\"============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- RUNNING HJ-MAD-LS Algorithm ---------------------------\n",
      "dimension =  30 n_samples =  1000\n",
      "[  0]: fk = 1.61e+01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 4.32618e-01\n",
      "[  1]: fk = 4.33e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 4.11296e-01\n",
      "[  2]: fk = 4.11e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 4.10510e-01\n",
      "[  3]: fk = 4.11e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.19043e-01\n",
      "[  4]: fk = 2.19e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.12080e-01\n",
      "[  5]: fk = 2.12e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.12075e-01\n",
      "[  6]: fk = 2.12e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.09167e-01\n",
      "[  7]: fk = 2.09e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.05294e-01\n",
      "[  8]: fk = 2.05e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.04912e-01\n",
      "[  9]: fk = 2.05e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.04337e-01\n",
      "[ 10]: fk = 2.04e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.03599e-01\n",
      "[ 11]: fk = 2.04e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.03575e-01\n",
      "[ 12]: fk = 2.04e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.03567e-01\n",
      "[ 13]: fk = 2.04e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.03561e-01\n",
      "[ 14]: fk = 2.04e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.03489e-01\n",
      "[ 15]: fk = 2.03e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.03431e-01\n",
      "[ 16]: fk = 2.03e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.03430e-01\n",
      "[ 17]: fk = 2.03e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.02258e-01\n",
      "[ 18]: fk = 2.02e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.02218e-01\n",
      "[ 19]: fk = 2.02e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.02171e-01\n",
      "[ 20]: fk = 2.02e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.02111e-01\n",
      "[ 21]: fk = 2.02e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 2.00666e-01\n",
      "[ 22]: fk = 2.01e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.98503e-01\n",
      "[ 23]: fk = 1.99e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.97935e-01\n",
      "[ 24]: fk = 1.98e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.97761e-01\n",
      "[ 25]: fk = 1.98e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.97257e-01\n",
      "[ 26]: fk = 1.97e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.96879e-01\n",
      "[ 27]: fk = 1.97e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.96879e-01\n",
      "[ 28]: fk = 1.97e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.96878e-01\n",
      "[ 29]: fk = 1.97e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.96873e-01\n",
      "[ 30]: fk = 1.97e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.96758e-01\n",
      "[ 31]: fk = 1.97e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.87156e-01\n",
      "[ 32]: fk = 1.87e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.86254e-01\n",
      "[ 33]: fk = 1.86e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.84276e-01\n",
      "[ 34]: fk = 1.84e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.83960e-01\n",
      "[ 35]: fk = 1.84e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.83395e-01\n",
      "[ 36]: fk = 1.83e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.83303e-01\n",
      "[ 37]: fk = 1.83e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.83287e-01\n",
      "[ 38]: fk = 1.83e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.82819e-01\n",
      "[ 39]: fk = 1.83e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.82594e-01\n",
      "[ 40]: fk = 1.83e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.82578e-01\n",
      "[ 41]: fk = 1.83e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.82560e-01\n",
      "[ 42]: fk = 1.83e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.82559e-01\n",
      "[ 43]: fk = 1.83e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.82480e-01\n",
      "[ 44]: fk = 1.82e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.82409e-01\n",
      "[ 45]: fk = 1.82e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.82402e-01\n",
      "[ 46]: fk = 1.82e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.71782e-01\n",
      "[ 47]: fk = 1.72e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.70812e-01\n",
      "[ 48]: fk = 1.71e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.70411e-01\n",
      "[ 49]: fk = 1.70e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.70295e-01\n",
      "[ 50]: fk = 1.70e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.68607e-01\n",
      "[ 51]: fk = 1.69e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.68124e-01\n",
      "[ 52]: fk = 1.68e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.68124e-01\n",
      "[ 53]: fk = 1.68e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.68119e-01\n",
      "[ 54]: fk = 1.68e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.68115e-01\n",
      "[ 55]: fk = 1.68e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.68082e-01\n",
      "[ 56]: fk = 1.68e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.68041e-01\n",
      "[ 57]: fk = 1.68e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.68028e-01\n",
      "[ 58]: fk = 1.68e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.67688e-01\n",
      "[ 59]: fk = 1.68e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.67371e-01\n",
      "[ 60]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.67210e-01\n",
      "[ 61]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.67117e-01\n",
      "[ 62]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.67052e-01\n",
      "[ 63]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.67049e-01\n",
      "[ 64]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.67049e-01\n",
      "[ 65]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.67026e-01\n",
      "[ 66]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.66999e-01\n",
      "[ 67]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.66999e-01\n",
      "[ 68]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.66853e-01\n",
      "[ 69]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.66604e-01\n",
      "[ 70]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.66604e-01\n",
      "[ 71]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.66592e-01\n",
      "[ 72]: fk = 1.67e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.65823e-01\n",
      "[ 73]: fk = 1.66e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.65148e-01\n",
      "[ 74]: fk = 1.65e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.65122e-01\n",
      "[ 75]: fk = 1.65e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.65107e-01\n",
      "[ 76]: fk = 1.65e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.21393e-01\n",
      "[ 77]: fk = 1.21e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.09439e-01\n",
      "[ 78]: fk = 1.09e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.06901e-01\n",
      "[ 79]: fk = 1.07e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.03614e-01\n",
      "[ 80]: fk = 1.04e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.02636e-01\n",
      "[ 81]: fk = 1.03e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.00344e-01\n",
      "[ 82]: fk = 1.00e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.00213e-01\n",
      "[ 83]: fk = 1.00e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.00196e-01\n",
      "[ 84]: fk = 1.00e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.00195e-01\n",
      "[ 85]: fk = 1.00e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.00180e-01\n",
      "[ 86]: fk = 1.00e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 1.00011e-01\n",
      "[ 87]: fk = 1.00e-01 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.96955e-02\n",
      "[ 88]: fk = 9.97e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.96947e-02\n",
      "[ 89]: fk = 9.97e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.96486e-02\n",
      "[ 90]: fk = 9.96e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.96414e-02\n",
      "[ 91]: fk = 9.96e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.95207e-02\n",
      "[ 92]: fk = 9.95e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.92625e-02\n",
      "[ 93]: fk = 9.93e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.91791e-02\n",
      "[ 94]: fk = 9.92e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.91618e-02\n",
      "[ 95]: fk = 9.92e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.89493e-02\n",
      "[ 96]: fk = 9.89e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.86862e-02\n",
      "[ 97]: fk = 9.87e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.84767e-02\n",
      "[ 98]: fk = 9.85e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.83883e-02\n",
      "[ 99]: fk = 9.84e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.83731e-02\n",
      "[100]: fk = 9.84e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.83723e-02\n",
      "[101]: fk = 9.84e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.83550e-02\n",
      "[102]: fk = 9.84e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.81476e-02\n",
      "[103]: fk = 9.81e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.81451e-02\n",
      "[104]: fk = 9.81e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.80432e-02\n",
      "[105]: fk = 9.80e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.80059e-02\n",
      "[106]: fk = 9.80e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.79813e-02\n",
      "[107]: fk = 9.80e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.79810e-02\n",
      "[108]: fk = 9.80e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.79665e-02\n",
      "[109]: fk = 9.80e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.79032e-02\n",
      "[110]: fk = 9.79e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.78781e-02\n",
      "[111]: fk = 9.79e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.78042e-02\n",
      "[112]: fk = 9.78e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.77822e-02\n",
      "[113]: fk = 9.78e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.77821e-02\n",
      "[114]: fk = 9.78e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.77821e-02\n",
      "[115]: fk = 9.78e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.77763e-02\n",
      "[116]: fk = 9.78e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.77638e-02\n",
      "[117]: fk = 9.78e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.76172e-02\n",
      "[118]: fk = 9.76e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.76128e-02\n",
      "[119]: fk = 9.76e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.75330e-02\n",
      "[120]: fk = 9.75e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.75328e-02\n",
      "[121]: fk = 9.75e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.75328e-02\n",
      "[122]: fk = 9.75e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.74677e-02\n",
      "[123]: fk = 9.75e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.73512e-02\n",
      "[124]: fk = 9.74e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.73508e-02\n",
      "[125]: fk = 9.74e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.73298e-02\n",
      "[126]: fk = 9.73e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.73137e-02\n",
      "[127]: fk = 9.73e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.73060e-02\n",
      "[128]: fk = 9.73e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.73050e-02\n",
      "[129]: fk = 9.73e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.72403e-02\n",
      "[130]: fk = 9.72e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.71755e-02\n",
      "[131]: fk = 9.72e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.71409e-02\n",
      "[132]: fk = 9.71e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.70512e-02\n",
      "[133]: fk = 9.71e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.70421e-02\n",
      "[134]: fk = 9.70e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.70403e-02\n",
      "[135]: fk = 9.70e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.70292e-02\n",
      "[136]: fk = 9.70e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.70117e-02\n",
      "[137]: fk = 9.70e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.69804e-02\n",
      "[138]: fk = 9.70e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.69380e-02\n",
      "[139]: fk = 9.69e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.68821e-02\n",
      "[140]: fk = 9.69e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.67037e-02\n",
      "[141]: fk = 9.67e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.66854e-02\n",
      "[142]: fk = 9.67e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.65972e-02\n",
      "[143]: fk = 9.66e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.65092e-02\n",
      "[144]: fk = 9.65e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.65068e-02\n",
      "[145]: fk = 9.65e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.65050e-02\n",
      "[146]: fk = 9.65e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.64873e-02\n",
      "[147]: fk = 9.65e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.64609e-02\n",
      "[148]: fk = 9.65e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.64533e-02\n",
      "[149]: fk = 9.65e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.63024e-02\n",
      "[150]: fk = 9.63e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.61922e-02\n",
      "[151]: fk = 9.62e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.60217e-02\n",
      "[152]: fk = 9.60e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.59559e-02\n",
      "[153]: fk = 9.60e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.59546e-02\n",
      "[154]: fk = 9.60e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.59378e-02\n",
      "[155]: fk = 9.59e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.59343e-02\n",
      "[156]: fk = 9.59e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.58969e-02\n",
      "[157]: fk = 9.59e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.58148e-02\n",
      "[158]: fk = 9.58e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.56771e-02\n",
      "[159]: fk = 9.57e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.56119e-02\n",
      "[160]: fk = 9.56e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.54454e-02\n",
      "[161]: fk = 9.54e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.53682e-02\n",
      "[162]: fk = 9.54e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.53419e-02\n",
      "[163]: fk = 9.53e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.53212e-02\n",
      "[164]: fk = 9.53e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.53210e-02\n",
      "[165]: fk = 9.53e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.53203e-02\n",
      "[166]: fk = 9.53e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.51706e-02\n",
      "[167]: fk = 9.52e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.51575e-02\n",
      "[168]: fk = 9.52e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.44842e-02\n",
      "[169]: fk = 9.45e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.43830e-02\n",
      "[170]: fk = 9.44e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.43706e-02\n",
      "[171]: fk = 9.44e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.42557e-02\n",
      "[172]: fk = 9.43e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.39688e-02\n",
      "[173]: fk = 9.40e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.39417e-02\n",
      "[174]: fk = 9.39e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.32917e-02\n",
      "[175]: fk = 9.33e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.28078e-02\n",
      "[176]: fk = 9.28e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.28061e-02\n",
      "[177]: fk = 9.28e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.27508e-02\n",
      "[178]: fk = 9.28e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.24859e-02\n",
      "[179]: fk = 9.25e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.21688e-02\n",
      "[180]: fk = 9.22e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.21493e-02\n",
      "[181]: fk = 9.21e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.20820e-02\n",
      "[182]: fk = 9.21e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.20743e-02\n",
      "[183]: fk = 9.21e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.19492e-02\n",
      "[184]: fk = 9.19e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.19327e-02\n",
      "[185]: fk = 9.19e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.19117e-02\n",
      "[186]: fk = 9.19e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.19118e-02\n",
      "[187]: fk = 9.19e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.14567e-02\n",
      "[188]: fk = 9.15e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.14565e-02\n",
      "[189]: fk = 9.15e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.14500e-02\n",
      "[190]: fk = 9.14e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.14488e-02\n",
      "[191]: fk = 9.14e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.14430e-02\n",
      "[192]: fk = 9.14e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.14420e-02\n",
      "[193]: fk = 9.14e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.12627e-02\n",
      "[194]: fk = 9.13e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.11963e-02\n",
      "[195]: fk = 9.12e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.11211e-02\n",
      "[196]: fk = 9.11e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.09200e-02\n",
      "[197]: fk = 9.09e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.07825e-02\n",
      "[198]: fk = 9.08e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.07756e-02\n",
      "[199]: fk = 9.08e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.07722e-02\n",
      "[200]: fk = 9.08e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.07673e-02\n",
      "[201]: fk = 9.08e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.07595e-02\n",
      "[202]: fk = 9.08e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.06869e-02\n",
      "[203]: fk = 9.07e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.06849e-02\n",
      "[204]: fk = 9.07e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.06340e-02\n",
      "[205]: fk = 9.06e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.05331e-02\n",
      "[206]: fk = 9.05e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.05269e-02\n",
      "[207]: fk = 9.05e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.05266e-02\n",
      "[208]: fk = 9.05e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.05264e-02\n",
      "[209]: fk = 9.05e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.05263e-02\n",
      "[210]: fk = 9.05e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.05263e-02\n",
      "[211]: fk = 9.05e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.05228e-02\n",
      "[212]: fk = 9.05e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.04738e-02\n",
      "[213]: fk = 9.05e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.04711e-02\n",
      "[214]: fk = 9.05e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.01325e-02\n",
      "[215]: fk = 9.01e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.00952e-02\n",
      "[216]: fk = 9.01e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 9.00686e-02\n",
      "[217]: fk = 9.01e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.95630e-02\n",
      "[218]: fk = 8.96e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.92776e-02\n",
      "[219]: fk = 8.93e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.92674e-02\n",
      "[220]: fk = 8.93e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.92599e-02\n",
      "[221]: fk = 8.93e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.91272e-02\n",
      "[222]: fk = 8.91e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.90290e-02\n",
      "[223]: fk = 8.90e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.90281e-02\n",
      "[224]: fk = 8.90e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.89746e-02\n",
      "[225]: fk = 8.90e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.87330e-02\n",
      "[226]: fk = 8.87e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.87278e-02\n",
      "[227]: fk = 8.87e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.87201e-02\n",
      "[228]: fk = 8.87e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.87193e-02\n",
      "[229]: fk = 8.87e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.87189e-02\n",
      "[230]: fk = 8.87e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.86899e-02\n",
      "[231]: fk = 8.87e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.86770e-02\n",
      "[232]: fk = 8.87e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.86483e-02\n",
      "[233]: fk = 8.86e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.86474e-02\n",
      "[234]: fk = 8.86e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.86215e-02\n",
      "[235]: fk = 8.86e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.72210e-02\n",
      "[236]: fk = 8.72e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.67554e-02\n",
      "[237]: fk = 8.68e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.67456e-02\n",
      "[238]: fk = 8.67e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.67317e-02\n",
      "[239]: fk = 8.67e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.67121e-02\n",
      "[240]: fk = 8.67e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.66946e-02\n",
      "[241]: fk = 8.67e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.60581e-02\n",
      "[242]: fk = 8.61e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.60575e-02\n",
      "[243]: fk = 8.61e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.58917e-02\n",
      "[244]: fk = 8.59e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.58846e-02\n",
      "[245]: fk = 8.59e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.58123e-02\n",
      "[246]: fk = 8.58e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.58103e-02\n",
      "[247]: fk = 8.58e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.54625e-02\n",
      "[248]: fk = 8.55e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.53522e-02\n",
      "[249]: fk = 8.54e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.53130e-02\n",
      "[250]: fk = 8.53e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.53129e-02\n",
      "[251]: fk = 8.53e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.52634e-02\n",
      "[252]: fk = 8.53e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    Improvement from line search | f(prox_ls): 8.50713e-02\n",
      "[253]: fk = 8.51e-02 | deltak = 1.00e-04 | tk = 1.00e+01\n",
      "    Number of non-zero softmax weights on samples: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Run HJ_MAD to optimize the model parameters\u001b[39;00m\n\u001b[1;32m     39\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 40\u001b[0m new_parameters, loss, xk_hist, loss_history, delta_hist, tk_hist, iterations \u001b[38;5;241m=\u001b[39m \u001b[43mHJ_MAD_alg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_vector\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Run HJ_MAD to optimize the model parameters\u001b[39;00m\n\u001b[1;32m     41\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time  \u001b[38;5;66;03m# Calculate elapsed time\u001b[39;00m\n\u001b[1;32m     43\u001b[0m func_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39miterations \u001b[38;5;241m*\u001b[39m (int_samples)\n",
      "File \u001b[0;32m~/hj-global-opt/hj_mad_ls.py:308\u001b[0m, in \u001b[0;36mHJ_MD_LS.run\u001b[0;34m(self, f, x0)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Line Search\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_search:\n\u001b[0;32m--> 308\u001b[0m     prox_xk_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimprove_prox_with_line_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprox_xk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdeltak\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     f_prox_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(prox_xk_new\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f_prox_new \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.9\u001b[39m\u001b[38;5;241m*\u001b[39mf_prox:\n",
      "File \u001b[0;32m~/hj-global-opt/hj_mad_ls.py:127\u001b[0m, in \u001b[0;36mHJ_MD_LS.improve_prox_with_line_search\u001b[0;34m(self, xk, prox_xk, deltak, tk)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Convert into f form\u001b[39;00m\n\u001b[1;32m    126\u001b[0m y \u001b[38;5;241m=\u001b[39m xk_expanded \u001b[38;5;241m+\u001b[39m tau\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m direction_expanded\n\u001b[0;32m--> 127\u001b[0m f_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Size (int_samples,1)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Apply Rescaling to Exponent\u001b[39;00m\n\u001b[1;32m    130\u001b[0m rescaled_exponent \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m rescale_factor\u001b[38;5;241m*\u001b[39mf_values\u001b[38;5;241m/\u001b[39m deltak\n",
      "Cell \u001b[0;32mIn[62], line 3\u001b[0m, in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 94\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(x, model, inputs, targets, plot)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Extract parameters for the current sample\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     model_params \u001b[38;5;241m=\u001b[39m x[i, :]  \u001b[38;5;66;03m# Take the i-th sample\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update model parameters\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Forward pass and loss computation\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "Cell \u001b[0;32mIn[59], line 63\u001b[0m, in \u001b[0;36mShallowNet.set_parameters\u001b[0;34m(self, new_parameters)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     new_param_values \u001b[38;5;241m=\u001b[39m new_parameters[\u001b[38;5;241m0\u001b[39m, offset:offset \u001b[38;5;241m+\u001b[39m param_size]  \u001b[38;5;66;03m# Access batch dimension\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m new_param_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_param_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Match the original shape\u001b[39;00m\n\u001b[1;32m     64\u001b[0m param\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy_(new_param_values)  \u001b[38;5;66;03m# Update parameter\u001b[39;00m\n\u001b[1;32m     65\u001b[0m offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m param_size\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define f\n",
    "def f(x):\n",
    "    return loss_function(x, model, x_tensor, y_tensor)\n",
    "\n",
    "def plot(k, xk, fk, deltak, tk, ax):\n",
    "    model.set_parameters(xk)\n",
    "    ax.clear()  # Clear previous plot to avoid overlap\n",
    "    ax.plot(x, y_tensor, label='True function')  # Assuming `x` and `y_tensor` are available\n",
    "    ax.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')  # Assuming model and x_tensor are available\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"Iteration: {k}, Loss: {fk.item():.4e},\\nDelta: {deltak:.2e}, T: {tk:.2e}\")\n",
    "\n",
    "\n",
    "model = ShallowNet(n_neurons)\n",
    "\n",
    "# HJ_MAD hyperparameters\n",
    "delta = 0.1        # Small delta -> Better Approximation of the Moreau Envelope Provided Large T or Close to Global Minimum\n",
    "t =   10          # Large T -> Better Exploration (Moreau Envelope closer to approximate quadratic)\n",
    "int_samples = int(1000)#500  # Large N -> Better Expectations for Larger T -> Better Exploration (But more expensive)\n",
    "max_iters = int(1000)   # Maximum number of iterations\n",
    "loss_tol = 5e-5         # Stopping criterion for the loss\n",
    "distribution=\"Cauchy\" # Set sampling distribution \n",
    "\n",
    "# Use HJ_MAD to optimize the model parameters\n",
    "HJ_MAD_alg = HJ_MD_LS(delta=delta, t=t,distribution=distribution,\n",
    "                      int_samples=int_samples, max_iters=max_iters, f_tol=loss_tol,\n",
    "                      verbose=True, adaptive_time=False,adaptive_delta=False,\n",
    "                      line_search=True, stepsize=1.0)\n",
    "\n",
    "# Get the model parameters as a flattened tensor\n",
    "parameters = list(model.parameters())\n",
    "parameter_vector = torch.cat([param.flatten() for param in parameters])\n",
    "parameter_vector = parameter_vector.unsqueeze(0) # shape (1, num_features)\n",
    "\n",
    "# Compute Current Loss\n",
    "loss_old = f(parameter_vector)  # Compute the loss for the current parameters\n",
    "\n",
    "# Run HJ_MAD to optimize the model parameters\n",
    "start_time = time.time()\n",
    "new_parameters, loss, xk_hist, loss_history, delta_hist, tk_hist, iterations = HJ_MAD_alg.run(f, parameter_vector)  # Run HJ_MAD to optimize the model parameters\n",
    "elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "\n",
    "func_evals = 2*iterations * (int_samples)\n",
    "print(\"\\n=== Optimization Results ===\")\n",
    "print(f\"Function Evaluations: {func_evals}\")\n",
    "print(f\"Iterations          : {iterations} (Max: {max_iters})\")\n",
    "print(f\"Elapsed Time        : {elapsed_time:.2f} seconds\")\n",
    "print(f\"Final Loss          : {f(new_parameters).item():.5e}\")\n",
    "print(\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the data for animation\n",
    "# fk_hist = loss_history  # Assuming loss_history has the loss at each iteration\n",
    "# deltak_hist = delta_hist  # Assuming delta_hist has delta values at each iteration\n",
    "# tk_hist = tk_hist  # Assuming tk_hist has t values at each iteration\n",
    "# xk_hist = xk_hist  # Assuming xk_hist has the model parameters at each iteration\n",
    "\n",
    "# # Create figure and axis for animation\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# def plot(k, xk, fk, deltak, tk, ax):\n",
    "#     model.set_parameters(xk)\n",
    "#     ax.clear()  # Clear previous plot to avoid overlap\n",
    "#     ax.plot(x, y_tensor, label='True function')  # Assuming `x` and `y_tensor` are available\n",
    "#     ax.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')  # Assuming model and x_tensor are available\n",
    "#     ax.legend()\n",
    "#     ax.set_title(f\"SNN Neurons={n_neurons}, Loss: {fk.item():.4e},\\nIteration: {k}, Delta: {deltak:.2e}, T: {tk:.2e}\")\n",
    "\n",
    "# # Define the animation update function\n",
    "# def update_plot(frame):\n",
    "#     k = frame\n",
    "#     xk = xk_hist[:,k]\n",
    "#     fk = fk_hist[k]\n",
    "#     deltak = deltak_hist[k]\n",
    "#     tk = tk_hist[k]\n",
    "    \n",
    "#     plot(k, xk, fk, deltak, tk, ax)\n",
    "\n",
    "# # Ensure the histories have the same length\n",
    "# iterations = len(fk_hist)  # Assuming xk_hist is the longest list and matches the others\n",
    "# print(f'Number of iterations: {iterations}')  # Make sure this is correct\n",
    "\n",
    "# # Create the animation using FuncAnimation\n",
    "# ani = FuncAnimation(fig, update_plot, frames=range(iterations), interval=500, repeat=False)\n",
    "\n",
    "# # Save the animation as a GIF (or any format you like)\n",
    "# writer = PillowWriter(fps=50)\n",
    "# ani.save(f'optimization_{n_neurons}.gif', writer=writer)\n",
    "\n",
    "# # Show the plot (optional, depending on whether you want to display it interactively)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(loss_history, label='Loss History', color='blue')\n",
    "plt.title('Loss History during Optimization')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the loss history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(delta_hist, label='delta History', color='blue')\n",
    "plt.title('Delta History during Optimization')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Delta')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(tk_hist, label='t History', color='blue')\n",
    "plt.title('T History during Optimization')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('t')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualise the learned function\n",
    "plt.figure()\n",
    "plt.plot(x, y, label='True function')\n",
    "plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# from hj_prox_ls import HJ_PROX_LS\n",
    "\n",
    "# # Hyperparameters\n",
    "# n_neurons = 50  # Number of neurons in the shallow network\n",
    "# learning_rate = 0.01\n",
    "# num_epochs = 1000\n",
    "# batch_size = 100\n",
    "\n",
    "# # Generate data\n",
    "# no_of_samples = 500\n",
    "# noise_level = 1e-3\n",
    "# x = np.linspace(0, 2 * np.pi, no_of_samples)\n",
    "# y = np.sin(x) + noise_level * np.random.randn(*x.shape)  # Add noise to the samples\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "# y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Create dataset and dataloader\n",
    "# dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Define the shallow neural network\n",
    "# class ShallowNet(nn.Module):\n",
    "#     def __init__(self, n_neurons):\n",
    "#         super(ShallowNet, self).__init__()\n",
    "#         self.linear = nn.Linear(1, n_neurons)\n",
    "#         self.coeffs = nn.Parameter(torch.randn(n_neurons, 1))\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear(x)      # Compute w_j * x + b_j\n",
    "#         x = self.relu(x)        # Apply ReLU activation\n",
    "#         x = x @ self.coeffs     # Compute sum of c_j * ReLU(...)\n",
    "#         return x\n",
    "\n",
    "#     # def set_parameters(self, new_parameters):\n",
    "#     #     \"\"\"\n",
    "#     #     Set model parameters from the flattened parameter vector.\n",
    "#     #     This method updates the model's parameters from a single flattened tensor.\n",
    "#     #     \"\"\"\n",
    "#     #     offset = 0\n",
    "#     #     for param in self.parameters():\n",
    "#     #         param_size = param.numel() \n",
    "#     #         new_param_values = new_parameters[offset:offset + param_size]\n",
    "#     #         new_param_values = new_param_values.view(param.shape)  \n",
    "#     #         param.data.copy_(new_param_values)  \n",
    "#     #         offset += param_size  \n",
    "\n",
    "#     def set_parameters(self, new_parameters):\n",
    "#         \"\"\"\n",
    "#         Set model parameters from the flattened parameter vector.\n",
    "#         This method updates the model's parameters from a single flattened tensor.\n",
    "\n",
    "#         Args:\n",
    "#             new_parameters: A flattened tensor containing all parameters.\n",
    "#         \"\"\"\n",
    "#         offset = 0\n",
    "#         for param in self.parameters():\n",
    "#             param_size = param.numel()  # Total number of elements in the parameter\n",
    "#             # Extract the relevant slice and reshape it to the parameter's original shape\n",
    "#             if len(new_parameters.shape) == 1:\n",
    "#                 new_param_values = new_parameters[offset:offset + param_size]\n",
    "#             else:\n",
    "#                 new_param_values = new_parameters[0, offset:offset + param_size]  # Access batch dimension\n",
    "#             new_param_values = new_param_values.view_as(param)  # Match the original shape\n",
    "#             param.data.copy_(new_param_values)  # Update parameter\n",
    "#             offset += param_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize model, loss function, and optimizer\n",
    "# model = ShallowNet(n_neurons)\n",
    "# criterion = nn.MSELoss()\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # HJ_MAD hyperparameters\n",
    "# delta0 = 1\n",
    "# t = 1\n",
    "# int_samples = int(100)\n",
    "# delta_dampener=0.8\n",
    "# beta=0.0\n",
    "# first_moment = None\n",
    "\n",
    "# # Use HJ_MAD to optimize the model parameters\n",
    "# HJ_PROX_LS_alg = HJ_PROX_LS(delta=delta0, t=t, int_samples=int_samples,delta_dampener=delta_dampener,beta=beta,verbose=True)\n",
    "\n",
    "# # Define the function f (e.g., a loss function based on the model's predictions)\n",
    "# def Objective_function(x, model, inputs, targets):\n",
    "#     \"\"\"\n",
    "#     Computes the loss based on the given model parameters.\n",
    "\n",
    "#     Args:\n",
    "#         x: Tensor of shape (n_samples, n_features) representing flattened parameters.\n",
    "#         model: The neural network model.\n",
    "#         inputs: Input batch.\n",
    "#         targets: Ground truth values.\n",
    "\n",
    "#     Returns:\n",
    "#         Tensor of shape (n_samples,) containing the loss for each sample.\n",
    "#     \"\"\"\n",
    "#     n_samples = x.shape[0]\n",
    "    \n",
    "#     # Initialize list to store losses\n",
    "#     sample_losses = []\n",
    "\n",
    "#     # Compute losses for each sample in the batch\n",
    "#     for i in range(n_samples):\n",
    "#         # Extract parameters for the current sample\n",
    "#         model_params = x[i, :]  # Take the i-th sample\n",
    "#         model.set_parameters(model_params)  # Update model parameters\n",
    "\n",
    "#         # Forward pass and loss computation\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         sample_losses.append(loss.item())\n",
    "\n",
    "#     # Convert the list of losses to a tensor\n",
    "#     return torch.tensor(sample_losses)\n",
    "\n",
    "\n",
    "# # Training loop: Iterate through the epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Iterate through the dataset in batches\n",
    "#     for inputs, targets in dataloader:\n",
    "#         # Define f\n",
    "#         def f(x):\n",
    "#             return Objective_function(x, model, inputs, targets)\n",
    "        \n",
    "#         # Get the model parameters as a flattened tensor\n",
    "#         parameters = list(model.parameters())\n",
    "#         parameter_vector = torch.cat([param.flatten() for param in parameters])\n",
    "#         parameter_vector = parameter_vector.unsqueeze(0) # shape (1, num_features)\n",
    "\n",
    "#         # Compute Current Loss\n",
    "#         loss_old = f(parameter_vector)  # Compute the loss for the current parameters\n",
    "\n",
    "#         # Run HJ_MAD to optimize the model parameters\n",
    "#         new_parameters, loss, first_moment = HJ_PROX_LS_alg.run(f, parameter_vector)  # Run HJ_MAD to optimize the model parameters\n",
    "\n",
    "#         # Update the model parameters with the optimized values\n",
    "#         model.set_parameters(new_parameters)\n",
    "        \n",
    "#     # Print the loss every 100 epochs for monitoring\n",
    "#     if (epoch + 1) % 100 == 0:\n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Delta: {HJ_PROX_LS_alg.delta}')\n",
    "\n",
    "# # Visualise the learned function\n",
    "# plt.figure()\n",
    "# plt.plot(x, y, label='True function')\n",
    "# plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ShallowNet(n_neurons)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "elapsed_time = time.time() - start_time\n",
    "# Visualise the learned function\n",
    "plt.figure()\n",
    "plt.plot(x, y, label='True function')\n",
    "plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(model(x_tensor), y_tensor)\n",
    "print(\"\\n=== Optimization Results ===\")\n",
    "print(f\"Elapsed Time        : {elapsed_time:.2f} seconds\")\n",
    "print(f\"Final Loss          : {loss.item():.5e}\")\n",
    "print(\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define f\n",
    "# def f(x):\n",
    "#     return loss_function(x, model, x_tensor, y_tensor)\n",
    "\n",
    "# def plot(k, xk, fk, deltak, tk, ax):\n",
    "#     model.set_parameters(xk)\n",
    "#     ax.clear()  # Clear previous plot to avoid overlap\n",
    "#     ax.plot(x, y_tensor, label='True function')  # Assuming `x` and `y_tensor` are available\n",
    "#     ax.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')  # Assuming model and x_tensor are available\n",
    "#     ax.legend()\n",
    "#     ax.set_title(f\"Iteration: {k}, Loss: {fk.item():.4e},\\nDelta: {deltak:.2e}, T: {tk:.2e}\")\n",
    "\n",
    "\n",
    "# model = ShallowNet(n_neurons)\n",
    "\n",
    "# # HJ_MAD hyperparameters\n",
    "# delta = 1e-12#5e-11          # Small delta -> Better Approximation of the Moreau Envelope Provided Large T or Close to Global Minimum\n",
    "# t =   5e+7#5e7            # Large T -> Better Exploration (Moreau Envelope closer to approximate quadratic)\n",
    "# int_samples = int(500)#500  # Large N -> Better Expectations for Larger T -> Better Exploration (But more expensive)\n",
    "# max_iters = int(1000)   # Maximum number of iterations\n",
    "# loss_tol = 9e-5         # Stopping criterion for the loss\n",
    "# distribution=\"Cauchy\" # Set sampling distribution \n",
    "\n",
    "# # Use HJ_MAD to optimize the model parameters\n",
    "# HJ_MAD_alg = HJ_MD_LS(delta=delta, t=t,distribution=distribution,\n",
    "#                       int_samples=int_samples, max_iters=max_iters, f_tol=loss_tol,\n",
    "#                       verbose=True, adaptive_time=True,adaptive_delta=True,\n",
    "#                       line_search=False, stepsize=1.0)\n",
    "\n",
    "# # Get the model parameters as a flattened tensor\n",
    "# parameters = list(model.parameters())\n",
    "# parameter_vector = torch.cat([param.flatten() for param in parameters])\n",
    "# parameter_vector = parameter_vector.unsqueeze(0) # shape (1, num_features)\n",
    "\n",
    "# # Compute Current Loss\n",
    "# loss_old = f(parameter_vector)  # Compute the loss for the current parameters\n",
    "\n",
    "# # Run HJ_MAD to optimize the model parameters\n",
    "# start_time = time.time()\n",
    "# new_parameters, loss, xk_hist, loss_history, delta_hist, tk_hist, iterations = HJ_MAD_alg.run(f, parameter_vector)  # Run HJ_MAD to optimize the model parameters\n",
    "# elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "\n",
    "# func_evals = 2*iterations * (int_samples)\n",
    "# print(\"\\n=== Optimization Results ===\")\n",
    "# print(f\"Function Evaluations: {func_evals}\")\n",
    "# print(f\"Iterations          : {iterations} (Max: {max_iters})\")\n",
    "# print(f\"Elapsed Time        : {elapsed_time:.2f} seconds\")\n",
    "# print(f\"Final Loss          : {f(new_parameters).item():.5e}\")\n",
    "# print(\"============================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
