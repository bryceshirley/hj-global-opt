{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq1zNUTPg1JJ"
   },
   "source": [
    "### Monte Carlo Integration\n",
    "\n",
    "Monte Carlo integration approximates an integral by interpreting it as an **expected value**. For an integral over an interval $[a, b]$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_a^b f(x) \\, dx,\n",
    "\\end{equation}\n",
    "\n",
    "we can approximate this by sampling $x$ uniformly in $[a, b]$ and computing the expectation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\mathbb{E}_{x \\sim \\text{Uniform}(a, b)}[f(x)] = \\frac{b - a}{N} \\sum_{i=1}^N f(x_i),.\n",
    "\\end{equation}\n",
    "\n",
    "This is achieved by taking multiple uniform samples of $x$, evaluating $f(x)$ at each, averaging the results, and scaling by the interval width $(b - a)$\n",
    "\n",
    "**Convergence in  $\\mathbb{R}^n$**: The order of convergence remains $O\\left(\\frac{1}{\\sqrt{N}}\\right)$ regardless of the dimension $n$, unlike grid-based methods that suffer from exponential growth in computational cost (the \"curse of dimensionality\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdiGOuXQA8qO",
    "outputId": "9d09c67a-aa8a-4138-9582-f52a98dc3a32"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def exact_integral_x2(a, b):\n",
    "    \"\"\"Calculate the exact integral of x^2 over [a, b].\"\"\"\n",
    "    return (b**3 / 3) - (a**3 / 3)\n",
    "\n",
    "def trapezium_rule_x2(a, b, N):\n",
    "    \"\"\"Approximate the integral of x^2 over [a, b] using the trapezium rule.\"\"\"\n",
    "    x = np.linspace(a, b, N)\n",
    "    h = (b - a) / (N - 1)\n",
    "    f_x = x**2\n",
    "    integral = h * (0.5 * f_x[0] + np.sum(f_x[1:-1]) + 0.5 * f_x[-1])\n",
    "    return integral\n",
    "\n",
    "def monte_carlo_x2(a, b, N):\n",
    "    \"\"\"Approximate the integral of x^2 over [a, b] using Monte Carlo sampling.\"\"\"\n",
    "    samples = np.random.uniform(a, b, N)\n",
    "    f_samples = samples**2\n",
    "    integral = (b - a) * np.mean(f_samples)\n",
    "    return integral\n",
    "\n",
    "# Define the interval and the exact solution for comparison\n",
    "a, b = 0, 2  # Integrate over [0, 2]\n",
    "exact_value = exact_integral_x2(a, b)\n",
    "\n",
    "# Number of samples for Monte Carlo and trapezium rule\n",
    "sample_sizes = [10, 50, 100, 500, 1000, 5000, 10000, 100000]\n",
    "\n",
    "# Arrays to store results\n",
    "trapezium_results = []\n",
    "monte_carlo_results = []\n",
    "trapezium_errors = []\n",
    "monte_carlo_errors = []\n",
    "\n",
    "for N in sample_sizes:\n",
    "    # Trapezium rule integration\n",
    "    trapezium_result = trapezium_rule_x2(a, b, N)\n",
    "    trapezium_results.append(trapezium_result)\n",
    "    trapezium_errors.append(abs(trapezium_result - exact_value))\n",
    "\n",
    "    # Monte Carlo integration\n",
    "    monte_carlo_result = monte_carlo_x2(a, b, N)\n",
    "    monte_carlo_results.append(monte_carlo_result)\n",
    "    monte_carlo_errors.append(abs(monte_carlo_result - exact_value))\n",
    "\n",
    "# Display results in a structured format\n",
    "results_df = pd.DataFrame({\n",
    "    'N (Samples)': sample_sizes,\n",
    "    'Trapezium Result': trapezium_results,\n",
    "    'Monte Carlo Result': monte_carlo_results,\n",
    "})\n",
    "\n",
    "results_df_errors = pd.DataFrame({\n",
    "    'N (Samples)': sample_sizes,\n",
    "    'Trapezium Error': trapezium_errors,\n",
    "    'Monte Carlo Error': monte_carlo_errors\n",
    "})\n",
    "\n",
    "print(\"Integration results for f(x) = x^2 over [0, 2]:\")\n",
    "print(\"Exact Solution:\", exact_value)\n",
    "print(\"\")\n",
    "print(results_df)\n",
    "print(\"\")\n",
    "print(results_df_errors)\n",
    "\n",
    "# Plot 1: Function f(x) = x^2 with a single Monte Carlo rectangle\n",
    "x_vals = np.linspace(a, b, 1000)\n",
    "f_x_vals = x_vals**2\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_vals, f_x_vals, label=r'$f(x) = x^2$', color='blue')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Function $f(x) = x^2$ over [0, 2] with Monte Carlo Intregration from 10 Samples')\n",
    "\n",
    "# Generate 10 random samples for the Monte Carlo estimate\n",
    "N_samples = 10\n",
    "samples = np.random.uniform(a, b, N_samples)\n",
    "sample_heights = samples**2\n",
    "average_height = np.mean(sample_heights)\n",
    "width = b - a\n",
    "\n",
    "# Draw the Monte Carlo rectangle\n",
    "plt.bar((a + b) / 2, average_height, width=width, color='orange', edgecolor='black', alpha=0.3, align='center', label='Monte Carlo Sample Mean')\n",
    "\n",
    "# Add vertical dotted lines for each sample\n",
    "for sample_x, sample_y in zip(samples, sample_heights):\n",
    "    plt.vlines(sample_x, ymin=0, ymax=sample_y, color='red', linestyle='--')  # Vertical lines for sample points\n",
    "\n",
    "# Legend and grid\n",
    "plt.legend(['$f(x) = x^2$', 'Monte Carlo Samples'])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Comparison of Integration Results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sample_sizes, [exact_value] * len(sample_sizes), 'k--', label='Exact Value')\n",
    "plt.plot(sample_sizes, trapezium_results, 'o-', label='Trapezium Rule')\n",
    "plt.plot(sample_sizes, monte_carlo_results, 's-', label='Monte Carlo')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of Samples (log scale)')\n",
    "plt.ylabel('Integral Estimate')\n",
    "plt.title('Comparison of Integration Methods for $f(x) = x^2$')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Error Comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sample_sizes, trapezium_errors, 'o-', label='Trapezium Rule Error')\n",
    "plt.plot(sample_sizes, monte_carlo_errors, 's-', label='Monte Carlo Error')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of Samples (log scale)')\n",
    "plt.ylabel('Absolute Error (log-log scale)')\n",
    "plt.title('Error Comparison for Trapezium Rule vs. Monte Carlo')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Integration General case\n",
    "\n",
    "In the general case, if $ f(x) $ is integrated over a domain $ D $ using a probability density function (pdf) $ p(x) $ defined on $ D $, the Monte Carlo approximation becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_D f(x) \\, dx \\approx \\mathbb{E}_{x \\sim p(x)} \\left[ \\frac{f(x)}{p(x)} \\right] = \\frac{1}{N} \\sum_{i=1}^N \\frac{f(x_i)}{p(x_i)},\n",
    "\\end{equation}\n",
    "\n",
    "where $x_i$ are samples drawn from the distribution with density $p(x)$, and $N$ is the total number of samples. \n",
    "\n",
    "Substituting in the pdf for the uniform distribution between $a$ and $b$:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{b - a}, & \\text{if } a \\leq x \\leq b, \\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "it can be seen how the Monte Carlo Integration for uniform distributions is formed.\n",
    "\n",
    "The ability to choose the pdf allows for **importance sampling**, where $p(x)$ is chosen to concentrate samples in regions where $f(x)$ is large, improving the approximation's efficiency.\n",
    "\n",
    "### Using Probability Density Functions for Importance Sampling\n",
    "\n",
    "While uniform sampling works well for simple functions, **importance sampling** can significantly improve efficiency for integrals of functions with non-uniform behavior. Importance sampling uses a sampling distribution that closely matches the shape of the integrand, allowing for more accurate estimates with fewer samples by focusing on regions where $f(x)$ is large.\n",
    "\n",
    "For instance, if $f(x)$ has a peak around certain values, using a **Gaussian distribution** centered on that peak will place more samples in critical regions, increasing the precision of the estimate. The Monte Carlo approximation for an integral of $f(x)$ can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_{-\\infty}^{\\infty} f(x) \\, dx \\approx \\frac{1}{N} \\sum_{i=1}^N f(x_i) \\times \\frac{1}{p(x_i)},\n",
    "\\end{equation}\n",
    "\n",
    "where $p(x)$ is the probability density function of the chosen sampling distribution, such as a Gaussian distribution. This approach allows the estimator to adjust for non-uniform sampling, concentrating samples in regions that contribute the most to the integral.\n",
    "\n",
    "### Example: Integrating $f(x) = Ae^{-Bx^2}$ Using Gaussian Importance Sampling\n",
    "\n",
    "Letâ€™s consider the function $f(x) = Ae^{-Bx^2}$, which has a peak at $x = 0$ and decays rapidly for large $|x|$. Integrating $f(x)$ over $[-\\infty, \\infty]$ yields $\\sqrt{\\pi}$. However, integrating this over a finite interval (e.g., $[-5, 5]$) approximates the result closely due to the rapid decay of $f(x)$.\n",
    "\n",
    "Using importance sampling with a Gaussian distribution $\\mathcal{N}(0, 1)$ aligns well with the behavior of $f(x)$. The approximation becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_{-\\infty}^{\\infty} e^{-x^2} \\, dx \\approx \\frac{1}{N} \\sum_{i=1}^N Ae^{-Bx_i^2} \\times \\frac{1}{p(x_i)},\n",
    "\\end{equation}\n",
    "\n",
    "where $p(x)$ is a Gaussian PDF. The term $ f(x) / p(x$ adjusts each sample to reflect its contribution under the Gaussian distribution, resulting in a more efficient and accurate approximation of the integral. This approach, known as **importance sampling**, reduces variance by focusing on the most significant regions of $f(x)$, often leading to faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import quad\n",
    "\n",
    "A=5\n",
    "B=5\n",
    "# Define the integrand\n",
    "def integrand(x):\n",
    "    return A*np.exp(-B*x**2)\n",
    "\n",
    "# Define the exact integral of exp(-x^2) from -infinity to infinity (for comparison)\n",
    "exact_value = np.sqrt(np.pi)\n",
    "\n",
    "def exact_integral_exp(a, b):\n",
    "    \"\"\"Calculate the exact integral of exp(-x^2) over [a, b] using scipy.\"\"\"\n",
    "    result, _ = quad(lambda x: A*np.exp(-B*x**2), a, b)\n",
    "    return result\n",
    "\n",
    "def monte_carlo_uniform_exp(a, b, N):\n",
    "    \"\"\"Approximate the integral of exp(-x^2) over [a, b] using Monte Carlo sampling with uniform distribution.\"\"\"\n",
    "    samples = np.random.uniform(a, b, N)\n",
    "    f_samples = integrand(samples)\n",
    "    integral = (b - a) * np.mean(f_samples)\n",
    "    return integral\n",
    "\n",
    "def monte_carlo_gaussian_exp(mu, sigma, N):\n",
    "    \"\"\"Approximate the integral of exp(-x^2) over [-inf, inf] using importance sampling with a Gaussian distribution.\"\"\"\n",
    "    samples = np.random.normal(mu, sigma, N)\n",
    "    f_samples = integrand(samples)\n",
    "    weights = np.exp(-samples**2 / (2 * sigma**2)) / (np.sqrt(2 * np.pi * sigma**2))  # Gaussian PDF\n",
    "    integral = np.mean(f_samples / weights)\n",
    "    return integral\n",
    "\n",
    "# Set interval, number of samples, and Gaussian parameters for importance sampling\n",
    "a, b = -5, 5\n",
    "sample_sizes = [10, 50, 100, 500, 1000, 5000, 10000, 100000]\n",
    "mu, sigma = 0, 0.4  # Gaussian centered at 0 with standard deviation 1\n",
    "\n",
    "# Arrays to store results\n",
    "exact_results = []\n",
    "uniform_mc_results = []\n",
    "gaussian_mc_results = []\n",
    "uniform_mc_errors = []\n",
    "gaussian_mc_errors = []\n",
    "\n",
    "# Compute exact solution over the interval [a, b] for comparison\n",
    "exact_result = exact_integral_exp(a, b)\n",
    "\n",
    "# Monte Carlo integrations with both uniform and Gaussian sampling\n",
    "for N in sample_sizes:\n",
    "    uniform_result = monte_carlo_uniform_exp(a, b, N)\n",
    "    gaussian_result = monte_carlo_gaussian_exp(mu, sigma, N)\n",
    "    \n",
    "    # Store results and compute errors\n",
    "    uniform_mc_results.append(uniform_result)\n",
    "    gaussian_mc_results.append(gaussian_result)\n",
    "    uniform_mc_errors.append(abs(uniform_result - exact_result))\n",
    "    gaussian_mc_errors.append(abs(gaussian_result - exact_result))\n",
    "\n",
    "# Display results in a structured format\n",
    "results_df = pd.DataFrame({\n",
    "    'N (Samples)': sample_sizes,\n",
    "    'Exact Result': [exact_result] * len(sample_sizes),\n",
    "    'Uniform MC Result': uniform_mc_results,\n",
    "    'Gaussian MC Result': gaussian_mc_results,\n",
    "    'Uniform MC Error': uniform_mc_errors,\n",
    "    'Gaussian MC Error': gaussian_mc_errors\n",
    "})\n",
    "\n",
    "flabel = fr'$f(x) = {A} e^{{-{B} x^2}}$'\n",
    "print(f\"Integration results for {flabel} over [-5, 5]:\")\n",
    "print(results_df)\n",
    "\n",
    "# Plotting\n",
    "\n",
    "# Plot the integrand f(x) and sampling distributions\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Define the range and values of the integrand for visualization\n",
    "x_vals = np.linspace(-5, 5, 1000)\n",
    "y_vals = integrand(x_vals)\n",
    "\n",
    "# Plot the integrand\n",
    "plt.plot(x_vals, y_vals, 'b-', label=flabel)\n",
    "plt.fill_between(x_vals, y_vals, color='blue', alpha=0.1)\n",
    "\n",
    "# Generate sample points for the uniform and Gaussian distributions\n",
    "uniform_samples = np.random.uniform(a, b, 1000)\n",
    "gaussian_samples = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "# Overlay the sampling distributions\n",
    "plt.hist(uniform_samples, bins=30, range=(-5, 5), density=True, color='orange', alpha=0.4, label='Uniform Distribution [-5, 5]')\n",
    "plt.hist(gaussian_samples, bins=30, range=(-5, 5), density=True, color='green', alpha=0.4, label=r'Gaussian Distribution $\\mathcal{N}(0, 0.4)$')\n",
    "\n",
    "# Labeling the plot\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x) or Density')\n",
    "plt.title(f'Integrand {flabel} and Sampling Distributions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the integration results and errors\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot the integration results\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(sample_sizes, [exact_result] * len(sample_sizes), 'k--', label='Exact Value')\n",
    "plt.plot(sample_sizes, uniform_mc_results, 'o-', label='Uniform MC')\n",
    "plt.plot(sample_sizes, gaussian_mc_results, 's-', label='Gaussian MC')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of Samples (log scale)')\n",
    "plt.ylabel('Integral Estimate')\n",
    "plt.title(f'Integration Results for {flabel}')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the error comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(sample_sizes, uniform_mc_errors, 'o-', label='Uniform MC Error')\n",
    "plt.plot(sample_sizes, gaussian_mc_errors, 's-', label='Gaussian MC Error')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of Samples (log scale)')\n",
    "plt.ylabel('Absolute Error (log scale)')\n",
    "plt.title('Error Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the general formula for Monte Carlo integration includes dividing by the pdf $p(x)$ from which we are sampling, as shown below:\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_D f(x) \\, dx \\approx \\mathbb{E}_{x \\sim p(x)} \\left[ \\frac{f(x)}{p(x)} \\right] = \\frac{1}{N} \\sum_{i=1}^N \\frac{f(x_i)}{p(x_i)}.\n",
    "\\end{equation}\n",
    "\n",
    "### Gaussian Probability Density Function (pdf)\n",
    "\n",
    "The pdf for a Gaussian (normal) distribution with mean $\\mu$ and variance $\\sigma^2$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x - \\mu)^2}{2 \\sigma^2} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Importance Sampling in the Context of Hamilton-Jacobi Equations\n",
    "\n",
    "In Hamilton-Jacobi (HJ) equations, integrals often involve convolutions with a **heat kernel**, representing diffusion over time. For instance, the viscous solution $v_\\delta(x, t)$ of the HJ equation can be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "v_\\delta(x, t) = (2 \\pi \\delta t)^{-n/2} \\int_{\\mathbb{R}^n} \\exp \\left( -\\frac{f(y)}{\\delta} \\right) \\exp \\left( -\\frac{(x - y)^2}{2 \\delta t} \\right) dy,\n",
    "\\end{equation}\n",
    "\n",
    "where the Gaussian term acts as a kernel centered at $x$ with variance $\\delta t$. This Gaussian kernel is proportional to the **probability density function** of a normal distribution with mean $x$ and standard deviation $\\sqrt{\\delta t}$, which allows us to rewrite the convolution as an **expectation** over samples from a Gaussian distribution.\n",
    "\n",
    "### Using the Heat Kernel for Expectations\n",
    "\n",
    "Since the heat kernel corresponds to a Gaussian distribution $\\mathcal{N}(x, \\delta t)$, we can rewrite $\\nabla v_\\delta(x, t)$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla v_\\delta(x, t) = (2 \\pi \\delta t)^{-n/2} \\int_{\\mathbb{R}^n} \\frac{x - y}{\\delta t} \\exp \\left( -\\frac{(x - y)^2}{2 \\delta t} \\right) \\exp \\left( -\\frac{f(y)}{\\delta} \\right) dy.\n",
    "\\end{equation}\n",
    "\n",
    "Using this Gaussian form for sampling, we can approximate the gradient of $u_\\delta(x, t)$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla u_\\delta(x, t) = -\\delta \\cdot \\frac{\\nabla v_\\delta(x, t)}{v_\\delta(x, t)},\n",
    "\\end{equation}\n",
    "\n",
    "where $v_\\delta(x, t)$ is approximated through expectations.\n",
    "\n",
    "### Monte Carlo Approximation for the Expectation\n",
    "\n",
    "We can use Monte Carlo sampling to estimate these expectations by drawing samples $y \\sim \\mathcal{N}(x, \\delta t)$. This leads to the approximation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla u_\\delta(x, t) \\approx \\frac{1}{t} \\cdot \\frac{\\mathbb{E}_{y \\sim \\mathcal{N}(x, \\delta t)} \\left[(x - y) \\exp \\left( -\\frac{f(y)}{\\delta} \\right) \\right]}{\\mathbb{E}_{y \\sim \\mathcal{N}(x, \\delta t)} \\left[ \\exp \\left( -\\frac{f(y)}{\\delta} \\right) \\right]}\n",
    "                        = \\frac{1}{t} \\cdot \\left( x - \\frac{\\mathbb{E}_{y \\sim \\mathcal{N}(x, \\delta t)} \\left[(y \\exp \\left( -\\frac{f(y)}{\\delta} \\right) \\right]}{\\mathbb{E}_{y \\sim \\mathcal{N}(x, \\delta t)} \\left[ \\exp \\left( -\\frac{f(y)}{\\delta} \\right) \\right]} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where the ratio of these expected values provides an approximation of the gradient $\\nabla u_\\delta(x, t)$.\n",
    "\n",
    "Importantly, when we sample from a Gaussian $ p(x)$, the Gaussian term in the integral cancels with the gaussian pdf $p(x)$ in the denominator (recall the gneral Monte Carlo formula), simplifying the estimation.\n",
    "\n",
    "### Practical Considerations for $t$-Dependent Sampling\n",
    "\n",
    "The number of samples required depends on the value of $t$:\n",
    "- **Small $t$**: When $t$ is small, the Gaussian kernel is sharply concentrated around $x$, so fewer samples are typically sufficient to estimate the expectation accurately.\n",
    "- **Large $t$**: For larger $t$, the kernel spreads out, requiring more samples to capture the range of values effectively.\n",
    "\n",
    "This approach enables us to approximate the solution to the HJ equation through sampling-based methods, where importance sampling with a Gaussian distribution centered at $x$ effectively captures the main contributions to the integral due to it's gaussian form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underflow/ Overflow Issues\n",
    "\n",
    "In this code, we encounter a numerical issue due to the Monte Carlo integral result being very close to zero. Since the integral approximates values close to zero, dividing by this integral leads to significant numerical instability. This can cause underflow (if the value is extremely small and rounds to zero) or overflow (if the inverse becomes exceedingly large) issues during computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function f(x) / delta\n",
    "def minus_f_over_delta(x, delta):\n",
    "    return -(1 / delta) * (5 * np.sin(x + 10) + 0.1 * (x + 10) ** 2)\n",
    "\n",
    "# Parameters\n",
    "delta = 1.0\n",
    "x_k = 0  # Center of Gaussian distribution for sampling\n",
    "t = 0.5  # Variance factor for Gaussian sampling\n",
    "standard_deviation=np.sqrt(delta * t)\n",
    "n_samples=10\n",
    "\n",
    "# Generate 10 samples y_i from a Gaussian distribution around x_k with variance delta * t\n",
    "y_samples = np.random.normal(x_k, standard_deviation, n_samples)\n",
    "\n",
    "# Calculate f(y_i) / delta for each sample\n",
    "f_y_samples = minus_f_over_delta(y_samples, delta)\n",
    "\n",
    "# Define a range for x to plot the function\n",
    "x_range = np.linspace(-20, 20, 400)\n",
    "\n",
    "# Calculate f(x) / delta for the range of x\n",
    "f_x_range = minus_f_over_delta(x_range, delta)\n",
    "\n",
    "# Calculate exp(-f(x) / delta) and exp(-f(y_i) / delta) for each point\n",
    "exp_neg_f_x = np.exp(f_x_range)\n",
    "exp_neg_f_y_samples = np.exp(f_y_samples)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot f(x) / delta\n",
    "plt.plot(x_range, f_x_range, label=r'$-f(x) / \\delta$', color='blue')\n",
    "\n",
    "# Plot the points (y_i, f(y_i) / delta)\n",
    "plt.scatter(y_samples, f_y_samples, color='red', label=r'$(y_i, -f(y_i) / \\delta)$')\n",
    "\n",
    "# Plot exp(-f(x) / delta)\n",
    "plt.plot(x_range, exp_neg_f_x, label=r'$\\exp(-f(x) / \\delta)$', color='green', linestyle='--')\n",
    "\n",
    "# Plot exp(-f(y_i) / delta) as scatter points\n",
    "plt.scatter(y_samples, exp_neg_f_y_samples, color='purple', label=r'$\\exp(-f(y_i) / \\delta)$')\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Value')\n",
    "plt.title(r'Plot of $f(x) / \\delta$, $(y_i, f(y_i) / \\delta)$, $\\exp(-f(x) / \\delta)$, and $\\exp(-f(y_i) / \\delta)$')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Rescaling of the Exponent\n",
    "\n",
    "\n",
    "Subtracting the maximum exponent shifts all values closer to zero, reducing the risk of underflow/overflow while preserving the relative sizes of the values. This adjustment stabilizes the computation, allowing for accurate and reliable results without altering the relationships between terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling the exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for 1 Sample\n",
    "\n",
    "\n",
    "When using 1 sample we would expect the prox to be exactly the sample as the exponential terms cancel out. However, if we are niave about how we deal with under/overflow we will not get the expected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Test Case for Sine-based Function\n",
    "def minus_f_over_delta(x, delta):\n",
    "    return -(1 / delta) * (5 * np.sin(x + 10) + 0.1 * (x + 10) ** 2)\n",
    "\n",
    "# Parameters for original test case\n",
    "x_k = -100\n",
    "n_samples=1\n",
    "y_samples = np.random.normal(x_k, standard_deviation, n_samples)\n",
    "eps = 1e-10\n",
    "\n",
    "# Calculate exp(-f(y) / delta) for each sample, subtracting the max for numerical stability\n",
    "exponents = minus_f_over_delta(y_samples, delta)\n",
    "max_exponent = np.max(exponents)\n",
    "exp_values_maxscale = np.exp(exponents - max_exponent)\n",
    "\n",
    "# Monte Carlo estimation for the original function with max scaling\n",
    "expectation_numerator = np.mean(y_samples * exp_values_maxscale)\n",
    "expectation_denominator = np.mean(exp_values_maxscale)\n",
    "prox_xk_maxscale = expectation_numerator / expectation_denominator\n",
    "\n",
    "# Naive Approach for original function with epsilon\n",
    "exp_values_eps = np.exp(exponents)\n",
    "expectation_numerator_eps = np.mean(y_samples * exp_values_eps)\n",
    "expectation_denominator_eps = np.mean(exp_values_eps)\n",
    "prox_xk_eps = expectation_numerator_eps / (expectation_denominator_eps + eps)\n",
    "\n",
    "# Completely naive approach for the original function\n",
    "try:\n",
    "    prox_xk_naive = expectation_numerator_eps / expectation_denominator_eps\n",
    "except ZeroDivisionError:\n",
    "    prox_xk_naive = float('nan')\n",
    "\n",
    "# Output results for the original function\n",
    "print(\"\\nOriginal Function Test Case:\")\n",
    "print(\"Approximated Prox_xk using max scaling (maxscale):\", prox_xk_maxscale)\n",
    "print(\"Approximated Prox_xk using epsilon approach:\", prox_xk_eps)\n",
    "print(\"Approximated Prox_xk completely naive approach:\", prox_xk_naive)\n",
    "print(\"Single sample (for comparison):\", y_samples[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for a Known Prox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test quadratic function f(x)\n",
    "def f(x):\n",
    "    return (1/2)*x ** 2\n",
    "\n",
    "# Parameters\n",
    "delta = 0.01\n",
    "x_k = 20  # Test point for the quadratic function proximal\n",
    "t = 0.1\n",
    "standard_deviation = np.sqrt(delta * t)\n",
    "\n",
    "# Generate samples from a Gaussian distribution centered at x_k with variance delta * t\n",
    "num_samples = 10\n",
    "y_samples = np.random.normal(x_k, standard_deviation, num_samples)\n",
    "\n",
    "# Calculate exp(-f(y) / delta) for each sample, subtracting the max for numerical stability\n",
    "exponents = -f(y_samples)/delta\n",
    "max_exponent = np.max(exponents)\n",
    "exp_values_maxscale = np.exp(exponents - max_exponent)\n",
    "\n",
    "# Monte Carlo estimation of the expectation with max exponent scaling\n",
    "expectation_numerator_maxscale = np.mean(y_samples * exp_values_maxscale)\n",
    "expectation_denominator_maxscale = np.mean(exp_values_maxscale)\n",
    "prox_xk_maxscale = expectation_numerator_maxscale / expectation_denominator_maxscale\n",
    "\n",
    "# Exact solution for the proximal operator of the quadratic function\n",
    "prox_xk_exact = x_k/(1+t)\n",
    "\n",
    "# Naive Approach with epsilon to avoid division by zero\n",
    "eps = 1e-10\n",
    "exp_values = np.exp(exponents)\n",
    "expectation_numerator = np.mean(y_samples * exp_values)\n",
    "expectation_denominator = np.mean(exp_values)\n",
    "prox_xk_eps = expectation_numerator / (expectation_denominator + eps)\n",
    "\n",
    "# Completely naive approach without any stabilization\n",
    "try:\n",
    "    prox_xk_naive = expectation_numerator / expectation_denominator\n",
    "except Exception:\n",
    "    prox_xk_naive = float('nan')  # Assign NaN if division by zero occurs\n",
    "\n",
    "# Via variance effecting rescaling of f\n",
    "rescale = 0.001\n",
    "standard_dev_rescale = np.sqrt(delta * t/rescale)\n",
    "y_samples_rescale = np.random.normal(x_k, standard_dev_rescale, num_samples)\n",
    "exponents_rescale = -rescale*f(y_samples_rescale)/delta\n",
    "exp_values_rescale = np.exp(exponents_rescale)\n",
    "\n",
    "# Monte Carlo estimation of the expectation with variance effecting rescaling\n",
    "expectation_numerator_rescale = np.mean(y_samples_rescale * exp_values_rescale)\n",
    "expectation_denominator_rescale = np.mean(exp_values_rescale)\n",
    "prox_xk_rescale = expectation_numerator_rescale / expectation_denominator_rescale\n",
    "\n",
    "# Output the results\n",
    "print(f\"Quadratic Function Test for {num_samples} samples:\")\n",
    "print(\"Approximated Prox_xk using max scaling (stable):\", prox_xk_maxscale)\n",
    "print(\"Exact Prox_xk for quadratic function:\", prox_xk_exact)\n",
    "print(\"Error (stable):\", np.abs(prox_xk_maxscale - prox_xk_exact))\n",
    "\n",
    "print(\"\\nNaive Approach Results:\")\n",
    "print(\"Approximated Prox_xk using epsilon approach:\", prox_xk_eps)\n",
    "print(\"Error (epsilon approach):\", np.abs(prox_xk_eps - prox_xk_exact))\n",
    "print(\"Approximated Prox_xk completely naive approach:\", prox_xk_naive)\n",
    "print(\"Error (completely naive approach):\", np.abs(prox_xk_naive - prox_xk_exact) if not np.isnan(prox_xk_naive) else \"NaN due to division by zero\")\n",
    "print(\"Approximated Prox_xk variance effecting rescaling naive approach:\", prox_xk_rescale)\n",
    "print(\"Error (variance effecting rescaling naive approach):\", np.abs(prox_xk_rescale - prox_xk_exact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Operator of $f(x)=0.5x^2 + k$\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{prox}_{t (0.5x^2 + k)}(v) = \\frac{v}{1 + t}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Exact Integral Solution for $v_\\delta$\n",
    "\n",
    "\\begin{equation}\n",
    "v_\\delta(x, t) = (2 \\pi \\delta t)^{-n/2} \\int_{\\mathbb{R}^n} \\exp \\left( -\\frac{0.5 \\, y^2 + k}{\\delta} \\right) \\exp \\left( -\\frac{(x - y)^2}{2 \\delta t} \\right) \\, dy\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "v_\\delta(x, t) = \\exp \\left( -\\frac{k}{\\delta} - \\frac{x^2}{2 \\delta (1 + t)} \\right) (1 + t)^{-n/2}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "k = 0.5 * 50**2 # constant to\n",
    "def f(x):\n",
    "    \"\"\"Quadratic test function.\"\"\"\n",
    "    return 0.5 * x**2 + k\n",
    "\n",
    "# Parameters\n",
    "delta = 0.5\n",
    "x_k = 50  # Test point for the quadratic function proximal\n",
    "t = 0.01\n",
    "standard_deviation = np.sqrt(delta * t)\n",
    "rescale = 0.01\n",
    "\n",
    "def exact_v_delta(x,delta,max_exponent=0.0):\n",
    "    if t == -1:\n",
    "        raise ValueError(\"Parameter 't' must not be -1, as it causes division by zero.\")\n",
    "    if delta == 0:\n",
    "        raise ValueError(\"Parameter 'delta' must not be zero.\")\n",
    "\n",
    "    # Calculate the exponent and scale factor separately for readability\n",
    "    exponent = -(k / delta) - (x**2 / (2 * delta * (1 + t))) - np.log(1+t)/2\n",
    "\n",
    "    v_delta_x_exact=np.exp(exponent-max_exponent)\n",
    "    \n",
    "    return v_delta_x_exact, exponent\n",
    "\n",
    "def exact_prox(x):\n",
    "    if t == -1:\n",
    "        raise ValueError(\"Parameter 't' must not be -1, as it causes division by zero.\")\n",
    "    if delta == 0:\n",
    "        raise ValueError(\"Parameter 'delta' must not be zero.\")\n",
    "\n",
    "    prox_xk_exact = x / (1 + t)\n",
    "    \n",
    "    return prox_xk_exact\n",
    "\n",
    "# Exact solution for the proximal operator of the quadratic function\n",
    "\n",
    "v_delta_x_exact_scaled,_ = exact_v_delta(x_k, delta/rescale)\n",
    "\n",
    "prox_xk_exact = exact_prox(x_k)\n",
    "\n",
    "# Define sample sizes for testing\n",
    "sample_sizes = np.logspace(1, 7, num=7, dtype=int)\n",
    "\n",
    "# Initialize lists to store errors and lost percentages\n",
    "errors_maxscaled = []\n",
    "errors_rescale = []\n",
    "errors_rescale_maxscaled = []\n",
    "lost_percentages_numinator_maxscaled = []\n",
    "lost_percentages_denominator_maxscaled = []\n",
    "\n",
    "def loss_sum(arr):\n",
    "    \"\"\"Perform summation with tracking for lost values.\"\"\"\n",
    "    total = 0.0\n",
    "    t = 0.0\n",
    "    lost_count = 0\n",
    "    arr = arr.astype(np.float32)\n",
    "\n",
    "    for value in arr:\n",
    "        t += value\n",
    "        if t == total:  # Count as lost if no contribution to the sum\n",
    "            lost_count += 1\n",
    "        total = t\n",
    "\n",
    "    lost_percentage = lost_count / len(arr) * 100\n",
    "    return total, lost_percentage  # Return sum and lost percentage\n",
    "\n",
    "def calculate_proximal(y_samples, exp_values):\n",
    "    \"\"\"Calculate proximal operator with summation tracking.\"\"\"\n",
    "    numerator, lost_numerator_percentage = loss_sum(y_samples * exp_values)\n",
    "    denominator, lost_denominator_percentage = loss_sum(exp_values)\n",
    "\n",
    "    loss_percentages = (lost_numerator_percentage, lost_denominator_percentage)\n",
    "    \n",
    "    return numerator / denominator, loss_percentages\n",
    "\n",
    "def compute_error(prox_result, exact_solution):\n",
    "    \"\"\"Compute error between computed and exact proximal operator.\"\"\"\n",
    "    return np.abs(prox_result - exact_solution)\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "def log_approach_details(approach, exponents, loss_percentages, error, max_exponent=None, \n",
    "                         exact_exponent=None, exact_diff=None, min_exp_value=None, \n",
    "                         v_delta_exact=None, v_delta=None, max_exp_value_scaled=None):\n",
    "    \"\"\"Log detailed information for each approach.\"\"\"\n",
    "    lost_numerator_percentage, lost_denominator_percentage = loss_percentages\n",
    "    logging.info(f\"{approach} Approach\")\n",
    "    logging.info(f\"  Lost in Numerator: {lost_numerator_percentage:.5f}%, Lost in Denominator: {lost_denominator_percentage:.5f}%\")\n",
    "    logging.info(f\"  Error: {error:.4f}\")\n",
    "    \n",
    "    if max_exponent is not None:\n",
    "        logging.info(f\"  max_exponent: {max_exponent}\")\n",
    "    if exact_exponent is not None:\n",
    "        logging.info(f\"  exact_exponent: {exact_exponent}\")\n",
    "    if exact_diff is not None:\n",
    "        logging.info(f\"  Difference (exact - max): {exact_diff}\")\n",
    "    if min_exp_value is not None:\n",
    "        logging.info(f\"  Minimum exp value: {min_exp_value}\")\n",
    "    if v_delta_exact is not None:\n",
    "        logging.info(f\"  v_delta_x_exact: {v_delta_exact}\")\n",
    "    if v_delta is not None:\n",
    "        logging.info(f\"  v_delta: {v_delta}\")\n",
    "    if max_exp_value_scaled is not None:\n",
    "        logging.info(f\"  Scaled max exp value: {max_exp_value_scaled}\")\n",
    "    logging.info(\"\")  # Blank line for readability\n",
    "\n",
    "# Main loop to calculate errors for each sample size\n",
    "for num_samples in sample_sizes:\n",
    "    logging.info(f\"\\n{num_samples} Samples\")\n",
    "\n",
    "    # Max Scale Approach\n",
    "    y_samples = np.random.normal(x_k, standard_deviation, num_samples)\n",
    "    exponents = -f(y_samples) / delta\n",
    "    max_exponent = np.max(exponents)\n",
    "    maxscaled_exponents = exponents - max_exponent\n",
    "    exp_values_maxscaled = np.exp(maxscaled_exponents)\n",
    "    v_delta_x_exact_maxscaled, exact_exponent = exact_v_delta(x_k, delta, max_exponent)\n",
    "\n",
    "    # Compute error and store it\n",
    "    prox_xk_maxscaled, loss_percentages = calculate_proximal(y_samples, exp_values_maxscaled)\n",
    "    error = compute_error(prox_xk_maxscaled, prox_xk_exact)\n",
    "    errors_maxscaled.append(error)\n",
    "    lost_percentages_numinator_maxscaled.append(loss_percentages[1])\n",
    "    lost_percentages_denominator_maxscaled.append(loss_percentages[1])\n",
    "    log_approach_details(\"Max Scale\", maxscaled_exponents, loss_percentages, error, max_exponent, exact_exponent, exact_exponent - max_exponent, np.min(exp_values_maxscaled), v_delta_x_exact_maxscaled, np.mean(exp_values_maxscaled), np.max(exp_values_maxscaled) * np.exp(max_exponent))\n",
    "\n",
    "    # Rescale Approach\n",
    "    standard_dev_rescale = np.sqrt(delta * t / rescale)\n",
    "    y_samples_rescale = np.random.normal(x_k, standard_dev_rescale, num_samples)\n",
    "    exponents_rescale = -rescale * f(y_samples_rescale) / delta\n",
    "    exp_values_rescale = np.exp(exponents_rescale)\n",
    "\n",
    "    # Compute error and store it\n",
    "    prox_xk_rescale, loss_percentages = calculate_proximal(y_samples_rescale, exp_values_rescale)\n",
    "    error = compute_error(prox_xk_rescale, prox_xk_exact)\n",
    "    errors_rescale.append(error)\n",
    "    log_approach_details(\"Rescale\", exponents_rescale, loss_percentages, error, min_exp_value=np.min(exp_values_rescale), v_delta_exact=v_delta_x_exact_scaled, v_delta=np.mean(exp_values_rescale), max_exp_value_scaled=np.max(exp_values_rescale))\n",
    "\n",
    "    # Rescale + Max Scale Approach\n",
    "    max_exponent = np.max(exponents_rescale)\n",
    "    maxscaled_exponents_rescale = exponents_rescale - max_exponent\n",
    "    exp_values_rescale_maxscale = np.exp(maxscaled_exponents_rescale)\n",
    "\n",
    "    # Compute error and store it\n",
    "    prox_xk_rescale_maxscale, loss_percentages = calculate_proximal(y_samples_rescale, exp_values_rescale_maxscale)\n",
    "    error = compute_error(prox_xk_rescale_maxscale, prox_xk_exact)\n",
    "    errors_rescale_maxscaled.append(error)\n",
    "    log_approach_details(\"Rescale + Max Scale\", maxscaled_exponents_rescale, loss_percentages, error, min_exp_value=np.min(exp_values_rescale_maxscale), v_delta_exact=v_delta_x_exact_scaled, v_delta=np.mean(exp_values_rescale_maxscale), max_exp_value_scaled=np.max(exp_values_rescale_maxscale))\n",
    "\n",
    "\n",
    "# Convert errors to float to avoid potential plotting issues\n",
    "errors_maxscaled = [float(error) for error in errors_maxscaled]\n",
    "errors_rescale = [float(error) for error in errors_rescale]\n",
    "errors_rescale_maxscaled = [float(error) for error in errors_rescale_maxscaled]\n",
    "\n",
    "# Plot the errors for all approaches\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sample_sizes, errors_maxscaled, label='Max Scale Approach', marker='o')\n",
    "plt.plot(sample_sizes, errors_rescale, label='Rescale Approach', marker='x')\n",
    "plt.plot(sample_sizes, errors_rescale_maxscaled, label='Rescale + Max Scale Approach', marker='^')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Error Comparison for Max Scale and Rescale Approaches')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the lost percentage for Max Scale approach\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sample_sizes, lost_percentages_numinator_maxscaled, label='Lost Percentage Numerator (Max Scale)', marker='o', color='r')\n",
    "plt.plot(sample_sizes, lost_percentages_denominator_maxscaled, label='Lost Percentage Denominator (Max Scale)', marker='*', color='g')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Percentage of Lost Samples %')\n",
    "plt.title('Percentage of Lost Samples in Max Scale Approach')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def test_floating_point_precision_loss(large_num):\n",
    "    print(\"Testing with float:\")\n",
    "\n",
    "    for i in range(0, 20):\n",
    "        # Decreasing the small number by an order of magnitude each time\n",
    "        small_num = 10 ** -i\n",
    "        result = large_num + small_num\n",
    "        \n",
    "        # Print results to show when precision is lost\n",
    "        print(f\"Adding {small_num:.1e} to {large_num:.1e}: Result = {result}\")\n",
    "\n",
    "        # Check if the addition did not change the result due to precision loss\n",
    "        if math.isclose(result, large_num, rel_tol=1e-15):\n",
    "            print(f\"Precision loss occurs at small number magnitude: 10^{-(i-1)}\\n\")\n",
    "            break\n",
    "\n",
    "\n",
    "# Run the tests\n",
    "large_num = 1e16  # Adjusted to 1e15 to observe precision better\n",
    "test_floating_point_precision_loss(large_num)\n",
    "large_num = 1e15  # Adjusted to 1e15 to observe precision better\n",
    "test_floating_point_precision_loss(large_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Findings on Handling Overflow/Underflow in Proximal Operator Calculations\n",
    "\n",
    "\n",
    "#### 1. **Naive Approach**\n",
    "   - **Method**: Directly computes the mean of exponentiated terms without scaling or adjustments.\n",
    "   - **Limitations**:\n",
    "     - Highly susceptible to overflow/underflow due to large exponent values.\n",
    "     - In cases where the denominator becomes extremely small, it may result in `NaN` values due to division by zero.\n",
    "   - **Recommendation**: Not suitable for cases with large exponents; this method should be avoided for stability.\n",
    "\n",
    "#### 2. **Addition of Epsilon to Denominator**\n",
    "   - **Method**: Adds a small epsilon value to the denominator to avoid division by zero.\n",
    "   - **Limitations**:\n",
    "     - While it avoids `NaN` values, adding epsilon can still result in a distorted prox_xk if the denominator values are very close to zero, as the result may be overly influenced by the epsilon term.\n",
    "     - Rounding issues remain, as extremely small values compared to epsilon can lead to inaccuracies.\n",
    "   - **Recommendation**: Better than the naive approach for small samples, but still prone to numerical errors for very large or very small exponent values.\n",
    "\n",
    "#### 3. **Max Scaling**\n",
    "   - **Method**: Subtracts the maximum exponent value from all terms before exponentiating, reducing the size  of the exponent values without effecting the variance.\n",
    "   - **Limitations**:\n",
    "     - If the difference between minimum and maximum exponents is too large this won't be fixed, only values close to the max exponent will contribute meaningfully to the mean, effectively truncating smaller contributions.\n",
    "     - This means that increasing sample size will not lead to convergence to the correct prox.\n",
    "   - **Recommendation**: Problems occur when f(x) has sharp structure leading to large differences in exponent values. Max scaling is okay if you are happy to lose accuracy due to truncation of smaller values.\n",
    "\n",
    "#### 4. **Rescale Approach**\n",
    "   - **Method**: Applies a rescaling factor to reduce the overall spread of exponent values.\n",
    "   - **Limitations**:\n",
    "     - Requires tuning of the rescale factor (this could be done be running it until the demoninator is greater than a certain tolerence but will cost time)\n",
    "     - the scaling results in a potientially large increase in variance and as a result more samples are needed for an accuracte expectation.\n",
    "   - **Recommendation**: This methods does converge as the sample size increases.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion: Best Approach\n",
    "\n",
    "The **Rescale Approach** paired with a method to find the optimal rescaling factor is the best as it allows for stability and convergence when sample size increases, however, more samples will be needed then in the niave approaches.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
