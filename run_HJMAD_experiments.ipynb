{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPOilvdqgxGu"
   },
   "source": [
    "# This notebook is based on the paper: \"Global-Convergence-Nonconvex-Optimization\". \n",
    "\n",
    "The aim of this project is to find the global solution to  \n",
    "\\begin{equation}\n",
    "  \\min_{x \\in \\mathbb{R}^n} f(x).\n",
    "\\end{equation}\n",
    "\n",
    "To obtain a global minimizer, the main idea is to minimize the Moreau Envelop instead, which \"convexifies\" the original function. To make the Moreau envelope tractable, we use connections to Hamilton-Jacobi Equations via the Cole-Hopf and Hopf-Lax formulas to efficiently compute the gradients of the Moreau envelope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdiGOuXQA8qO",
    "outputId": "9d09c67a-aa8a-4138-9582-f52a98dc3a32"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "\n",
    "epsilon_double = np.finfo(np.float64).eps\n",
    "\n",
    "from test_functions import Griewank, AlpineN1, Drop_Wave, Levy, Rastrigin, Ackley\n",
    "from test_functions import Griewank_numpy, AlpineN1_numpy, Drop_Wave_numpy, Levy_numpy, Rastrigin_numpy, Ackley_numpy\n",
    "\n",
    "from test_functions import MultiMinimaFunc, MultiMinimaAbsFunc\n",
    "from test_functions import MultiMinimaFunc_numpy, MultiMinimaAbsFunc_numpy\n",
    "\n",
    "seed   = 30\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq1zNUTPg1JJ"
   },
   "source": [
    "### Create Class for Hamilton-Jacobi Moreau Adaptive Descent (HJ_MAD)\n",
    "\n",
    "The Moreau envelop of $f$ is given by\n",
    "\\begin{equation}\n",
    "  u(x,t) \\triangleq \\inf_{z\\in \\mathbb{R}^n} f(z) + \\dfrac{1}{2t}\\|z-x\\|^2.\n",
    "\\end{equation}\n",
    "\n",
    "We leverage the fact that the solution to the Moreau envelope above satisfies the Hamilton-Jacobi Equation\n",
    "\\begin{equation}\n",
    "  \\begin{split}\n",
    "    u_t^\\delta  + \\frac{1}{2}\\|Du^\\delta  \\|^2 \\ = \\frac{\\delta}{2} \\Delta u^\\delta \\qquad &\\text{ in }  \\mathbb{R}^n\\times (0,T]\n",
    "    \\\\\n",
    "    u = f \\qquad &\\text{ in } \\mathbb{R}^n\\times \\{t = 0\\}\n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "when $\\delta = 0$. \n",
    "\n",
    "By adding a viscous term ($\\delta > 0$), we are able to approximate the solution to the HJ equation using the Cole-Hopf formula to obtain\n",
    "\\begin{equation}\n",
    "  u^\\delta(x,t) = - \\delta \\ln\\Big(\\Phi_t * \\exp(-f/\\delta)\\Big)(x) = - \\delta \\ln \\int_{\\mathbb{R}^n} \\Phi(x-y,t)  \\exp\\left(\\frac{-f(y)}{\\delta}\\right) dy \n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{equation}\n",
    "  \\Phi(x,t) = \\frac{1}{{(4\\pi \\delta t)}^{n/2}} \\exp{\\frac{-\\|x\\|^2}{4\\delta t}}. \n",
    "\\end{equation}\n",
    "This allows us to write the Moreau Envelope (and its gradient) explicitly as an expectation. In this case, we compute the gradient as\n",
    "\\begin{equation}\n",
    "  \\nabla u^\\delta(x,t) = \\dfrac{1}{t}\\cdot  \\dfrac{\\mathbb{E}_{y\\sim  \\mathbb{P}_{x,t}}\\left[(x-y) \\exp\\left(-\\delta^{-1}\\tilde{f}(y)\\right) \\right]}\n",
    "    {\\mathbb{E}_{y\\sim  \\mathbb{P}_{x,t}}\\left[ \\exp\\left(-\\delta^{-1} \\tilde{f}(y)\\right) \\right]}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLVImqaPe-ph"
   },
   "source": [
    "### Define classes for HJ-MAD, Pure Random Search, and Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aI9QUu3vA9Yj"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# HJ Moreau Adaptive Descent\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class HJ_MAD:\n",
    "    ''' \n",
    "        Hamilton-Jacobi Moreau Adaptive Descent (HJ_MAD) is used to solve nonconvex minimization\n",
    "        problems via a zeroth-order sampling scheme.\n",
    "        \n",
    "        Inputs:\n",
    "          1)  f            = function to be minimized. Inputs have size (n_samples x n_features). Outputs have size n_samples\n",
    "          2)  x_true       = true global minimizer\n",
    "          3)  delta        = coefficient of viscous term in the HJ equation\n",
    "          4)  int_samples  = number of samples used to approximate expectation in heat equation solution\n",
    "          5)  x_true       = true global minimizer\n",
    "          6)  t_vec        = time vector containig [initial time, minimum time allowed, maximum time]\n",
    "          7)  max_iters    = max number of iterations\n",
    "          8)  tol          = stopping tolerance\n",
    "          9)  theta        = parameter used to update tk\n",
    "          10) beta         = exponential averaging term for gradient beta (beta multiplies history, 1-beta multiplies current grad)\n",
    "          11) eta_vec      = vector containing [eta_minus, eta_plus], where eta_minus < 1 and eta_plus > 1 (part of time update)\n",
    "          11) alpha        = step size. has to be in between (1-sqrt(eta_minus), 1+sqrt(eta_plus))\n",
    "          12) fixed_time   = boolean for using adaptive time\n",
    "          13) verbose      = boolean for printing\n",
    "          14) momentum     = For acceleration.\n",
    "          15) accelerated  = boolean for using Accelerated Gradient Descent\n",
    "\n",
    "        Outputs:\n",
    "          1) x_opt                    = optimal x_value approximation\n",
    "          2) xk_hist                  = update history\n",
    "          3) tk_hist                  = time history\n",
    "          4) fk_hist                  = function value history\n",
    "          5) xk_error_hist            = error to true solution history \n",
    "          6) rel_grad_uk_norm_hist    = relative grad norm history of Moreau envelope\n",
    "    '''\n",
    "    def __init__(self, f, x_true, delta=0.1, int_samples=100, t_vec = [1.0, 1e-3, 1e1], max_iters=5e4, \n",
    "                 tol=5e-2, theta=0.9, beta=[0.9], eta_vec = [0.9, 1.1], alpha=1.0, fixed_time=False, \n",
    "                 verbose=True,rescale0=1e-1, momentum=None):\n",
    "      \n",
    "      self.delta            = delta\n",
    "      self.f                = f\n",
    "      self.int_samples      = int_samples\n",
    "      self.max_iters        = max_iters\n",
    "      self.tol              = tol\n",
    "      self.t_vec            = t_vec\n",
    "      self.theta            = theta\n",
    "      self.x_true           = x_true\n",
    "      self.beta             = beta \n",
    "      self.alpha            = alpha \n",
    "      self.eta_vec          = eta_vec\n",
    "      self.fixed_time       = fixed_time\n",
    "      self.verbose          = verbose\n",
    "      self.momentum         = momentum\n",
    "      self.rescale0         = rescale0\n",
    "\n",
    "      \n",
    "      # check that alpha is in right interval\n",
    "      assert(alpha >= 1-np.sqrt(eta_vec[0]))\n",
    "      assert(alpha <= 1+np.sqrt(eta_vec[1]))\n",
    "\n",
    "    def find_rescale_factor(self,x, t,f,delta,rescale_factor):\n",
    "\n",
    "      n_features = x.shape[0]\n",
    "      standard_dev = np.sqrt(delta*t/rescale_factor)\n",
    "\n",
    "      samples = self.int_samples\n",
    "\n",
    "      # if max_fy is not None:\n",
    "      #   samples = np.abs(int(t*max_fy/(self.tol*np.log(epsilon_double))))\n",
    "      #   print(f\"{samples=}\")\n",
    "      \n",
    "      y = standard_dev * torch.randn(samples, n_features) + x\n",
    "\n",
    "      f_values = f(y)\n",
    "\n",
    "      # max_fy = torch.abs(torch.max(f_values))\n",
    "\n",
    "      exp_term = torch.exp(-rescale_factor*f_values/delta)\n",
    "      v_delta       = torch.mean(exp_term)\n",
    "\n",
    "      return v_delta,standard_dev#,max_fy\n",
    "    \n",
    "    def compute_grad_uk(self, x, t,f,delta, eps=1e-12): #1e-6\n",
    "      ''' \n",
    "          compute gradient of Moreau envelope\n",
    "      '''\n",
    "\n",
    "      # standard_dev = np.sqrt(delta*t)\n",
    "\n",
    "      # n_features = x.shape[0]\n",
    "      # y = standard_dev * torch.randn(self.int_samples, n_features) + x\n",
    "      \n",
    "      # exp_term = torch.exp(-f(y)/delta)\n",
    "      # v_delta       = torch.mean(exp_term)\n",
    "\n",
    "      # # separate grad_v into two terms for numerical stability\n",
    "      # numerator = y*exp_term.view(self.int_samples, 1)\n",
    "      # numerator = torch.mean(numerator, dim=0)\n",
    "      # grad_uk = (x -  numerator/(v_delta + eps)) # the t gets canceled with the update formula\n",
    "\n",
    "      # uk       = -delta * torch.log(v_delta+eps)\n",
    "\n",
    "      # # Compute Estimated prox_xk\n",
    "      # prox_xk = numerator / (v_delta+eps)\n",
    "\n",
    "      rescale = self.rescale0\n",
    "\n",
    "      v_delta,standard_dev = self.find_rescale_factor(x, t,f,delta,rescale)\n",
    "\n",
    "      loop_iteration=0\n",
    "      while v_delta < epsilon_double:\n",
    "        rescale = rescale/2\n",
    "        v_delta,standard_dev = self.find_rescale_factor(x, t,f,delta,rescale)\n",
    "        loop_iteration+=1\n",
    "      print(f'Loops to find rescale factor: {loop_iteration}')\n",
    "\n",
    "      # Update it\n",
    "      n_features = x.shape[0]\n",
    "      standard_dev = np.sqrt(delta*t/rescale)\n",
    "      \n",
    "      y = standard_dev * torch.randn(self.int_samples, n_features) + x\n",
    "\n",
    "      exp_term = torch.exp(-rescale*f(y)/delta)\n",
    "      v_delta       = torch.mean(exp_term)\n",
    "\n",
    "      # separate grad_v into two terms for numerical stability\n",
    "      numerator = y*exp_term.view(self.int_samples, 1)\n",
    "      numerator = torch.mean(numerator, dim=0)\n",
    "      grad_uk = (x -  numerator/(v_delta)) # the t gets canceled with the update formula\n",
    "\n",
    "      uk       = -delta * torch.log(v_delta)\n",
    "\n",
    "      # Compute Estimated prox_xk\n",
    "      prox_xk = numerator / (v_delta)\n",
    "\n",
    "\n",
    "      # CHECK PROX IS COMPUTING CORRECTLY FOR 1 SAMPLE\n",
    "      # print(' ')\n",
    "      # print(f'prox_xk: {prox_xk}')\n",
    "      # print(f'y: {y}')\n",
    "      # print(f'v_delta: {v_delta}')\n",
    "      # print(f'mean(rescale*f(y)/delta): {torch.mean(rescale*f(y)/delta)}')\n",
    "      # print(f'eps: {eps}')\n",
    "      # print(' ')\n",
    "\n",
    "      return prox_xk, grad_uk, uk\n",
    "\n",
    "    def gradient_descent(self, xk, tk):\n",
    "        # Compute prox and gradient\n",
    "        prox_xk, grad_uk, _ = self.compute_grad_uk(xk, tk,self.f,self.delta)\n",
    "\n",
    "        # Perform gradient descent update\n",
    "        xk_plus1 = xk - self.alpha * (xk-prox_xk)\n",
    "\n",
    "        return xk_plus1, grad_uk\n",
    "\n",
    "    def update_time(self, tk, rel_grad_uk_norm):\n",
    "      '''\n",
    "        time step rule\n",
    "\n",
    "        if ‖gk_plus‖≤ theta (‖gk‖+ eps):\n",
    "          min (eta_plus t,T)\n",
    "        else\n",
    "          max (eta_minus t,t_min) otherwise\n",
    "\n",
    "        OR:\n",
    "        \n",
    "        if rel grad norm too small, increase tk (with maximum T).\n",
    "        else if rel grad norm is too \"big\", decrease tk with minimum (t_min)\n",
    "      '''\n",
    "\n",
    "      eta_minus = self.eta_vec[0]\n",
    "      eta_plus = self.eta_vec[1]\n",
    "      T = self.t_vec[2]\n",
    "      t_min = self.t_vec[1]\n",
    "\n",
    "      if rel_grad_uk_norm <= self.theta:\n",
    "        # increase t when relative gradient norm is smaller than theta\n",
    "        tk = min(eta_plus*tk , T) \n",
    "      else:\n",
    "        # decrease otherwise t when relative gradient norm is smaller than theta\n",
    "        tk = max(eta_minus*tk, t_min)\n",
    "\n",
    "      return tk\n",
    "\n",
    "    def run(self, x0):\n",
    "\n",
    "      n_features            = x0.shape[0]\n",
    "\n",
    "      xk_hist               = torch.zeros(self.max_iters, n_features)\n",
    "      xk_error_hist         = torch.zeros(self.max_iters)\n",
    "      rel_grad_uk_norm_hist = torch.zeros(self.max_iters)\n",
    "      fk_hist               = torch.zeros(self.max_iters)\n",
    "      tk_hist               = torch.zeros(self.max_iters)\n",
    "      counter               = 1\n",
    "\n",
    "      xk    = x0\n",
    "      x_opt = xk\n",
    "\n",
    "      if self.momentum is not None:\n",
    "        xk_minus_1 = xk\n",
    "\n",
    "      tk    = self.t_vec[0]\n",
    "      t_max = self.t_vec[2]\n",
    "\n",
    "      # For Accelerated GD when k=0, yk = xk\n",
    "      #first_moment, _       = self.compute_grad_uk(xk, tk, self.f, self.delta)\n",
    "      \n",
    "      rel_grad_uk_norm      = 1.0\n",
    "\n",
    "      fmt = '[{:3d}]: fk = {:6.2e} | xk_err = {:6.2e} '\n",
    "      fmt += ' | |grad_uk| = {:6.2e} | tk = {:6.2e}'\n",
    "\n",
    "      print('-------------------------- RUNNING HJ-MAD ---------------------------')\n",
    "      print('dimension = ', dim, 'n_samples = ', self.int_samples)\n",
    "\n",
    "      _, grad_uk, _ = self.compute_grad_uk(xk, tk,self.f,self.delta)\n",
    "\n",
    "      for k in range(self.max_iters):\n",
    "\n",
    "        xk_hist[k,:]    = xk\n",
    "\n",
    "        rel_grad_uk_norm_hist[k]  = rel_grad_uk_norm\n",
    "\n",
    "        xk_error_hist[k] = torch.norm(xk - self.x_true)\n",
    "        tk_hist[k]       = tk\n",
    "\n",
    "        fk_hist[k]       = self.f(xk.view(1, n_features))\n",
    "\n",
    "        if self.verbose:\n",
    "          print(fmt.format(k+1, fk_hist[k], xk_error_hist[k], rel_grad_uk_norm_hist[k], tk))\n",
    "\n",
    "        if xk_error_hist[k] < self.tol:\n",
    "          tk_hist = tk_hist[0:k+1]\n",
    "          xk_hist = xk_hist[0:k+1,:]\n",
    "          xk_error_hist = xk_error_hist[0:k+1]\n",
    "          rel_grad_uk_norm_hist = rel_grad_uk_norm_hist[0:k+1]\n",
    "          fk_hist               = fk_hist[0:k+1]\n",
    "          print('HJ-MAD converged with rel grad norm {:6.2e}'.format(rel_grad_uk_norm_hist[k]))\n",
    "          print('iter = ', k, ', number of function evaluations = ', len(xk_error_hist)*int_samples)\n",
    "          break\n",
    "        elif k==self.max_iters-1:\n",
    "          print('HJ-MAD failed to converge with rel grad norm {:6.2e}'.format(rel_grad_uk_norm_hist[k]))\n",
    "          print('iter = ', k, ', number of function evaluations = ', len(xk_error_hist)*int_samples)\n",
    "          print('Used fixed time = ', self.fixed_time)\n",
    "\n",
    "        if k>0:\n",
    "          if fk_hist[k] < fk_hist[k-1]:\n",
    "            x_opt = xk \n",
    "\n",
    "\n",
    "       # _, grad_uk, _ = self.compute_grad_uk(xk, tk,self.f,self.delta)\n",
    "        \n",
    "        grad_uk_norm_old  = torch.norm(grad_uk)\n",
    "\n",
    "        # Accelerate Gradient Descent if momentum is not none\n",
    "        if self.momentum is not None and k>0: # when k=0 go to else with as x_{-1} = x_0\n",
    "          yk = xk + self.momentum * (xk - xk_minus_1)\n",
    "          xk_minus_1 = xk\n",
    "        else:\n",
    "          yk=xk\n",
    "\n",
    "        # Perform Gradient Descent\n",
    "        xk, grad_uk = self.gradient_descent(yk,tk)\n",
    "\n",
    "        # Compute Relative Gradients\n",
    "        grad_uk_norm      = torch.norm(grad_uk)\n",
    "        rel_grad_uk_norm  = grad_uk_norm / (grad_uk_norm_old + 1e-12)\n",
    "\n",
    "\n",
    "        # Update tk\n",
    "        if self.fixed_time == False:\n",
    "          tk = self.update_time(tk, rel_grad_uk_norm)\n",
    "\n",
    "      return x_opt, xk_hist, tk_hist, xk_error_hist, rel_grad_uk_norm_hist, fk_hist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gvAxgASfH66"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Pure Random Search\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class PRS:\n",
    "    ''' \n",
    "        Pure Random Search is used to solve nonconvex minimization problems.\n",
    "        \n",
    "        Inputs:\n",
    "          1) f            = function to be minimized. Inputs have size (n_samples x n_features). Outputs have size n_samples\n",
    "          2) x_true       = true global minimizer\n",
    "          3) int_samples  = number of samples used at each iter\n",
    "    '''\n",
    "\n",
    "    def __init__(self, f, x_true, int_samples=100, max_iters=1000, \n",
    "                 tol=5e-2, domain_boundary=10, verbose = True):\n",
    "      \n",
    "        self.f            = f\n",
    "        self.int_samples  = int_samples\n",
    "        self.max_iters    = max_iters\n",
    "        self.tol          = tol\n",
    "        self.x_true       = x_true\n",
    "        self.domain_boundary = domain_boundary\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def run(self, x0):\n",
    "\n",
    "      print('------------------------------ RUNNING PRS ------------------------------\\n')\n",
    "      n_features = x0.shape[0]\n",
    "      xk_hist    = torch.zeros(self.max_iters, n_features)\n",
    "      xk_error_hist   = torch.zeros(self.max_iters)\n",
    "      fk_hist   = torch.zeros(self.max_iters)\n",
    "      xk         = x0\n",
    "\n",
    "      fmt = '[{:3d}]: fk = {:6.2e} | xk_err = {:6.2e} '\n",
    "\n",
    "      for k in range(self.max_iters):\n",
    "\n",
    "        xk_hist[k,:]      = xk\n",
    "        fk_hist[k]        = self.f(xk.view(1, n_features)) \n",
    "        xk_error_hist[k]  = torch.norm(xk - self.x_true)\n",
    "\n",
    "        if xk_error_hist[k] < self.tol:\n",
    "          xk_hist       = xk_hist[0:k+1,:]\n",
    "          xk_error_hist = xk_error_hist[0:k+1]\n",
    "          fk_hist       = fk_hist[0:k+1]\n",
    "          print('PRS converged')\n",
    "          print('number of function evaluations = ', len(xk_error_hist)*int_samples)\n",
    "          break\n",
    "        elif k==self.max_iters-1:\n",
    "          print('PRS failed to converge')\n",
    "          print('number of function evaluations = ', len(xk_error_hist)*int_samples)\n",
    "\n",
    "        if self.verbose:\n",
    "          print(fmt.format(k+1, fk_hist[k], xk_error_hist[k]))\n",
    "        x_samples = 2*self.domain_boundary*torch.rand(self.int_samples, n_features, dtype=torch.double) - self.domain_boundary\n",
    "        x_samples = torch.cat((x_samples, xk.view(1,n_features)))\n",
    "\n",
    "        min_index = torch.argmin(f(x_samples))\n",
    "        xk            = x_samples[min_index,:]\n",
    "\n",
    "      return xk, xk_hist, xk_error_hist, fk_hist\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Gradient Descent\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class Grad_Descent:\n",
    "  def __init__(self, f, x_true, max_iters=1000, tol=1e-3, step_size=1e-1, verbose = True):\n",
    "    '''\n",
    "      Gradient Descent class for solving nonconvex minimization problems\n",
    "\n",
    "      Inputs: \n",
    "        1) f            = function to be minimized. Inputs have size (n_samples x n_features). Outputs have size n_samples\n",
    "        2) x_true       = true global minimizer\n",
    "        3) max_iters    = max number of iterations\n",
    "        4) tol          = stopping tolerance\n",
    "        5) step_size    = step size\n",
    "    '''\n",
    "\n",
    "    self.f = f\n",
    "    self.max_iters    = max_iters\n",
    "    self.tol          = tol\n",
    "    self.step_size    = step_size\n",
    "    self.verbose      = verbose\n",
    "\n",
    "  def run(self, x0):\n",
    "    \n",
    "    print('------------------------------ RUNNING Gradient Descent ------------------------------\\n')\n",
    "\n",
    "    fmt = '[{:3d}]: fk = {:6.2e} | xk_err = {:6.2e} | |grad_fk| = {:6.2e} '\n",
    "    \n",
    "    xk                  = x0.clone()\n",
    "    n_features          = x0.shape[0]\n",
    "    xk_hist             = torch.zeros(self.max_iters, n_features) \n",
    "    rel_grad_norm_hist  = torch.zeros(self.max_iters)\n",
    "    xk_error_hist       = torch.zeros(self.max_iters)\n",
    "    fk_hist             = torch.zeros(self.max_iters)\n",
    "    \n",
    "    xk.requires_grad_(True)\n",
    "    f_val   = self.f(xk.view(1, n_features))\n",
    "\n",
    "    grad_fk = torch.autograd.grad(outputs=f_val, \n",
    "                                  inputs=xk, grad_outputs=torch.ones(f_val.size()), only_inputs=True)[0].detach()\n",
    "    grad_fk = grad_fk.view(xk.shape)\n",
    "    grad_fk_norm        = torch.norm(grad_fk)\n",
    "    grad_fk_norm0       = grad_fk_norm.clone() \n",
    "    xk.detach()\n",
    "\n",
    "    for k in range(self.max_iters):\n",
    "\n",
    "      xk_hist[k]            = xk\n",
    "      rel_grad_norm_hist[k] = grad_fk_norm/grad_fk_norm0\n",
    "      xk_error_hist[k]      = torch.norm(xk - x_true)\n",
    "      fk_hist[k]            = f_val.detach()\n",
    "\n",
    "      # if rel_grad_uk_norm < tol:\n",
    "      if rel_grad_norm_hist[k] <= self.tol:\n",
    "        rel_grad_norm_hist  = rel_grad_norm_hist[0:k+1]\n",
    "        xk_hist             = xk_hist[0:k+1,:]\n",
    "        xk_error_hist       = xk_error_hist[0:k+1]\n",
    "        fk_hist             = fk_hist[0:k+1]\n",
    "        print('Gradient Descent converged with rel grad norm {:6.2e}'.format(rel_grad_norm_hist[k]))\n",
    "        print('number of function evaluations = ', len(xk_error_hist)*int_samples)\n",
    "        break\n",
    "      elif k==self.max_iters-1:\n",
    "        print('Gradient Descent failed to converge with rel grad norm {:6.2e}'.format(rel_grad_norm_hist[k]))\n",
    "        print('number of function evaluations = ', len(xk_error_hist)*int_samples)\n",
    "\n",
    "      if self.verbose:\n",
    "        print(fmt.format(k+1, fk_hist[k], xk_error_hist[k], rel_grad_norm_hist[k]))\n",
    "\n",
    "      xk.requires_grad_(True)\n",
    "      f_val = self.f(xk.view(1, n_features))\n",
    "\n",
    "      grad_fk     = torch.autograd.grad(outputs=f_val, \n",
    "                                          inputs=xk, grad_outputs=torch.ones(f_val.size()), only_inputs=True)[0].detach()\n",
    "      grad_fk     = grad_fk.view(xk.shape)\n",
    "      grad_fk_norm  = torch.norm(grad_fk)\n",
    "      xk.detach()\n",
    "\n",
    "      xk = xk - self.step_size * grad_fk\n",
    "\n",
    "    return xk.detach(), xk_hist.detach(), xk_error_hist.detach(), rel_grad_norm_hist, fk_hist.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb--RgyjfPAq"
   },
   "source": [
    "### Set up hyperparameters for HJ-MAD for different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbHcFJ3e6Qfa"
   },
   "outputs": [],
   "source": [
    "# Default values\n",
    "delta         = 5e-3\n",
    "max_iters     = int(1e5)\n",
    "tol           = 5e-2\n",
    "momentum      = 0.5\n",
    "rescale0      = 1e-1\n",
    "# Set the number of trials to run\n",
    "avg_trials = 1\n",
    "\n",
    "# def f(x):\n",
    "#   return MultiMinimaFunc(x)\n",
    "# def f_numpy(x):\n",
    "#   return MultiMinimaFunc_numpy(x)\n",
    "# ax_bry  = 30\n",
    "# f_name  = 'MultiMinimaFunc'\n",
    "# dim = 1; int_samples = int(100);\n",
    "# x0      = -30*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = -1.51034569*torch.ones(dim, dtype=torch.double)\n",
    "\n",
    "# delta         = 0.1\n",
    "# max_iters     = int(100)\n",
    "# tol           = 1e-3\n",
    "# momentum = 0.5\n",
    "\n",
    "# theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# beta          = 0.0\n",
    "# t_min     = 1e-1\n",
    "# t_max     = 300\n",
    "# t_init    = 220\n",
    "# alpha     = 0.1\n",
    "# eta_min = 0.99\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def f(x):\n",
    "#   return Griewank(x)\n",
    "# def f_numpy(x):\n",
    "#   return Griewank_numpy(x)\n",
    "# ax_bry  = 20\n",
    "# f_name  = 'Griewank'\n",
    "# dim = 2; int_samples = int(1000000);#int(1000000);\n",
    "# x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "\n",
    "# delta         = 1e-6\n",
    "# max_iters     = int(1e5)\n",
    "# tol           = 5e-2\n",
    "# momentum = 0.64\n",
    "\n",
    "# theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# beta          = 0.9\n",
    "# t_min     = 1e-1/delta\n",
    "# t_max     = int(2e1)/delta\n",
    "# t_init    = 1e-1/delta\n",
    "# alpha     = 5e-2\n",
    "# eta_min = 0.99\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------\n",
    "# rescale0=1e-2\n",
    "\n",
    "\n",
    "# def f(x):\n",
    "#   return Griewank(x)\n",
    "# def f_numpy(x):\n",
    "#   return Griewank_numpy(x)\n",
    "# dim = 100; int_samples = int(1000000); # this one has higher dimension\n",
    "# x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "\n",
    "# theta     = 1.0 # note: larger theta => easier to increase time\n",
    "# beta      = 0.9\n",
    "# t_min     = 1e-1/delta\n",
    "# t_max     = int(2e1)/delta\n",
    "# t_init    = 1e-1/delta\n",
    "# alpha     = 1.0\n",
    "# eta_min = 0.5\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def f(x):\n",
    "#   return Drop_Wave(x)\n",
    "# def f_numpy(x):\n",
    "#   return Drop_Wave_numpy(x)\n",
    "# ax_bry  = 20\n",
    "# f_name  = 'Drop_Wave'\n",
    "\n",
    "# dim = 2; int_samples = int(1000000);\n",
    "# x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "\n",
    "# momentum      = 0.91\n",
    "\n",
    "# theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# beta          = 0.8\n",
    "# t_min     = 1e-6\n",
    "# t_max     = int(2e1)/delta\n",
    "# t_init    = 1e3\n",
    "# alpha     = 1\n",
    "# eta_min = 0.5\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def f(x):\n",
    "#   return AlpineN1(x)\n",
    "# def f_numpy(x):\n",
    "#   return AlpineN1_numpy(x)\n",
    "# ax_bry  = 20\n",
    "# f_name  = 'AlpineN1'\n",
    "\n",
    "# dim = 2; int_samples = int(50);\n",
    "# x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "\n",
    "# momentum      = 0.45\n",
    "\n",
    "# theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# beta          = 0.0\n",
    "# t_max     = int(2e1)/delta\n",
    "# t_init    = 1e-3\n",
    "# t_min     = t_init\n",
    "# alpha     = 0.25\n",
    "# eta_min = 0.6\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "def f(x):\n",
    "  return Levy(x)\n",
    "def f_numpy(x):\n",
    "  return Levy_numpy(x)\n",
    "ax_bry  = 20\n",
    "f_name  = 'Levy'\n",
    "\n",
    "# Set the number of trials to run\n",
    "\n",
    "tol           = 5e-2\n",
    "\n",
    "\n",
    "dim = 2; int_samples = int(1000000);\n",
    "x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "x_true  = torch.ones(dim, dtype=torch.double)\n",
    "\n",
    "theta         = 0.9 # note: larger theta => easier to increase time\n",
    "beta          = 0.5\n",
    "t_max     = int(2e2)/delta\n",
    "t_init    = 100\n",
    "t_min     = t_init\n",
    "alpha     = 0.25\n",
    "eta_min = 0.6\n",
    "eta_plus = 1.5\n",
    "eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def f(x):\n",
    "#   return Rastrigin(x)\n",
    "# def f_numpy(x):\n",
    "#   return Rastrigin_numpy(x)\n",
    "# ax_bry  = 20\n",
    "# f_name  = 'Rastrigin'\n",
    "\n",
    "# dim = 2; int_samples = int(1000000);\n",
    "# x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "# momentum      = 0.25\n",
    "# theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# beta          = 0.5\n",
    "# t_max     = int(2e1)/delta\n",
    "# t_init    = 5.0\n",
    "# t_min     = t_init\n",
    "# alpha     = 0.5\n",
    "# eta_min = 0.5\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def f(x):\n",
    "#   return Ackley(x)\n",
    "# def f_numpy(x):\n",
    "#   return Ackley_numpy(x)\n",
    "# ax_bry  = 20\n",
    "# f_name  = 'Ackley'\n",
    "\n",
    "# dim = 2; int_samples = int(100000);\n",
    "# x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "# momentum      = 0.25\n",
    "# theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# beta          = 0.9\n",
    "# t_max     = int(2e1)/delta\n",
    "# t_init    = 1e-3\n",
    "# t_min     = t_init\n",
    "# alpha     = 5e-1\n",
    "# eta_min = 0.5\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-AcVUFHfahc"
   },
   "source": [
    "Run HJ-MAD and average its results over avg_trials trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HJ_MAD_alg_acc = HJ_MAD(f, x_true, delta=delta,\n",
    "                    int_samples=int_samples, t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False, verbose=True,rescale0=rescale0,momentum=momentum)\n",
    "\n",
    "\n",
    "# Initialize accumulators for averages\n",
    "avg_func_evals = 0\n",
    "sum_elapsed_time = 0\n",
    "total_iterations = 0  # To store total iterations across trials\n",
    "\n",
    "# Run the specified number of trials\n",
    "for _ in range(avg_trials):\n",
    "    start_time = time.time()  # Record the start time\n",
    "\n",
    "    # Execute the HJ_MAD algorithm and retrieve results\n",
    "    x_opt_MAD_acc, xk_hist_MAD_acc, tk_hist_MAD_acc, xk_error_hist_MAD_acc, rel_grad_uk_norm_hist_MAD_acc, fk_hist_MAD_acc= HJ_MAD_alg_acc.run(x0)\n",
    "\n",
    "    elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "    sum_elapsed_time += elapsed_time  # Accumulate elapsed time\n",
    "\n",
    "    total_iterations += len(xk_error_hist_MAD_acc)  # Add iterations used in this trial\n",
    "    avg_func_evals += len(xk_error_hist_MAD_acc) * int_samples  # Update average function evaluations\n",
    "\n",
    "    print(f\"Elapsed time: {elapsed_time:.4f} seconds\")  # Print elapsed time for the current trial\n",
    "\n",
    "# Compute averages after all trials\n",
    "avg_func_evals /= avg_trials  # Average function evaluations per trial\n",
    "average_iterations = total_iterations / avg_trials  # Average number of iterations per trial\n",
    "\n",
    "# Output results\n",
    "print('\\n\\n avg_func_evals = ', avg_func_evals)\n",
    "print(f\"Average iterations before convergence: {average_iterations:.2f}\")\n",
    "print(f\"Average elapsed time: {sum_elapsed_time / avg_trials:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73eSi37bA9ak",
    "outputId": "8649873a-41e6-4860-b70e-762d57c05cdb"
   },
   "outputs": [],
   "source": [
    "HJ_MAD_alg = HJ_MAD(f, x_true, delta=delta,\n",
    "                    int_samples=int_samples, t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False, verbose=True,rescale0=rescale0)\n",
    "\n",
    "# Initialize accumulators for averages\n",
    "avg_func_evals = 0\n",
    "sum_elapsed_time = 0\n",
    "total_iterations = 0  # To store total iterations across trials\n",
    "\n",
    "# Run the specified number of trials\n",
    "for _ in range(avg_trials):\n",
    "    start_time = time.time()  # Record the start time\n",
    "\n",
    "    # Execute the HJ_MAD algorithm and retrieve results\n",
    "    x_opt_MAD, xk_hist_MAD, tk_hist_MAD, xk_error_hist_MAD, rel_grad_uk_norm_hist_MAD, fk_hist_MAD = HJ_MAD_alg.run(x0)\n",
    "\n",
    "    elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "    sum_elapsed_time += elapsed_time  # Accumulate elapsed time\n",
    "\n",
    "    total_iterations += len(xk_error_hist_MAD)  # Add iterations used in this trial\n",
    "    avg_func_evals += len(xk_error_hist_MAD) * int_samples  # Update average function evaluations\n",
    "\n",
    "    print(f\"Elapsed time: {elapsed_time:.4f} seconds\")  # Print elapsed time for the current trial\n",
    "\n",
    "# Compute averages after all trials\n",
    "avg_func_evals /= avg_trials  # Average function evaluations per trial\n",
    "average_iterations = total_iterations / avg_trials  # Average number of iterations per trial\n",
    "\n",
    "# Output results\n",
    "print('\\n\\n avg_func_evals = ', avg_func_evals)\n",
    "print(f\"Average iterations before convergence: {average_iterations:.2f}\")\n",
    "print(f\"Average elapsed time: {sum_elapsed_time / avg_trials:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eglceMCDhIz4"
   },
   "source": [
    "Run Pure Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpnWQyk7A9fk",
    "outputId": "c88dab72-04d3-42a2-cff2-a1035c8a7267"
   },
   "outputs": [],
   "source": [
    "# domain_boundary_PRS = 30\n",
    "PRS_alg = PRS(f, x_true, int_samples = int_samples, max_iters=max_iters, tol=tol, domain_boundary=30, verbose=False)\n",
    "xk_PRS, xk_hist_PRS, xk_error_hist_PRS, fk_hist_PRS = PRS_alg.run(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1StjgtcLhKpv"
   },
   "source": [
    "Run Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKgUpAlqYlJl",
    "outputId": "39fd280f-1c12-4320-be57-70e8d0763248"
   },
   "outputs": [],
   "source": [
    "GD_alg = Grad_Descent(f, x_true, max_iters=max_iters, tol=tol, step_size = 1e-1, verbose = True)\n",
    "xk_GD, xk_hist_GD, xk_error_hist_GD, rel_grad_fk_norm_hist_GD, fk_hist_GD = GD_alg.run(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dmzq0IFjfRac",
    "outputId": "408905b1-ed6b-4eb8-ad01-5ffc530f1d66"
   },
   "outputs": [],
   "source": [
    "print('function name = ', f_name)\n",
    "print('number of function evaluations for HJ-MAD = ', len(xk_error_hist_MAD)*int_samples)\n",
    "print('number of function evaluations for PRS = ', len(xk_error_hist_PRS)*int_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBT0oupCFD_u"
   },
   "source": [
    "### Generate Convergence Histories and Optimization Path Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "OGEzn9rLTAbV",
    "outputId": "6293e921-9c2b-42fb-cf53-d3f728623162"
   },
   "outputs": [],
   "source": [
    "my_blue = '#1f77b4'\n",
    "# if dim > 1:\n",
    "\n",
    "#   surface_plot_resolution = 50\n",
    "#   x = np.linspace(-ax_bry, ax_bry, surface_plot_resolution)\n",
    "#   y = np.linspace(-ax_bry, ax_bry, surface_plot_resolution)\n",
    "\n",
    "#   X, Y = np.meshgrid(x, y)\n",
    "#   n_features = 2\n",
    "\n",
    "#   t_final = t_max\n",
    "\n",
    "#   Z                 = np.zeros(X.shape)\n",
    "#   Z_MAD             = np.zeros(X.shape)\n",
    "\n",
    "#   for i in range(X.shape[0]):\n",
    "#     for j in range(X.shape[1]):\n",
    "#       Z[i,j] = f(torch.FloatTensor([X[i,j],Y[i,j]]).view(1,n_features))  \n",
    "#       _,temp, Z_MAD[i,j] = HJ_MAD_alg.compute_grad_uk(torch.FloatTensor([X[i,j],Y[i,j]]).view(1,2), t_final, f, delta)\n",
    "\n",
    "#   fig, ax = plt.subplots(1, 1)\n",
    "#   im = ax.contourf(X, Y, Z, 20, cmap=plt.get_cmap('gray'))\n",
    "#   plt.style.use('default')\n",
    "\n",
    "#   title_fontsize = 22\n",
    "#   fontsize       = 15\n",
    "\n",
    "#   ax.plot(xk_hist_MAD[:,0], xk_hist_MAD[:,1], '-o', color=my_blue)\n",
    "#   # ax.plot(xk_hist_EGD[:,0], xk_hist_EGD[:,1], 'm-o')\n",
    "#   ax.plot(xk_hist_GD[:,0], xk_hist_GD[:,1], 'g-o')\n",
    "\n",
    "#   ax.plot(x_true[0], x_true[1], 'rx', markeredgewidth=3, markersize=12)\n",
    "#   ax.plot(x0[0], x0[1], 'kx', markeredgewidth=3, markersize=12)\n",
    "#   # ax.plot(7.5, 7.5, 'rx', markeredgewidth=2.5, markersize=10)\n",
    "#   # ax.scatter(x_true[0], x_true[1], linewidth=30, color='red', marker=x')\n",
    "\n",
    "#   ax.legend(['HJ-MAD', 'GD', 'global min', 'initial guess'], fontsize=12, facecolor='white', markerfirst=False, loc='lower right')\n",
    "\n",
    "#   ax.set_xlim(-ax_bry,ax_bry)\n",
    "#   cb = plt.colorbar(im)\n",
    "\n",
    "#   save_loc = 'optimization_paths.png'\n",
    "#   plt.savefig(save_loc,bbox_inches='tight')\n",
    "#   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "gHiwldOKLkKT",
    "outputId": "e50a8767-a37d-4e8a-965d-9cf9349bf15e"
   },
   "outputs": [],
   "source": [
    "title_fontsize = 22\n",
    "fontsize       = 18\n",
    "fig1 = plt.figure()\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "ax = plt.axes()\n",
    "ax.semilogy(xk_error_hist_MAD, color='red', linewidth=3)\n",
    "ax.semilogy(xk_error_hist_MAD_acc, color=my_blue, linewidth=3);\n",
    "# ax.semilogy(xk_error_hist_EGD[0:len(xk_error_hist_GD)], 'm-', linewidth=3)\n",
    "ax.semilogy(xk_error_hist_GD[0:len(xk_error_hist_GD)], 'g-', linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Iterations\", fontsize=title_fontsize)\n",
    "ax.legend(['HJ-MAD','HJ-MAD-Accelerated', 'GD'],fontsize=fontsize)\n",
    "# title_str = 'Relative Errors'\n",
    "# ax.set_title(title_str, fontsize=title_fontsize)\n",
    "ax.tick_params(labelsize=fontsize, which='both', direction='in')\n",
    "\n",
    "save_str = 'griewank_error_hist.png'\n",
    "fig1.savefig(save_str, dpi=300 , bbox_inches=\"tight\", pad_inches=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "3cYDR5eYFNFm",
    "outputId": "2d3f6069-c74a-4ae7-f9a4-77c1bc19578e"
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "ax = plt.axes()\n",
    "ax.semilogy(fk_hist_MAD, color='red', linewidth=3);\n",
    "ax.semilogy(fk_hist_MAD_acc, color=my_blue, linewidth=3);\n",
    "# ax.semilogy(fk_hist_EGD[0:len(xk_error_hist_GD)], 'm-', linewidth=3)\n",
    "ax.semilogy(fk_hist_GD[0:len(xk_error_hist_GD)], 'g-', linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Iterations\", fontsize=title_fontsize)\n",
    "ax.legend(['HJ-MAD','HJ-MAD-Accelerated', 'GD'],fontsize=fontsize)\n",
    "# title_str = 'Objective Function Values'\n",
    "# ax.set_title(title_str, fontsize=title_fontsize)\n",
    "ax.tick_params(labelsize=fontsize, which='both', direction='in')\n",
    "\n",
    "save_str = 'griewank_func_hist.png'\n",
    "fig1.savefig(save_str, dpi=300 , bbox_inches=\"tight\", pad_inches=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "Y1z8FamzEOz3",
    "outputId": "ed0f5eb0-821b-4bf0-ac2a-c8e22db9627c"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(2,2,1)\n",
    "plt.semilogy(xk_error_hist_MAD); plt.title('err hist')\n",
    "plt.semilogy(xk_error_hist_MAD_acc); plt.legend(['HJ-MAD','HJ-MAD-Accelerated'])\n",
    "plt.subplot(2,2,2)\n",
    "plt.semilogy(fk_hist_MAD); plt.title('fk hist')\n",
    "plt.semilogy(fk_hist_MAD_acc); plt.legend(['HJ-MAD','HJ-MAD-Accelerated'])\n",
    "plt.subplot(2,2,3)\n",
    "plt.semilogy(tk_hist_MAD); plt.title('tk hist')\n",
    "plt.semilogy(tk_hist_MAD_acc); plt.legend(['HJ-MAD','HJ-MAD-Accelerated'])\n",
    "plt.subplot(2,2,4)\n",
    "plt.semilogy(rel_grad_uk_norm_hist_MAD); plt.title('rel grad norm hist')\n",
    "plt.semilogy(rel_grad_uk_norm_hist_MAD_acc); plt.legend(['HJ-MAD','HJ-MAD-Accelerated'])\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=1.2, wspace=None, hspace=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GJD4wIN3rZe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC1VgT3pUBhV"
   },
   "source": [
    "### Save values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xiGr475SrnN"
   },
   "outputs": [],
   "source": [
    "filename = 'rel_errs_MAD.dat'\n",
    "with open(filename, 'w') as csv_file:\n",
    "  for idx, f_val in enumerate(xk_error_hist_MAD):\n",
    "    csv_file.write('%0.5e %0.5e\\n' % (idx, f_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PTJW97u3rcO"
   },
   "outputs": [],
   "source": [
    "filename = 'rel_errs_GD.dat'\n",
    "with open(filename, 'w') as csv_file:\n",
    "  for idx, f_val in enumerate(xk_error_hist_GD):\n",
    "    csv_file.write('%0.5e %0.5e\\n' % (idx, f_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGHQozIC3req"
   },
   "outputs": [],
   "source": [
    "filename = 'f_hist_MAD.dat'\n",
    "with open(filename, 'w') as csv_file:\n",
    "  for idx, f_val in enumerate(fk_hist_MAD):\n",
    "    csv_file.write('%0.5e %0.5e\\n' % (idx, f_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlSjIo-z3rg3"
   },
   "outputs": [],
   "source": [
    "filename = 'f_hist_GD.dat'\n",
    "with open(filename, 'w') as csv_file:\n",
    "  for idx, f_val in enumerate(fk_hist_GD):\n",
    "    csv_file.write('%0.5e %0.5e\\n' % (idx, f_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0_B6nXf3rjE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCSv8wJnIWO_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnWYfPeIR15H"
   },
   "source": [
    "## Benchmark Functions\n",
    "\n",
    "We also try some of scipy's global optimization algorithms on our test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZ3FgWW4SSZ2"
   },
   "outputs": [],
   "source": [
    "if dim >1:\n",
    "  from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "  ax_bry_3D_plot = 4\n",
    "  surface_plot_resolution = 50\n",
    "  x = np.linspace(-ax_bry_3D_plot, ax_bry_3D_plot, surface_plot_resolution)\n",
    "  y = np.linspace(-ax_bry_3D_plot, ax_bry_3D_plot, surface_plot_resolution)\n",
    "\n",
    "  X, Y = np.meshgrid(x, y)\n",
    "\n",
    "  Z     = np.zeros(X.shape)\n",
    "  for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "      # convert to torch tensor and reshape to n_samples x n_features\n",
    "      current_coordinate = torch.FloatTensor([X[i,j],Y[i,j]]).view(1, -1)\n",
    "      Z[i,j] = f(current_coordinate)\n",
    "\n",
    "  ax = plt.axes(projection='3d')\n",
    "  ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                  cmap='viridis',\n",
    "                  edgecolor='none')\n",
    "  ax.view_init(60, 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVLjGnWsR7bq"
   },
   "source": [
    "## Run Optimization Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuK4RmgP1Scv"
   },
   "outputs": [],
   "source": [
    "max_iter = int(5e4)\n",
    "basin_hopping_local_niter = int_samples # 100 iterations at each local minimization\n",
    "bounds = [(-20, 20), (-20, 20)]\n",
    "res_basin_hopping   = optimize.basinhopping(f_numpy, np.asarray(x0), niter=basin_hopping_local_niter)\n",
    "res_DE              = optimize.differential_evolution(f_numpy, bounds, maxiter=max_iter)\n",
    "res_dual_annealing  = optimize.dual_annealing(f_numpy, bounds, maxiter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJ02I6dhRFPG"
   },
   "outputs": [],
   "source": [
    "res_DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6j3mIVluHVkQ"
   },
   "outputs": [],
   "source": [
    "res_basin_hopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJ0XIvkq-kxW"
   },
   "outputs": [],
   "source": [
    "res_dual_annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olgPn_jckLxv"
   },
   "outputs": [],
   "source": [
    "print('\\n\\n avg_func_evals = ', avg_func_evals)\n",
    "print(f\"Average iterations before convergence: {average_iterations:.2f}\")\n",
    "print(f\"Average elapsed time: {sum_elapsed_time / avg_trials:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
