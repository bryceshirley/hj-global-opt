{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from ipywidgets import FloatSlider, Button, Output, VBox, HTML, HBox, Dropdown, interact, Output\n",
    "from IPython.display import Image, display, clear_output\n",
    "\n",
    "from test_functions1D import MultiMinimaFunc_numpy, Sinc_numpy, Sin_numpy\n",
    "from test_functions1D import MultiMinimaFunc, Sinc, Sin\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# HJ Moreau Adaptive Descent\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class HJ_MAD:\n",
    "    ''' \n",
    "        Hamilton-Jacobi Moreau Adaptive Descent (HJ_MAD) for nonconvex minimization.\n",
    "        Inputs and Outputs descriptions remain unchanged.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, f, x_true, delta=0.1, int_samples=100, t_vec = [1.0, 1e-3, 1e1], max_iters=5e4, \n",
    "                 tol=5e-2, theta=0.9, beta=[0.9], eta_vec = [0.9, 1.1], alpha=1.0, fixed_time=False, \n",
    "                 verbose=True, accelerated=False, plot_parameters = [-30,30,None]):\n",
    "      \n",
    "        self.delta            = delta\n",
    "        self.f                = f\n",
    "        self.int_samples      = int_samples\n",
    "        self.max_iters        = max_iters\n",
    "        self.tol              = tol\n",
    "        self.t_vec            = t_vec\n",
    "        self.theta            = theta\n",
    "        self.x_true           = x_true\n",
    "        self.beta             = beta \n",
    "        self.alpha            = alpha \n",
    "        self.eta_vec          = eta_vec\n",
    "        self.fixed_time       = fixed_time\n",
    "        self.verbose          = verbose\n",
    "        self.accelerated     = accelerated\n",
    "        # Plotting Parameters\n",
    "        self.plot_parameters = plot_parameters\n",
    "\n",
    "      \n",
    "        # check that alpha is in right interval\n",
    "        assert(alpha >= 1-np.sqrt(eta_vec[0]))\n",
    "        assert(alpha <= 1+np.sqrt(eta_vec[1]))\n",
    "    \n",
    "    def compute_grad_uk(self, x, t, f, delta, eps=1e-12):\n",
    "        ''' \n",
    "            Compute the gradient of the Moreau envelope\n",
    "        '''\n",
    "\n",
    "        standard_dev = np.sqrt(delta * t)\n",
    "        y = standard_dev * np.random.randn(self.int_samples) + x  # No need for shape (self.int_samples, 1)\n",
    "\n",
    "        exp_term = np.exp(-f(y) / delta)\n",
    "        v_delta = np.mean(exp_term)\n",
    "\n",
    "        # Compute the numerator correctly\n",
    "        numerator = np.mean(y * exp_term)  # Average over the samples\n",
    "        grad_uk = (x - numerator / (v_delta + eps))  # This should now give a scalar\n",
    "\n",
    "        uk = -delta * np.log(v_delta + eps)\n",
    "\n",
    "        return grad_uk, uk\n",
    "\n",
    "\n",
    "    def update_time(self, tk, rel_grad_uk_norm):\n",
    "        '''\n",
    "            time step rule\n",
    "\n",
    "            if ‖gk_plus‖≤ theta (‖gk‖+ eps):\n",
    "            min (eta_plus t,T)\n",
    "            else\n",
    "            max (eta_minus t,t_min) otherwise\n",
    "\n",
    "            OR:\n",
    "            \n",
    "            if rel grad norm too small, increase tk (with maximum T).\n",
    "            else if rel grad norm is too \"big\", decrease tk with minimum (t_min)\n",
    "        '''\n",
    "\n",
    "        eta_minus = self.eta_vec[0]\n",
    "        eta_plus = self.eta_vec[1]\n",
    "        T = self.t_vec[2]\n",
    "        t_min = self.t_vec[1]\n",
    "\n",
    "        if rel_grad_uk_norm <= self.theta:\n",
    "            # increase t when relative gradient norm is smaller than theta\n",
    "            tk = min(eta_plus*tk , T)\n",
    "        else:\n",
    "            # decrease otherwise t when relative gradient norm is smaller than theta\n",
    "            tk = max(eta_minus*tk, t_min)\n",
    "\n",
    "        return tk\n",
    "    \n",
    "    def init_first_moment(self,x0):\n",
    "        # For Accelerated GD when k=0, yk = xk\n",
    "        first_moment, _       = self.compute_grad_uk(x0, self.t_vec[0], self.f, self.delta)\n",
    "\n",
    "        return first_moment\n",
    "\n",
    "    def run(self, x0, animate):\n",
    "        xk_hist = np.zeros(self.max_iters)\n",
    "        xk_error_hist = np.zeros(self.max_iters)\n",
    "        rel_grad_uk_norm_hist = np.zeros(self.max_iters)\n",
    "        fk_hist = np.zeros(self.max_iters)\n",
    "        tk_hist = np.zeros(self.max_iters)\n",
    "\n",
    "        xk = x0\n",
    "        x_opt = xk\n",
    "        tk = self.t_vec[0]\n",
    "        t_max = self.t_vec[2]\n",
    "\n",
    "        # For Accelerated GD when k=0, yk = xk\n",
    "        first_moment = self.init_first_moment(x0)\n",
    "        rel_grad_uk_norm = 1.0\n",
    "\n",
    "        fmt = '[{:3d}]: fk = {:6.2e} | xk_err = {:6.2e} '\n",
    "        fmt += ' | |grad_uk| = {:6.2e} | tk = {:6.2e}'\n",
    "\n",
    "        print('-------------------------- RUNNING HJ-MAD ---------------------------')\n",
    "        print('dimension = 1, n_samples = ', self.int_samples)\n",
    "\n",
    "        for k in range(self.max_iters):\n",
    "            if animate:\n",
    "                self.plot(k, x0, xk, tk, first_moment)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            xk_hist[k] = xk\n",
    "\n",
    "            rel_grad_uk_norm_hist[k] = rel_grad_uk_norm\n",
    "\n",
    "            xk_error_hist[k] = np.linalg.norm(xk - self.x_true)\n",
    "            tk_hist[k] = tk\n",
    "\n",
    "            fk_hist[k] = self.f(xk)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(fmt.format(k + 1, fk_hist[k], xk_error_hist[k], rel_grad_uk_norm_hist[k], tk))\n",
    "\n",
    "            if xk_error_hist[k] < self.tol:\n",
    "                tk_hist = tk_hist[:k + 1]\n",
    "                xk_hist = xk_hist[:k + 1]\n",
    "                xk_error_hist = xk_error_hist[:k + 1]\n",
    "                rel_grad_uk_norm_hist = rel_grad_uk_norm_hist[:k + 1]\n",
    "                fk_hist = fk_hist[:k + 1]\n",
    "                print('-------------------------- HJ-MAD RESULTS ---------------------------')\n",
    "                print('HJ-MAD converged with rel grad norm {:6.2e}'.format(rel_grad_uk_norm_hist[k]))\n",
    "                print('iter = ', k, ', number of function evaluations = ', len(xk_error_hist) * self.int_samples)\n",
    "                break\n",
    "            elif k == self.max_iters - 1:\n",
    "                print('-------------------------- HJ-MAD RESULTS ---------------------------')\n",
    "                print('HJ-MAD failed to converge with rel grad norm {:6.2e}'.format(rel_grad_uk_norm_hist[k]))\n",
    "                print('iter = ', k, ', number of function evaluations = ', len(xk_error_hist) * self.int_samples)\n",
    "                print('Used fixed time = ', self.fixed_time)\n",
    "\n",
    "            if k > 0:\n",
    "                if fk_hist[k] < fk_hist[k - 1]:\n",
    "                    x_opt = xk\n",
    "\n",
    "            # --- Start of Edited Section to add Accelerated GD option ---\n",
    "            if self.accelerated and k > 0:  # when k=0 go to else with as x_{-1} = x_0\n",
    "                momentum = 0.1  # *(k-1)/(k+3)\n",
    "                yk = xk + momentum * (xk - xk_hist[k - 1, :])\n",
    "            else:\n",
    "                yk = xk\n",
    "\n",
    "            # Gradient descent (for accelerated update with momentum using yk instead of xk)\n",
    "            grad_uk, _ = self.compute_grad_uk(yk, tk, self.f, self.delta)\n",
    "            grad_uk_norm_old = np.linalg.norm(first_moment)\n",
    "            first_moment = self.beta * first_moment + (1 - self.beta) * grad_uk\n",
    "            grad_uk_norm = np.linalg.norm(first_moment)\n",
    "            rel_grad_uk_norm = grad_uk_norm / (grad_uk_norm_old + 1e-12)\n",
    "\n",
    "            # Update xk\n",
    "            xk = yk - self.alpha * first_moment  # tk gets canceled out with gradient formula\n",
    "            # --- End of Edited Section to add Accelerated GD option ---\n",
    "\n",
    "            if not self.fixed_time:\n",
    "                tk = self.update_time(tk, rel_grad_uk_norm)\n",
    "\n",
    "        self.plot(k, x0,x_opt, tk_hist[-1], first_moment)\n",
    "        display(Image(filename=\"interactive_plot.png\"))\n",
    "        return x_opt, xk_hist, tk_hist, xk_error_hist, rel_grad_uk_norm_hist, fk_hist\n",
    "    \n",
    "\n",
    "    def plot(self, k, x0, xk, tk, first_moment):\n",
    "        intervalx_a, intervalx_b,plot_output = self.plot_parameters\n",
    "\n",
    "        if k == 0:\n",
    "            first_moment = self.init_first_moment(x0)\n",
    "\n",
    "        # Use the selected global function to compute f_values\n",
    "        x_range = np.linspace(intervalx_a, intervalx_b, 1000)  # Adjust based on selected function\n",
    "        f_values = np.array([self.f(x) for x in x_range])\n",
    "\n",
    "        # Compute the Error\n",
    "        error = np.linalg.norm(xk - self.x_true)  # Calculate error using NumPy\n",
    "\n",
    "        # Calculate Moreau envelope at xk\n",
    "        x_hat = xk - tk * first_moment  # Estimated minima of prox\n",
    "        moreau_values = np.array([self.f(x_hat) + (1 / (2 * tk)) * ((x_hat - x) ** 2) for x in x_range])\n",
    "\n",
    "        # The function that prox minimizes\n",
    "        prox_func_values = np.array([self.f(x) + (1 / (2 * tk)) * (x - xk) ** 2 for x in x_range])\n",
    "\n",
    "        # Compute Next Iteration\n",
    "        #xk_plus1 = xk - self.alpha * first_moment\n",
    "\n",
    "        # Plot f(x) and Moreau envelope\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x_range, f_values, label='f(x)', color='black')\n",
    "\n",
    "        # Plot the function that prox minimizes at xk and its estimated minima\n",
    "        plt.plot(x_range, prox_func_values, label=r'$f(x) + \\frac{1}{2T} (x - x_k)^2$', color='orange')\n",
    "        plt.scatter(x_hat, self.f(x_hat) + (1 / (2 * tk)) * (x_hat - xk) ** 2,\n",
    "                    facecolors='none',edgecolors='orange', label=r'$\\hat{x}= x_k-t_k g_k\\approx prox_{tf}(x_k)$ and the Estimated' +\n",
    "                    f'\\nGlobal Minima of ' + r'$f(x) + \\frac{1}{2t_k} (x - x_k)^2$',\n",
    "                    s=100, zorder=4, marker='s')\n",
    "\n",
    "        # Plot Moreau Envelope\n",
    "        plt.plot(x_range, moreau_values, label=rf'Moreau Envelope, $u(x,t_k))$ where $t_k={tk:.1f}$', color='red')\n",
    "        plt.scatter(x_hat, self.f(x_hat), color='red', label=r'Minima of $u(x,t_k))$',s=80, zorder=4, marker='x')\n",
    "\n",
    "        # Plot Points for Current, Next, Initial Iteration, and Global Minima\n",
    "        plt.scatter(xk, self.f(xk), label=r'Current Iteration, $f(x_k)$', zorder=3, marker='^')\n",
    "        plt.scatter(x0, self.f(x0), color='green', label=r'Initial Iteration, $f(x_0)$',s=100, zorder=4, facecolors='none',edgecolors='blue')\n",
    "        plt.scatter(self.x_true, self.f(self.x_true), color='black', label=r'Global Minima, $f(x_{true})$',s=100, zorder=5, marker='x')\n",
    "        #plt.scatter(xk_plus1, self.f(xk_plus1), color='cyan', zorder=4, label=r'Next Iteration, $f(x_{k+1})$', marker='^')\n",
    "\n",
    "        plt.title(f'f(x) and Moreau Envelope\\nIteration {k+1}, Error={error:.3f}, Tol={self.tol}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('Function Value')\n",
    "\n",
    "        # Set limits and grid\n",
    "        plt.xlim(intervalx_a, intervalx_b)\n",
    "\n",
    "        # Dynamically set the y-limits based on function outputs\n",
    "        y_min = np.min(f_values)\n",
    "        y_max = np.max(f_values)\n",
    "\n",
    "        if y_max < 0:\n",
    "            # If all values are negative, set the limits to give some visual space\n",
    "            plt.ylim(1.2 * y_min, 0)  # Set lower limit 20% below min, upper limit at 0\n",
    "        elif y_min > 0:\n",
    "            plt.ylim(0.8 * y_min, 1.2 * y_max)  # 20% less than the min if min is positive\n",
    "        else:\n",
    "            plt.ylim(1.2 * y_min, 1.2 * y_max)  # 20% more than the min if min is zero or negative\n",
    "\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=1)\n",
    "        plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "        plt.minorticks_on()\n",
    "        plt.gca().xaxis.set_minor_locator(plt.MultipleLocator(1))\n",
    "        plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "\n",
    "        # Save the figure as a PNG file\n",
    "        plt.savefig(\"interactive_plot.png\", format='png', bbox_inches='tight')\n",
    "        plt.close()  # Close the plot to free up memory\n",
    "\n",
    "        with plot_output:\n",
    "            clear_output(wait=True)  # Clear previous plot\n",
    "            display(Image(filename=\"interactive_plot.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# UI Class to Interact with HJ_MAD\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class HJ_MAD_UI:\n",
    "    def __init__(self):\n",
    "        self.init_hj_mad()\n",
    "        self.create_ui()\n",
    "\n",
    "    def init_hj_mad(self):\n",
    "        # Output widget to display the plot\n",
    "        self.plot_output = Output()\n",
    "\n",
    "        # Fixed Variables\n",
    "        self.dim = 1\n",
    "        tol = 5e-3\n",
    "        eta_min = 0.5\n",
    "        eta_plus = 5.0\n",
    "        eta_vec = [eta_min, eta_plus]\n",
    "        theta = 1.0\n",
    "        beta = 0.0\n",
    "        t_min = 0.1\n",
    "\n",
    "        # Set Initial Interactive Values\n",
    "        int_samples = int(10)\n",
    "        delta = 5e-1\n",
    "        t_init = 39\n",
    "        t_max = 45\n",
    "        max_iters = int(100)\n",
    "        alpha = 0.5\n",
    "\n",
    "        # Default Function Settings\n",
    "        intervalx_a, intervalx_b = -20, 20\n",
    "        plot_parameters = [intervalx_a, intervalx_b,self.plot_output]\n",
    "        f = MultiMinimaFunc_numpy\n",
    "        x_true = -1.51035 \n",
    "        x0 = ((intervalx_a + intervalx_b) / 2)\n",
    "\n",
    "        # Create HJ_MAD instance with initial parameters\n",
    "        self.hj_mad = HJ_MAD(f=f, x_true=x_true, delta=delta, int_samples=int_samples, \n",
    "                             t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, \n",
    "                             theta=theta, beta=beta, eta_vec=eta_vec, alpha=alpha, \n",
    "                             fixed_time=False, verbose=False, plot_parameters=plot_parameters)\n",
    "        self.hj_mad.plot(0, x0, x0, t_init, 0)\n",
    "\n",
    "        # Initialize x0 slider\n",
    "        self.x_0_slider = FloatSlider(value=x0, min=intervalx_a, max=intervalx_b, step=0.01, description='x_k:')\n",
    "\n",
    "    def update_hj_mad_delta(self, b):\n",
    "        self.hj_mad.delta = self.delta_slider.value\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "    def update_hj_mad_int_samples(self, b):\n",
    "        self.hj_mad.int_samples = int(self.int_samples_slider.value)\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "    def update_hj_mad_t_init(self, b):\n",
    "        self.hj_mad.t_vec[0] = self.t_init_slider.value\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "    def update_hj_mad_t_max(self, b):\n",
    "        self.hj_mad.t_vec[2] = self.t_max_slider.value\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "    def update_hj_mad_max_iters(self, b):\n",
    "        self.hj_mad.max_iters = int(self.max_iters_slider.value)\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "    def update_hj_mad_alpha(self, b):\n",
    "        self.hj_mad.alpha = self.alpha_slider.value\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "    def update_hj_mad_x0(self, b):\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "    def create_ui(self):\n",
    "        # Dropdown to select the function\n",
    "        self.function_dropdown = Dropdown(\n",
    "            options=['Sinc', 'Sin', 'MultiMinima'],\n",
    "            value='MultiMinima',\n",
    "            description='Function:'\n",
    "        )\n",
    "        \n",
    "        # Sliders for parameters, initialized with default values\n",
    "        self.delta_slider = FloatSlider(value=self.hj_mad.delta, min=0, max=5.0, step=0.01, description='Delta:')\n",
    "        self.int_samples_slider = FloatSlider(value=self.hj_mad.int_samples, min=1, max=1001, step=10, description='Int Samples:')\n",
    "        self.t_max_slider = FloatSlider(value=self.hj_mad.t_vec[2], min=0.1, max=100, step=0.1, description='T Max:')\n",
    "        self.t_init_slider = FloatSlider(value=self.hj_mad.t_vec[0], min=0.1, max=100, step=0.1, description='T Init:')\n",
    "        self.max_iters_slider = FloatSlider(value=50, min=1, max=self.hj_mad.max_iters, step=1, description='Max Iterations:')\n",
    "        self.alpha_slider = FloatSlider(value=self.hj_mad.alpha, min=0.1, max=1.9, step=0.1, description='Alpha:')\n",
    "\n",
    "        # Buttons to run optimization\n",
    "        self.run_button = Button(description='Run Optimization')\n",
    "        self.animation_button = Button(description='Run Animation')\n",
    "\n",
    "        # Bind slider changes to specific update methods\n",
    "        self.delta_slider.observe(self.update_hj_mad_delta, names='value')\n",
    "        self.int_samples_slider.observe(self.update_hj_mad_int_samples, names='value')\n",
    "        self.t_init_slider.observe(self.update_hj_mad_t_init, names='value')\n",
    "        self.t_max_slider.observe(self.update_hj_mad_t_max, names='value')\n",
    "        self.max_iters_slider.observe(self.update_hj_mad_max_iters, names='value')\n",
    "        self.alpha_slider.observe(self.update_hj_mad_alpha, names='value')\n",
    "        self.x_0_slider.observe(self.update_hj_mad_x0, names='value')\n",
    "\n",
    "        # Bind button clicks to the respective methods\n",
    "        self.run_button.on_click(lambda b: self.run(animate=False))\n",
    "        self.animation_button.on_click(lambda b: self.run(animate=True))\n",
    "\n",
    "        # Dropdown event listener to update the selected function\n",
    "        self.function_dropdown.observe(self.update_function, names='value')\n",
    "\n",
    "        title = HTML(\"<h2>Moreau Adaptive Descent Visualization (in 1D)</h2>\")\n",
    "        internal_parameters = HTML(\"<h3>Internal Parameters:</h3>\")\n",
    "        initial_conditions = HTML(\"<h3>Initial Conditions Parameters:</h3>\")\n",
    "\n",
    "        # Instructions\n",
    "        instructions = HTML(\"<h4>Instructions:</h4>\"\n",
    "                            \"<p>1. Select a function from the dropdown menu.</p>\"\n",
    "                            \"<p>2. Adjust the parameters using the sliders.</p>\"\n",
    "                            \"<p>3. Click 'Run Optimization' to start the process.</p>\"\n",
    "                            \"<p>4. Click 'Run Animation' to visualize the optimization process over time.</p>\"\n",
    "                            \"<p>5. The plot will update automatically after running the optimization.</p>\")\n",
    "\n",
    "        # Display the UI\n",
    "        ui = VBox([title, internal_parameters, self.function_dropdown, self.delta_slider, \n",
    "                self.int_samples_slider, self.t_max_slider, \n",
    "                self.max_iters_slider, self.alpha_slider, \n",
    "                initial_conditions, self.t_init_slider, self.x_0_slider,\n",
    "                HBox([self.run_button, self.animation_button]), instructions,\n",
    "                self.plot_output])  # Add plot output to the UI\n",
    "        display(ui)\n",
    "\n",
    "    def update_function(self, change):\n",
    "        # Update the selected function based on dropdown selection\n",
    "        if change['new'] == 'Sinc':\n",
    "            intervalx_a, intervalx_b = -20, 20\n",
    "            self.hj_mad.plot_parameters = [intervalx_a, intervalx_b]\n",
    "            self.hj_mad.f = Sinc_numpy\n",
    "            self.hj_mad.x_true = 4.49341\n",
    "        elif change['new'] == 'Sin':\n",
    "            intervalx_a, intervalx_b = -20, 20\n",
    "            self.hj_mad.plot_parameters = [intervalx_a, intervalx_b]\n",
    "            self.hj_mad.f = Sin_numpy\n",
    "            self.hj_mad.x_true = -np.pi / 2\n",
    "        else:  # change['new'] == 'MultiMinimaFunc'\n",
    "            intervalx_a, intervalx_b = -20, 20\n",
    "            self.hj_mad.plot_parameters = [intervalx_a, intervalx_b]\n",
    "            self.hj_mad.f = MultiMinimaFunc_numpy\n",
    "            self.hj_mad.x_true = -1.51035\n",
    "\n",
    "        # Update the slider range for x_0 based on the new function\n",
    "        self.x_0_slider.min = intervalx_a\n",
    "        self.x_0_slider.max = intervalx_b\n",
    "        self.x_0_slider.value = (intervalx_a + intervalx_b) / 2\n",
    "\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "    def run(self, animate):\n",
    "        # Run optimization with current slider values\n",
    "        x0 = self.x_0_slider.value\n",
    "        self.hj_mad.run(x0, animate=animate)\n",
    "        time.sleep(2)\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "\n",
    "\n",
    "    def plot_hj_mad(self):\n",
    "        x0 = self.x_0_slider.value\n",
    "        self.hj_mad.plot(0, x0, x0, self.hj_mad.t_vec[0], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca75498fe564069bd09a87d3a260144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Moreau Adaptive Descent Visualization (in 1D)</h2>'), HTML(value='<h3>Internal …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "UI= HJ_MAD_UI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
