{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from ipywidgets import FloatSlider, Button, Output, VBox, HTML, HBox, Dropdown, interact, Output, Checkbox, FloatText\n",
    "from IPython.display import Image, display, clear_output, Math\n",
    "\n",
    "from test_functions1D import MultiMinimaFunc_numpy, Sinc_numpy, Sin_numpy, DiscontinuousFunc_numpy, MultiMinimaAbsFunc_numpy\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# HJ Moreau Adaptive Descent\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class HJ_MAD:\n",
    "    ''' \n",
    "        Hamilton-Jacobi Moreau Adaptive Descent (HJ_MAD) is used to solve nonconvex minimization\n",
    "        problems via a zeroth-order sampling scheme.\n",
    "        \n",
    "        Inputs:\n",
    "          1)  f            = function to be minimized. Inputs have size (n_samples x n_features). Outputs have size n_samples\n",
    "          2)  x_true       = true global minimizer\n",
    "          3)  delta        = coefficient of viscous term in the HJ equation\n",
    "          4)  int_samples  = number of samples used to approximate expectation in heat equation solution\n",
    "          5)  x_true       = true global minimizer\n",
    "          6)  t_vec        = time vector containig [initial time, minimum time allowed, maximum time]\n",
    "          7)  max_iters    = max number of iterations\n",
    "          8)  tol          = stopping tolerance\n",
    "          9)  theta        = parameter used to update tk\n",
    "          10) beta         = exponential averaging term for gradient beta (beta multiplies history, 1-beta multiplies current grad)\n",
    "          11) eta_vec      = vector containing [eta_minus, eta_plus], where eta_minus < 1 and eta_plus > 1 (part of time update)\n",
    "          11) alpha        = step size. has to be in between (1-sqrt(eta_minus), 1+sqrt(eta_plus))\n",
    "          12) fixed_time   = boolean for using adaptive time\n",
    "          13) verbose      = boolean for printing\n",
    "          14) momentum     = For acceleration.\n",
    "          15) accelerated  = boolean for using Accelerated Gradient Descent\n",
    "\n",
    "        Outputs:\n",
    "          1) x_opt                    = optimal x_value approximation\n",
    "          2) xk_hist                  = update history\n",
    "          3) tk_hist                  = time history\n",
    "          4) fk_hist                  = function value history\n",
    "          5) xk_error_hist            = error to true solution history \n",
    "          6) rel_grad_uk_norm_hist    = relative grad norm history of Moreau envelope\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, f, x_true, x0, delta=0.1, int_samples=100, t_vec = [1.0, 1e-3, 1e1], max_iters=5e4, \n",
    "                 tol=5e-2, theta=0.9, beta=0.0, eta_vec = [0.9, 1.1], alpha=1.0, fixed_time=False, \n",
    "                 verbose=True, accelerated=False,momentum=0.5, plot_parameters = [-30,30,None],\n",
    "                 sample_bool=False, trap_integration=False):\n",
    "      \n",
    "        self.delta            = delta\n",
    "        self.f                = f\n",
    "        self.int_samples      = int_samples\n",
    "        self.max_iters        = max_iters\n",
    "        self.tol              = tol\n",
    "        self.t_vec            = t_vec\n",
    "        self.theta            = theta\n",
    "        self.x_true           = x_true\n",
    "        self.x0               = x0\n",
    "        self.beta             = beta \n",
    "        self.alpha            = alpha \n",
    "        self.eta_vec          = eta_vec\n",
    "        self.fixed_time       = fixed_time\n",
    "        self.verbose          = verbose\n",
    "        self.accelerated      = accelerated\n",
    "        self.trap_integration = trap_integration\n",
    "        self.momentum         = momentum\n",
    "        # Plotting Parameters\n",
    "        self.plot_parameters  = plot_parameters\n",
    "        self.sample_bool      = sample_bool\n",
    "      \n",
    "        # check that alpha is in right interval\n",
    "        assert(alpha >= 1-np.sqrt(eta_vec[0]))\n",
    "        assert(alpha <= 1+np.sqrt(eta_vec[1]))\n",
    "\n",
    "    def find_rescale_factor(self, x, t, initial_rescale_factor=1, epsilon=1e-15, min_rescale=1e-10):\n",
    "        \"\"\"\n",
    "        Finds the optimal rescale factor to ensure that the exponential term remains above epsilon.\n",
    "        \n",
    "        Parameters:\n",
    "        - x : float or np.array\n",
    "            The current point or array of points.\n",
    "        - t : float\n",
    "            Time or other parameter affecting the standard deviation.\n",
    "        - initial_rescale_factor : float, optional\n",
    "            Starting value for the rescale factor.\n",
    "        - epsilon : float, optional\n",
    "            Threshold for the maximum exponential term to be considered valid.\n",
    "        - min_rescale : float, optional\n",
    "            Minimum value for the rescale factor to prevent infinite loops.\n",
    "        \n",
    "        Returns:\n",
    "        - tuple (float, int)\n",
    "            The optimal rescale factor and the number of iterations taken to reach it.\n",
    "        \"\"\"\n",
    "        \n",
    "        rescale_factor = initial_rescale_factor\n",
    "        iterations = 0\n",
    "        \n",
    "        while True:\n",
    "            # Calculate standard deviation and generate random samples\n",
    "            standard_dev = np.sqrt(self.delta * t / rescale_factor)\n",
    "            y = standard_dev * np.random.randn(self.int_samples) + x\n",
    "\n",
    "            # Compute function values and the max exponential term\n",
    "            f_values = self.f(y)\n",
    "            max_f_values = np.max(f_values)\n",
    "            v_delta = np.mean(np.exp(-rescale_factor * f_values / self.delta))\n",
    "\n",
    "\n",
    "            min_exponent = -rescale_factor * max_f_values / self.delta\n",
    "            v_delta_exponent = np.log(v_delta)\n",
    "\n",
    "            # print(f\"{iterations=}\")\n",
    "            # print(f\"{min_exponent=}\")\n",
    "            # print(f\"{v_delta_exponent=}\")\n",
    "            # print(f\"{np.log(epsilon)=}\\n\")\n",
    "\n",
    "            # Check if the maximum exponential term is within the desired range\n",
    "   \n",
    "            if v_delta >= epsilon or rescale_factor < min_rescale:\n",
    "                break\n",
    "\n",
    "            # Adjust rescale factor and increment iteration count\n",
    "            rescale_factor /= 2\n",
    "            iterations += 1\n",
    "\n",
    "        return rescale_factor, iterations\n",
    "    \n",
    "    def compute_grad_uk(self, x: float, t: float, grad_uk_old: Optional[float] = None) -> Tuple[float, float, float, np.ndarray]:\n",
    "        ''' \n",
    "        Compute the gradient of the Moreau envelope and related statistics.\n",
    "\n",
    "        Args:\n",
    "            x (float): Input point for evaluating the gradient.\n",
    "            t (float): Scaling factor for smoothing (typically 'tk').\n",
    "            eps (float, optional): Small constant for numerical stability. Default is 1e-14.\n",
    "            old_grad_uk (Optional[float], optional): Previous gradient for momentum updates. Default is None.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float, float, np.ndarray]: \n",
    "                - grad_uk: Gradient of the Moreau envelope.\n",
    "                - uk: Moreau envelope estimate.\n",
    "                - se_uk: Standard error of 'uk'.\n",
    "                - y: Random samples drawn from N(x, delta * t).\n",
    "        '''\n",
    "        delta = self.delta\n",
    "        f = self.f\n",
    "        \n",
    "        rescale,iterations = self.find_rescale_factor(x, t)\n",
    "        print(f\"{rescale=}\")\n",
    "        print(f\"{iterations=}\")\n",
    "\n",
    "        # Compute the function of the random variable y sampled from N(x,delta*t)\n",
    "        standard_dev = np.sqrt(delta * t/rescale) \n",
    "        y = standard_dev * np.random.randn(self.int_samples) + x  \n",
    "        exp_term = np.exp(-rescale*f(y) / delta)\n",
    "\n",
    "        # Compute Denominator and average over the samples (add eps for 0 error)\n",
    "        v_delta = np.mean(exp_term)\n",
    "\n",
    "         # Compute Numerator and average over the samples\n",
    "        numerator = np.mean(y * exp_term)\n",
    "\n",
    "        # Compute Estimated prox_xk\n",
    "        prox_xk = numerator / v_delta\n",
    "\n",
    "        # Compute Gradient at uk (times tk)\n",
    "        grad_uk = (x - prox_xk)\n",
    "\n",
    "        # Compute estimated uk\n",
    "        uk = -delta * np.log(v_delta)\n",
    "\n",
    "\n",
    "        # Compute the standard error for uk\n",
    "        # Filter out non-positive values from exp_term\n",
    "\n",
    "        valid_exp_term = exp_term[exp_term > 0]\n",
    "        no_valid_exp_terms = len(valid_exp_term)\n",
    "        # Compute the standard error for uk only with valid values\n",
    "        if len(valid_exp_term) > 0:\n",
    "            log_exp_terms = -delta * np.log(valid_exp_term)\n",
    "            sample_var = np.var(log_exp_terms, ddof=1)  # Weighted variance\n",
    "            se_uk = np.sqrt(sample_var / no_valid_exp_terms) \n",
    "        else:\n",
    "            se_uk = 0.0\n",
    "        \n",
    "        # ADAM's Method\n",
    "        if grad_uk_old is not None:\n",
    "            grad_uk = self.beta*grad_uk_old + (1-self.beta)*grad_uk\n",
    "\n",
    "        uk_info = (grad_uk, uk, se_uk, y)\n",
    "\n",
    "        # Return Gradient at uk, uk, prox at xk and standard error in uk sample\n",
    "        return uk_info\n",
    "    \n",
    "    def compute_grad_uk_trapezium(self, x: float, t: float, eps: float = 1e-12, grad_uk_old: Optional[float] = None) -> Tuple[float, float, float, np.ndarray]:\n",
    "        ''' \n",
    "        Compute the gradient of the Moreau envelope using the trapezium rule.\n",
    "\n",
    "        Args:\n",
    "            x (float): Input point for evaluating the gradient.\n",
    "            t (float): Scaling factor for smoothing (typically 'tk').\n",
    "            eps (float, optional): Small constant for numerical stability. Default is 1e-14.\n",
    "            old_grad_uk (Optional[float], optional): Previous gradient for momentum updates. Default is None.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float, np.ndarray]: \n",
    "                - grad_uk: Gradient of the Moreau envelope.\n",
    "                - uk: Moreau envelope estimate.\n",
    "                - y: Discretized points used in trapezium rule.\n",
    "        '''\n",
    "        delta = self.delta\n",
    "        f = self.f\n",
    "        \n",
    "        # Discretize the domain for trapezium rule\n",
    "        a, b = x - 10 * np.sqrt(delta * t), x + 10 * np.sqrt(delta * t)  # Define an interval around x (5 standard deviations)\n",
    "        N = self.int_samples # Number of intervals\n",
    "        y = np.linspace(a, b, N)\n",
    "        h = (b - a) / (N - 1)\n",
    "\n",
    "        # Define the functions to integrate\n",
    "        def integrand_v_delta(y):\n",
    "            \"\"\"Function for v_delta integrand.\"\"\"\n",
    "            return np.exp(-f(y) / delta) * np.exp(-(x - y)**2 / (2 * delta * t))\n",
    "        \n",
    "        def integrand_grad_v_delta(y):\n",
    "            \"\"\"Function for ∇v_delta integrand.\"\"\"\n",
    "            return (x - y) / t * integrand_v_delta(y)\n",
    "\n",
    "        # Calculate v_delta using the trapezium rule\n",
    "        discrete_integrand_v_delta = integrand_v_delta(y)\n",
    "        v_delta = (h / 2) * (discrete_integrand_v_delta[0] + 2 * np.sum(discrete_integrand_v_delta[1:N-1]) + discrete_integrand_v_delta[-1])\n",
    "        \n",
    "        # Calculate ∇v_delta using the trapezium rule\n",
    "        discrete_integrand_grad_v_delta = integrand_grad_v_delta(y)\n",
    "        grad_v_delta = (h / 2) * (discrete_integrand_grad_v_delta[0] + 2 * np.sum(discrete_integrand_grad_v_delta[1:N-1]) + discrete_integrand_grad_v_delta[-1])\n",
    "        \n",
    "        # Compute ∇u_delta using the simplified formula: -(grad_v_delta / v_delta)\n",
    "        grad_uk = grad_v_delta / (v_delta+ eps)\n",
    "\n",
    "        # Compute prox_xk\n",
    "        #prox_xk = x - t*grad_uk\n",
    "\n",
    "        # Compute estimated uk\n",
    "        uk = -delta * np.log(2 * np.pi * t * v_delta+ eps)\n",
    "        \n",
    "        # ADAM's Method\n",
    "        if grad_uk_old is not None:\n",
    "            print(self.beta)\n",
    "            grad_uk = self.beta * grad_uk_old + (1 - self.beta) * grad_uk\n",
    "\n",
    "        uk_info = (t*grad_uk, uk, None, None)\n",
    "\n",
    "        # Return Gradient at uk, uk, prox at xk and standard error in uk sample\n",
    "        return uk_info\n",
    "\n",
    "\n",
    "    def update_time(self, tk, rel_grad_uk_norm):\n",
    "        '''\n",
    "            time step rule\n",
    "\n",
    "            if ‖gk_plus‖≤ theta (‖gk‖+ eps):\n",
    "            min (eta_plus t,T)\n",
    "            else\n",
    "            max (eta_minus t,t_min) otherwise\n",
    "\n",
    "            OR:\n",
    "            \n",
    "            if rel grad norm too small, increase tk (with maximum T).\n",
    "            else if rel grad norm is too \"big\", decrease tk with minimum (t_min)\n",
    "        '''\n",
    "\n",
    "        eta_minus = self.eta_vec[0]\n",
    "        eta_plus = self.eta_vec[1]\n",
    "        T = self.t_vec[2]\n",
    "        t_min = self.t_vec[1]\n",
    "\n",
    "        if rel_grad_uk_norm <= self.theta:\n",
    "            # increase t when relative gradient norm is smaller than theta\n",
    "            tk = min(eta_plus*tk , T)\n",
    "        else:\n",
    "            # decrease otherwise t when relative gradient norm is smaller than theta\n",
    "            tk = max(eta_minus*tk, t_min)\n",
    "\n",
    "        return tk\n",
    "    \n",
    "    def gradient_descent(self, xk: float, tk: float,grad_uk_old: Optional[float] = None) -> Tuple[float, Tuple[float, float, float, np.ndarray]]:\n",
    "        '''\n",
    "        Perform a gradient descent update using the gradient of the Moreau envelope.\n",
    "\n",
    "        Args:\n",
    "            xk (float): Current point (iteration k) where the gradient is evaluated.\n",
    "            tk (float): Time scaling factor (typically 'tk') for smoothing in the gradient computation.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, Tuple[float, float, float, np.ndarray]]:\n",
    "                - xk_plus1 (float): Updated point after the gradient descent step.\n",
    "                - uk_info (Tuple[float, float, float, np.ndarray]): \n",
    "                    - grad_uk (float): Computed gradient of the Moreau envelope.\n",
    "                    - uk (float): Estimated Moreau envelope at 'xk'.\n",
    "                    - se_uk (float): Standard error of the Moreau envelope estimate.\n",
    "                    - y (np.ndarray): Random samples used in the gradient estimation.\n",
    "        '''\n",
    "        # Compute prox and gradient\n",
    "        if self.trap_integration:\n",
    "            uk_info = self.compute_grad_uk_trapezium(xk, tk,grad_uk_old=grad_uk_old)\n",
    "        else:\n",
    "            uk_info = self.compute_grad_uk(xk, tk,grad_uk_old=grad_uk_old)\n",
    "\n",
    "        # uk_info is a tuple such that uk_info[0] = grad_uk\n",
    "\n",
    "        # Perform gradient descent update\n",
    "        xk_plus1 = xk - self.alpha * uk_info[0]\n",
    "\n",
    "        return xk_plus1, uk_info\n",
    "\n",
    "    def run(self, animate,plot_bool=True):\n",
    "        xk_hist = np.zeros(self.max_iters)\n",
    "        xk_error_hist = np.zeros(self.max_iters)\n",
    "        rel_grad_uk_norm_hist = np.zeros(self.max_iters)\n",
    "        fk_hist = np.zeros(self.max_iters)\n",
    "        tk_hist = np.zeros(self.max_iters)\n",
    "\n",
    "        xk = self.x0\n",
    "        x_opt = xk\n",
    "        tk = self.t_vec[0]\n",
    "        t_max = self.t_vec[2]\n",
    "\n",
    "        uk_info  = self.compute_grad_uk(xk, tk)\n",
    "        grad_uk = uk_info[0]\n",
    "        grad_uk_old = None\n",
    "        \n",
    "        rel_grad_uk_norm = 1.0\n",
    "\n",
    "        if self.accelerated:\n",
    "            xk_minus_1 = xk\n",
    "            momentum = self.momentum\n",
    "\n",
    "        if self.beta != 0.0:\n",
    "            grad_uk_old = grad_uk\n",
    "\n",
    "\n",
    "        fmt = '[{:3d}]: fk = {:6.2e} | xk_err = {:6.2e} '\n",
    "        fmt += ' | |grad_uk| = {:6.2e} | tk = {:6.2e}'\n",
    "        for k in range(self.max_iters):\n",
    "            \n",
    "            # Update History\n",
    "            xk_hist[k] = xk\n",
    "            rel_grad_uk_norm_hist[k] = rel_grad_uk_norm\n",
    "            xk_error_hist[k] = np.linalg.norm(xk - self.x_true)\n",
    "            tk_hist[k] = tk\n",
    "            fk_hist[k] = self.f(xk)\n",
    "\n",
    "\n",
    "            if animate:\n",
    "                self.plot(k, xk, tk,xk_error_hist[k],uk_info=uk_info)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(fmt.format(k + 1, fk_hist[k], xk_error_hist[k], rel_grad_uk_norm_hist[k], tk))\n",
    "\n",
    "            if xk_error_hist[k] < self.tol:\n",
    "                tk_hist = tk_hist[:k + 1]\n",
    "                xk_hist = xk_hist[:k + 1]\n",
    "                xk_error_hist = xk_error_hist[:k + 1]\n",
    "                rel_grad_uk_norm_hist = rel_grad_uk_norm_hist[:k + 1]\n",
    "                fk_hist = fk_hist[:k + 1]\n",
    "\n",
    "                print('-------------------------- HJ-MAD RESULTS ---------------------------')\n",
    "                print('HJ-MAD converged with rel grad norm {:6.2e}'.format(rel_grad_uk_norm_hist[k]))\n",
    "                print('iter = ', k, ', number of function evaluations = ', len(xk_error_hist) * self.int_samples)\n",
    "                \n",
    "                break\n",
    "            elif k == self.max_iters - 1:\n",
    "                print('-------------------------- HJ-MAD RESULTS ---------------------------')\n",
    "                print('HJ-MAD failed to converge with rel grad norm {:6.2e}'.format(rel_grad_uk_norm_hist[k]))\n",
    "                print('iter = ', k, ', number of function evaluations = ', len(xk_error_hist) * self.int_samples)\n",
    "                print('Used fixed time = ', self.fixed_time)\n",
    "\n",
    "            if k > 0:\n",
    "                if fk_hist[k] < fk_hist[k - 1]:\n",
    "                    x_opt = xk\n",
    "\n",
    "            grad_uk_norm_old  =  np.linalg.norm(grad_uk)\n",
    "\n",
    "            # Accelerate\n",
    "            if self.accelerated and k > 0:\n",
    "                yk = xk + momentum * (xk - xk_minus_1)\n",
    "                xk_minus_1 = xk\n",
    "            else:\n",
    "                yk=xk\n",
    "\n",
    "\n",
    "            # Perform GD\n",
    "            xk, uk_info = self.gradient_descent(yk,tk,grad_uk_old=grad_uk_old)\n",
    "\n",
    "            grad_uk = uk_info[0]\n",
    "            \n",
    "            if self.beta != 0.0:\n",
    "                grad_uk_old = grad_uk\n",
    "    \n",
    "            # Compute Relative Grad Uk\n",
    "            grad_uk_norm = np.linalg.norm(grad_uk)\n",
    "            rel_grad_uk_norm = grad_uk_norm / (grad_uk_norm_old + 1e-12)\n",
    "\n",
    "            if not self.fixed_time:\n",
    "                tk = self.update_time(tk, rel_grad_uk_norm)\n",
    "\n",
    "\n",
    "        if plot_bool:\n",
    "            self.plot(k,xk, tk,xk_error_hist[k],plot_bool)\n",
    "\n",
    "        algorithm_hist = (xk_hist, tk_hist, xk_error_hist, rel_grad_uk_norm_hist, fk_hist)\n",
    "                     \n",
    "        return x_opt, algorithm_hist\n",
    "    \n",
    "\n",
    "    def plot(self, k: int, xk: float, tk: float, error: float, plot_bool: bool = True, \n",
    "         uk_info: Optional[Tuple[float, float, float, int]] = None) -> None:\n",
    "        intervalx_a, intervalx_b, plot_output = self.plot_parameters\n",
    "\n",
    "        # Use the selected global function to compute f_values\n",
    "        x_range = np.linspace(intervalx_a, intervalx_b, 500)  # Adjust based on selected function\n",
    "        f_values = np.array([self.f(x) for x in x_range])\n",
    "\n",
    "        # Compute infor about Moreau envelope at xk\n",
    "        if uk_info is None:\n",
    "            uk_info = self.compute_grad_uk(xk, tk)\n",
    "\n",
    "        grad_uk, uk, se_uk, samples = uk_info\n",
    "            \n",
    "        prox_xk = xk - grad_uk\n",
    "\n",
    "        #xk_plus1 = xk - self.alpha * grad_uk\n",
    "        #estimated_moreau_value = self.f(prox_xk) + (1 / (2 * tk)) * ((prox_xk - xk) ** 2)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute the Error\n",
    "        #xk_plus1 = xk - self.alpha * grad_uk\n",
    "        if k==0:\n",
    "            error = np.linalg.norm(self.x0 - self.x_true) \n",
    "\n",
    "        # The function that prox minimizes\n",
    "        prox_func_values = np.array([self.f(x) + (1 / (2 * tk)) * (x - xk) ** 2 for x in x_range])\n",
    "\n",
    "        # Plot f(x) \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x_range, f_values, label='f(x)', color='black')\n",
    "\n",
    "        # Plot the function that prox minimizes at xk and its estimated minima\n",
    "        plt.plot(x_range, prox_func_values, label=r'$f(x) + \\frac{1}{2T} (x - x_k)^2$', color='orange')\n",
    "        plt.scatter(prox_xk, self.f(prox_xk) + (1 / (2 * tk)) * (prox_xk - xk) ** 2,\n",
    "                    facecolors='none',edgecolors='orange', label=r'Estimated Prox at $ x_k$, $prox_{tf}(x_k)\\approx x_k-t_k\\nabla u^{\\delta}(x_k)$ and' +\n",
    "                    f'\\nthe Estimated Global Minima of ' + r'$f(x) + \\frac{1}{2t_k} (x - x_k)^2$',\n",
    "                    s=100, zorder=4, marker='s')\n",
    "        \n",
    "        # Samples\n",
    "        if self.sample_bool:\n",
    "            samples_func_values = np.array([self.f(sample) + (1 / (2 * tk)) * (sample - xk) ** 2 for sample in samples])\n",
    "            plt.scatter(samples, samples_func_values,color='blue', label=r'Samples', zorder=4, marker='*')\n",
    "        \n",
    "\n",
    "        # Plot Moreau Envelope Estimation with error bars\n",
    "        plt.errorbar(xk, uk, yerr=se_uk, label=rf'Estimate Moreau Envelope Value, $u(x_k,t_k))$, where $t_k={tk:.1f}$',\n",
    "                 color='red', fmt='o', markersize=5, zorder=4, marker='x', capsize=5)\n",
    "\n",
    "        # Plot Points for Current, Next, Initial Iteration, and Global Minima\n",
    "        plt.scatter(xk, self.f(xk), label=r'Current Iteration, $f(x_k)$', zorder=6,s=150,  marker='^')\n",
    "        plt.scatter(self.x0, self.f(self.x0), color='green', label=r'Initial Iteration, $f(x_0)$',s=100, zorder=4, facecolors='none',edgecolors='blue')\n",
    "        plt.scatter(self.x_true, self.f(self.x_true), color='black', label=r'Global Minima, $f(x_{true})$',s=100, zorder=5, marker='x')\n",
    "        #plt.scatter(xk_plus1, self.f(xk_plus1), color='cyan', zorder=4, label=r'Next Iteration, $f(x_{k+1})$', marker='^')\n",
    "\n",
    "        plt.title(f'f(x) and Moreau Envelope\\nIteration {k}, Error={error:.3e}, Tol={self.tol:.3e}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('Function Value')\n",
    "\n",
    "        # Set limits and grid\n",
    "        plt.xlim(intervalx_a, intervalx_b)\n",
    "\n",
    "        # Dynamically set the y-limits based on function outputs\n",
    "        y_min = np.min(f_values)\n",
    "        y_max = np.max(f_values)\n",
    "\n",
    "        if y_max < 0:\n",
    "            # If all values are negative, set the limits to give some visual space\n",
    "            plt.ylim(1.2 * y_min, 0)  # Set lower limit 20% below min, upper limit at 0\n",
    "        elif y_min > 0:\n",
    "            plt.ylim(0.8 * y_min, 1.2 * y_max)  # 20% less than the min if min is positive\n",
    "        else:\n",
    "            plt.ylim(1.2 * y_min, 1.2 * y_max)  # 20% more than the min if min is zero or negative\n",
    "\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=1)\n",
    "        plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "        plt.minorticks_on()\n",
    "        plt.gca().xaxis.set_minor_locator(plt.MultipleLocator(1))\n",
    "        plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "\n",
    "        # Save the figure as a PNG file\n",
    "        plt.savefig(\"MAD_interactive_plot.png\", format='png', bbox_inches='tight')\n",
    "        plt.close()  # Close the plot to free up memory\n",
    "\n",
    "        if plot_bool:\n",
    "            with plot_output:\n",
    "                clear_output(wait=True)  # Clear previous plot\n",
    "                display(Image(filename=\"MAD_interactive_plot.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# UI Class to Interact with HJ_MAD\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class HJ_MAD_UI:\n",
    "    def __init__(self):\n",
    "        self.init_hj_mad()\n",
    "        self.create_ui()\n",
    "\n",
    "    def init_hj_mad(self):\n",
    "        # Output widget to display the plot\n",
    "        self.plot_output = Output()\n",
    "        self.plot_results_output = Output()\n",
    "\n",
    "        # Fixed Variables\n",
    "        tol = 5e-4\n",
    "        eta_min = 0.9\n",
    "        eta_plus = 5.0\n",
    "        eta_vec = [eta_min, eta_plus]\n",
    "        theta = 0.9\n",
    "        beta = 0.0\n",
    "        self.t_min = 0.1\n",
    "\n",
    "        # Set Initial Interactive Values\n",
    "        int_samples = int(200)\n",
    "        delta = 0.1\n",
    "        t_init = 220\n",
    "        t_max = 500\n",
    "        max_iters = int(50)\n",
    "        alpha = 0.1\n",
    "\n",
    "        # Default Function Settings\n",
    "        intervalx_a, intervalx_b = -40, 25\n",
    "        plot_parameters = [intervalx_a, intervalx_b,self.plot_output]\n",
    "        f = MultiMinimaFunc_numpy\n",
    "        x_true = -1.51034568-10\n",
    "        self.gamma = 1.89777\n",
    "        x0 = -40\n",
    "\n",
    "        # Create HJ_MAD instance with initial parameters\n",
    "        self.hj_mad = HJ_MAD(f=f, x_true=x_true, x0=x0, delta=delta, int_samples=int_samples, \n",
    "                             t_vec=[t_init, self.t_min, t_max], max_iters=max_iters, tol=tol, \n",
    "                             theta=theta, beta=beta, eta_vec=eta_vec, alpha=alpha, \n",
    "                             fixed_time=False, verbose=False, accelerated=False,\n",
    "                             plot_parameters=plot_parameters)\n",
    "    \n",
    "    def create_ui(self):\n",
    "        # Title and Section Headers:\n",
    "        title = HTML(\"<h2>HJ Moreau Adaptive Descent Visualization (in 1D)</h2>\")\n",
    "        internal_parameters = HTML(\"<h3>Internal Parameters:</h3>\")\n",
    "        initial_conditions = HTML(\"<h3>Initial Conditions Parameters:</h3>\")\n",
    "        \n",
    "        # Dropdown to select the function\n",
    "        self.function_dropdown = Dropdown(\n",
    "            options=['Sinc', 'Sin', 'MultiMinima','MultiMinimaAbsFunc', 'DiscontinuousFunc'],\n",
    "            value='MultiMinima',\n",
    "            description='Function:',\n",
    "            tooltip='Select the mathematical function to optimize.'\n",
    "        )\n",
    "        \n",
    "        # Sliders for parameters, initialized with default values\n",
    "        self.x_0_slider = FloatSlider(value=self.hj_mad.x0, min=self.hj_mad.plot_parameters[0], max=self.hj_mad.plot_parameters[1], step=0.01, description=r'x_0:', tooltip='Adjust the initial position.')\n",
    "        self.delta_slider = FloatSlider(value=self.hj_mad.delta, min=0, max=5.0, step=0.005, description='Viscosity, δ:', tooltip='Adjust the viscosity parameter.')\n",
    "        self.int_samples_slider = FloatText(value=self.hj_mad.int_samples, description='Samples, n:', tooltip='Select the number of samples.')\n",
    "        self.max_iters_slider = FloatSlider(value=self.hj_mad.max_iters, min=1, max=500, step=1, description='Max Iter., N:', tooltip='Set the maximum number of iterations.')\n",
    "        self.alpha_slider = FloatSlider(value=self.hj_mad.alpha, min=1-np.sqrt(self.hj_mad.eta_vec[0]), max=1+np.sqrt(self.hj_mad.eta_vec[1]), step=0.01, description='Step Size, α:', tooltip='Adjust the step size for optimization.')\n",
    "        self.momentum_slider = FloatSlider(value=self.hj_mad.momentum, min=0.01, max=2, step=0.01, description='Momentum:', tooltip='Adjust the momentum for accelerate GD.')\n",
    "\n",
    "        self.t_max_slider = FloatSlider(value=self.hj_mad.t_vec[2], min=self.t_min, max=300, step=0.1, description='Max t, T:', tooltip='Set the maximum time for the optimization.')\n",
    "        self.t_init_slider = FloatSlider(value=self.hj_mad.t_vec[0], min=self.t_min, max=300, step=0.1, description='Initial t, t₀:', tooltip='Set the initial time for the optimization.')\n",
    "\n",
    "        # Text input\n",
    "        self.beta_input = FloatText(value=self.hj_mad.beta, description='ADAM, β:', tooltip='Adjust the Adam parameter for Adam method.')\n",
    "        \n",
    "        # Only visible when acceleration is on\n",
    "        self.momentum_slider.layout.visibility = 'hidden'\n",
    "        self.beta_input.layout.visibility = 'hidden'\n",
    "\n",
    "        # Value input for tolerence\n",
    "        self.tol_input = FloatText(value=self.hj_mad.tol, description='tol:', tooltip='Set the tolerence for optimization.')\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "        # Checkbox for fixed time and acceleration\n",
    "        self.fixed_time_checkbox = Checkbox(value=False, description='Fixed Time', tooltip='Check to use fixed time for optimization.')\n",
    "        self.acceleration_checkbox = Checkbox(value=False, description='Acceleration', tooltip='Check to use Accelerated GD for optimization.')\n",
    "        self.sample_bool_checkbox = Checkbox(value=False, description='Display Samples', tooltip='Check to use Display Sampling.')\n",
    "\n",
    "        # Checkbox to select the integration method\n",
    "        self.integration_method_checkbox = Checkbox(\n",
    "            value=False,\n",
    "            description='Use Trapezium Rule Integration',\n",
    "            tooltip='Check to use the trapezium rule instead of Monte Carlo sampling for integration.'\n",
    "        )\n",
    "\n",
    "        # Buttons to run optimization\n",
    "        self.run_button = Button(description='Run Optimization', tooltip='Start the optimization process.')\n",
    "        self.animation_button = Button(description='Run Animation', tooltip='Start the animation of the optimization process.')\n",
    "        self.reset_plot_button = Button(description='Reset Plot', tooltip='Reset the plot to the initial state.')\n",
    "\n",
    "        # Bind slider changes to specific update methods\n",
    "        self.delta_slider.observe(self.update_hj_mad_delta, names='value')\n",
    "        self.int_samples_slider.observe(self.update_hj_mad_int_samples, names='value')\n",
    "        self.t_init_slider.observe(self.update_hj_mad_t_init, names='value')\n",
    "        self.t_max_slider.observe(self.update_hj_mad_t_max, names='value')\n",
    "        self.max_iters_slider.observe(self.update_hj_mad_max_iters, names='value')\n",
    "        self.alpha_slider.observe(self.update_hj_mad_alpha, names='value')\n",
    "        self.momentum_slider.observe(self.update_hj_mad_momentum, names='value')\n",
    "        self.x_0_slider.observe(self.update_hj_mad_x0, names='value')\n",
    "\n",
    "        # Bind tolerence input changes to specific update methods\n",
    "        self.tol_input.observe(self.tol_input_update, names='value')\n",
    "        self.beta_input.observe(self.update_hj_mad_beta, names='value')\n",
    "\n",
    "        # Dropdown event listener to update the selected function\n",
    "        self.function_dropdown.observe(self.update_function, names='value')\n",
    "\n",
    "        # Checkbox event listener to update the fixed time state\n",
    "        self.fixed_time_checkbox.observe(self.update_fixed_time, names='value')\n",
    "        self.acceleration_checkbox.observe(self.update_acceleration, names='value')\n",
    "        self.sample_bool_checkbox.observe(self.display_sampling, names='value')\n",
    "        self.integration_method_checkbox.observe(self.update_integration_method_visibility, names='value')\n",
    "\n",
    "        \n",
    "        # # Initialize t_threshold_display and standard_error_display\n",
    "        self.t_threshold_display = HTML(\"\")\n",
    "        self.standard_error_display = HTML(\"\")\n",
    "        self.update_t_threshold_display()\n",
    "        self.update_standard_error_display()\n",
    "\n",
    "        # Bind button clicks to the respective methods\n",
    "        self.run_button.on_click(lambda b: self.run(animate=False))\n",
    "        self.animation_button.on_click(lambda b: self.run(animate=True))\n",
    "        self.reset_plot_button.on_click(lambda b: self.reset_plot())\n",
    "\n",
    "        # Display the UI\n",
    "        ui = VBox([title, internal_parameters, \n",
    "                HBox([self.function_dropdown, self.tol_input,self.max_iters_slider]),\n",
    "                HBox([self.delta_slider, self.int_samples_slider, self.t_max_slider]), \n",
    "                HBox([self.alpha_slider,self.momentum_slider,self.beta_input]),\n",
    "                HBox([self.sample_bool_checkbox, self.fixed_time_checkbox, self.acceleration_checkbox]),\n",
    "                self.integration_method_checkbox,\n",
    "                initial_conditions, \n",
    "                HBox([self.t_init_slider, self.x_0_slider]),\n",
    "                HBox([self.standard_error_display, self.t_threshold_display]),\n",
    "                HBox([self.run_button, self.animation_button,self.reset_plot_button]), # instructions,\n",
    "                HBox([self.plot_output,self.plot_results_output])])  # Add plot output to the UI\n",
    "\n",
    "        display(ui)\n",
    "        \n",
    "\n",
    "    def update_hj_mad_delta(self,b):\n",
    "        self.hj_mad.delta = self.delta_slider.value\n",
    "        self.plot_hj_mad()\n",
    "        self.update_standard_error_display()\n",
    "\n",
    "    def update_hj_mad_int_samples(self,b):\n",
    "        self.hj_mad.int_samples = int(self.int_samples_slider.value)\n",
    "        self.plot_hj_mad()\n",
    "        self.update_standard_error_display()\n",
    "\n",
    "    def update_hj_mad_t_init(self,b):\n",
    "        self.hj_mad.t_vec[0] = self.t_init_slider.value\n",
    "\n",
    "        # Don't let t_max become smaller than t_init\n",
    "        if self.t_max_slider.value < self.t_init_slider.value:\n",
    "            self.t_max_slider.value = self.t_init_slider.value\n",
    "        self.plot_hj_mad()\n",
    "        self.update_standard_error_display()\n",
    "\n",
    "    def update_hj_mad_t_max(self,b):\n",
    "        self.hj_mad.t_vec[2] = self.t_max_slider.value\n",
    "\n",
    "        # Don't let t_max become smaller than t_init\n",
    "        if self.t_max_slider.value < self.t_init_slider.value:\n",
    "            self.t_max_slider.value = self.t_init_slider.value\n",
    "        self.update_standard_error_display()\n",
    "\n",
    "    def update_hj_mad_max_iters(self,b):\n",
    "        self.hj_mad.max_iters = int(self.max_iters_slider.value)\n",
    "\n",
    "    def update_hj_mad_alpha(self,b):\n",
    "        self.hj_mad.alpha = self.alpha_slider.value\n",
    "\n",
    "    def update_hj_mad_beta(self,b):\n",
    "        self.hj_mad.beta = self.beta_input.value\n",
    "\n",
    "    def update_hj_mad_momentum(self,b):\n",
    "        self.hj_mad.momentum = self.momentum_slider.value\n",
    "\n",
    "    def update_t_threshold_display(self):\n",
    "        \"\"\"Update the display for the t_threshold and compute its value.\"\"\"\n",
    "\n",
    "        # Create the HTML string\n",
    "        html_content = \"\"\"\n",
    "        <div style=\"border: 1px solid black; padding: 10px; border-radius: 5px; margin-top: 10px;\">\n",
    "        <h4 style=\"margin: 0;\">Time Threshold <i>(Theoretical Lower Bound on Intial Time Steps for Convergence)</i></h4>\n",
    "        <div style=\"font-family: Times New Roman, serif; font-size: 14px;\">\n",
    "            <p><strong>t<sub>threshold</sub> = {}\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate t_threshold\n",
    "        if self.gamma is not None:\n",
    "            self.t_threshold = (np.linalg.norm(self.hj_mad.x_true - self.x_0_slider.value)**2) / (2 * self.gamma)\n",
    "            # Use str.format to insert the calculated value into html_content\n",
    "            html_content = html_content.format(f\"{self.t_threshold:.2f}, where</strong>:</p>\")\n",
    "        else:\n",
    "            # If self.gamma is None, modify html_content to show 'UnDefined'\n",
    "            self.t_threshold_display.value = html_content.format('UnDefined</p>')\n",
    "            return\n",
    "\n",
    "        # Make sure t_min is less that t_threshold (this isn't a necessary \n",
    "        # condition as long as t_min<=t_init but this gives the user more choice)\n",
    "        if self.t_threshold < self.t_min:\n",
    "            self.t_min = self.t_threshold*0.8\n",
    "            self.t_max_slider.min = self.t_min\n",
    "            self.t_max_slider.min = self.t_min\n",
    "\n",
    "\n",
    "        if self.t_threshold > self.t_max_slider.max:\n",
    "            self.t_max_slider.max = self.t_threshold*1.5\n",
    "            self.t_init_slider.max = self.t_threshold*1.5\n",
    "\n",
    "\n",
    "        if self.hj_mad.fixed_time:\n",
    "            html_content += f\"\"\"<p style=\"margin-left: 20px;\">\n",
    "                • t<sub>0</sub> ≥ ||x* - x<sub>0</sub>||<sup>2</sup> / (2γ) = t<sub>threshold</sub>\n",
    "            </p>\n",
    "            </div>\n",
    "            </div>\"\"\"\n",
    "            # Update the display\n",
    "            self.t_threshold_display.value = html_content\n",
    "            return\n",
    "        \n",
    "        html_content += f\"\"\" <p style=\"margin-left: 20px;\">\n",
    "                • T ≥ t<sub>0</sub> ≥ ||x* - x<sub>0</sub>||<sup>2</sup> / (2γ) = t<sub>threshold</sub>\n",
    "            </p>\n",
    "            <p style=\"margin-left: 20px;\">\n",
    "                • Also T ≥ t<sub>0</sub> ≥ τ > 0, τ = {self.t_min:.2f}\n",
    "            </p>\n",
    "            <p style=\"margin-left: 20px;\">• T, max time. τ, min time. t<sub>0</sub>, initial time. </p>\n",
    "        </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        # Update the display\n",
    "        self.t_threshold_display.value = html_content\n",
    "\n",
    "\n",
    "    def update_standard_error_display(self):\n",
    "        \"\"\"\n",
    "        Standard Error is a measure of how much the sample mean will vary from \n",
    "        sample to sample.\n",
    "        \"\"\"\n",
    "        # Compute the standard Deviation and Standard Error at Initial Time\n",
    "        uk_info = self.hj_mad.compute_grad_uk(self.x_0_slider.value, self.t_init_slider.value)\n",
    "        se_uk=uk_info[2]\n",
    "\n",
    "        self.standard_error_display.value = f\"\"\"\n",
    "            <div style=\"border: 1px solid black; padding: 10px; border-radius: 5px; margin-top: 10px;\">\n",
    "                <h4 style=\"margin: 0;\">Standard Error for u<sub>k</sub> <i>(Variation Of The Sample Mean Between Samples)</i></h4>\n",
    "            <div style=\"font-family: Times New Roman, serif; font-size: 14px;\">\n",
    "                <p><strong>Standard Error = <sup>σ</sup>/<sub>/√n</sub> = {se_uk:.4e}, where</strong>:</p>\n",
    "                    <p style=\"margin-left: 20px;\">• SE is Standard Error.</p>\n",
    "                    <p style=\"margin-left: 20px;\">• σ is the Standard Deviation(depends on δ, t, f and x).</p>\n",
    "                    <p style=\"margin-left: 20px;\">• n is the sample size.</p>\n",
    "            </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "\n",
    "    def update_hj_mad_x0(self,b):\n",
    "        self.hj_mad.x0 = self.x_0_slider.value\n",
    "        self.update_t_threshold_display()\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "    def update_function(self, change):\n",
    "        # Update the selected function based on dropdown selection\n",
    "        if change['new'] == 'Sinc':\n",
    "            intervalx_a, intervalx_b = -20, 20\n",
    "            self.hj_mad.plot_parameters = [intervalx_a, intervalx_b,self.plot_output]\n",
    "            self.hj_mad.f = Sinc_numpy\n",
    "            self.hj_mad.x_true = 4.49336839, # and -4.49336839\n",
    "\n",
    "            self.gamma = 0.1259\n",
    "        elif change['new'] == 'Sin':\n",
    "            intervalx_a, intervalx_b = -3.5 * pi, 2.5 * pi\n",
    "            self.hj_mad.plot_parameters = [intervalx_a, intervalx_b,self.plot_output]\n",
    "            self.hj_mad.f = Sin_numpy\n",
    "            self.hj_mad.x_true = -np.pi / 2\n",
    "            self.gamma = None\n",
    "        elif change['new'] == 'MultiMinimaAbsFunc':\n",
    "            intervalx_a, intervalx_b = -15, 15\n",
    "            self.hj_mad.plot_parameters = [intervalx_a, intervalx_b,self.plot_output]\n",
    "            self.hj_mad.f = MultiMinimaAbsFunc_numpy\n",
    "            self.hj_mad.x_true = 2\n",
    "            self.gamma = 1.43457\n",
    "        elif change['new'] == 'DiscontinuousFunc':\n",
    "            intervalx_a, intervalx_b = -15, 15\n",
    "            self.hj_mad.plot_parameters = [intervalx_a, intervalx_b,self.plot_output]\n",
    "            self.hj_mad.f = DiscontinuousFunc_numpy\n",
    "            self.hj_mad.x_true = 2\n",
    "            self.gamma = 7\n",
    "        else:  # change['new'] == 'MultiMinimaFunc'\n",
    "            intervalx_a, intervalx_b = -40, 25\n",
    "            self.hj_mad.plot_parameters = [intervalx_a, intervalx_b,self.plot_output]\n",
    "            self.hj_mad.f = MultiMinimaFunc_numpy\n",
    "            self.hj_mad.x_true = -1.51034568-10\n",
    "            self.gamma = 1.89777\n",
    "\n",
    "\n",
    "        # Update the slider range for x_0 based on the new function\n",
    "        self.x_0_slider.min = intervalx_a\n",
    "        self.x_0_slider.max = intervalx_b\n",
    "        self.x_0_slider.value = (intervalx_a + intervalx_b) / 2\n",
    "\n",
    "        self.update_t_threshold_display()\n",
    "        self.update_standard_error_display()\n",
    "        self.plot_results_output.clear_output()  # Clear previous results\n",
    "        self.plot_hj_mad()\n",
    "    \n",
    "    def update_fixed_time(self, change):\n",
    "        \"\"\"\n",
    "        Update the fixed time state based on the checkbox.\n",
    "        \"\"\"\n",
    "        self.hj_mad.fixed_time = change['new']\n",
    "        # self.update_standard_error_display()\n",
    "        # self.update_t_threshold_display()\n",
    "        if change['new']:\n",
    "            self.t_max_slider.layout.visibility = 'hidden'\n",
    "        else:\n",
    "            self.t_max_slider.layout.visibility = 'visible'\n",
    "\n",
    "    def update_acceleration(self, change):\n",
    "        \"\"\"\n",
    "        Update the accelerated state based on the checkbox.\n",
    "        \"\"\"\n",
    "        self.hj_mad.accelerated = change['new']\n",
    "        if change['new']:  # If the checkbox is checked\n",
    "            self.momentum_slider.layout.visibility = 'visible'\n",
    "            if not self.hj_mad.trap_integration:\n",
    "                self.beta_input.layout.visibility = 'visible'\n",
    "                self.beta_input.value = 0.1\n",
    "            else:\n",
    "                self.beta_input.value = 0.0\n",
    "        else:  # If the checkbox is unchecked\n",
    "            self.momentum_slider.layout.visibility = 'hidden'\n",
    "            self.beta_input.layout.visibility = 'hidden'\n",
    "            self.beta_input.value = 0.0\n",
    "\n",
    "    def update_integration_method_visibility(self, change):\n",
    "        # Show `beta` input only if the trapezium rule is NOT selected\n",
    "        self.hj_mad.trap_integration = change['new']\n",
    "        if change['new']:\n",
    "            self.beta_input.layout.visibility = 'hidden'\n",
    "            self.sample_bool_checkbox.layout.visibility = 'hidden'\n",
    "            self.standard_error_display.layout.visibility = 'hidden'\n",
    "            self.beta_input.value = 0.0\n",
    "            self.sample_bool_checkbox.value = False\n",
    "            self.display_sampling(self, change)\n",
    "        elif self.hj_mad.accelerated:\n",
    "            self.beta_input.layout.visibility = 'visible'\n",
    "            self.sample_bool_checkbox.layout.visibility = 'visible'\n",
    "            self.standard_error_display.layout.visibility = 'visible'\n",
    "        else:\n",
    "            self.sample_bool_checkbox.layout.visibility = 'visible'\n",
    "            self.standard_error_display.layout.visibility = 'visible'\n",
    "\n",
    "    def display_sampling(self, change):\n",
    "        \"\"\"\n",
    "        Update the sampling display state based on the checkbox.\n",
    "        \"\"\"\n",
    "        self.hj_mad.sample_bool = change['new']\n",
    "        self.plot_results_output.clear_output()  # Clear previous results\n",
    "        self.plot_hj_mad()\n",
    "\n",
    "    def run(self, animate):\n",
    "        self.plot_results_output.clear_output()  # Clear previous results\n",
    "        # Run optimization with current slider values\n",
    "        print('-------------------------- RUNNING HJ-MAD ---------------------------')\n",
    "        print('For the parameters:')\n",
    "        self.print_slider_values()\n",
    "\n",
    "        _, algorithm_hist = self.hj_mad.run(animate=animate)\n",
    "        \n",
    "        self.plot_results(algorithm_hist)\n",
    "        display(Image(filename=\"MAD_interactive_plot.png\"))\n",
    "\n",
    "    def plot_hj_mad(self):\n",
    "        self.hj_mad.plot(0, self.hj_mad.x0, self.hj_mad.t_vec[0],0)\n",
    "    \n",
    "    def plot_results(self, algorithm_hist):\n",
    "        # Unpack general case and accelerated case histories\n",
    "        non_acc_algorithm_hist = None\n",
    "\n",
    "        if self.hj_mad.accelerated:\n",
    "            self.hj_mad.accelerated = False\n",
    "            _, non_acc_algorithm_hist = self.hj_mad.run(animate=False,plot_bool=False)\n",
    "            _, non_acc_tk_hist, non_acc_xk_error_hist, non_acc_rel_grad_uk_norm_hist, non_acc_fk_hist = non_acc_algorithm_hist\n",
    "\n",
    "            if self.hj_mad.trap_integration:\n",
    "                self.hj_mad.accelerated = True\n",
    "            else:\n",
    "                self.hj_mad.beta = 0.7\n",
    "                _, adam_algorithm_hist = self.hj_mad.run(animate=False,plot_bool=False)\n",
    "                _, adam_tk_hist, adam_xk_error_hist, adam_rel_grad_uk_norm_hist, adam_fk_hist = adam_algorithm_hist\n",
    "\n",
    "                self.hj_mad.accelerated = True\n",
    "\n",
    "                self.hj_mad.beta = 0.3\n",
    "                _, acc_adam_algorithm_hist = self.hj_mad.run(animate=False,plot_bool=False)\n",
    "                _, acc_adam_tk_hist, acc_adam_xk_error_hist, acc_adam_rel_grad_uk_norm_hist, acc_adam_fk_hist = acc_adam_algorithm_hist\n",
    "\n",
    "                self.hj_mad.beta = 0.0\n",
    "            acc_string = ' (Accelerated)'\n",
    "        else:\n",
    "            acc_string = ' (Non-Accelerated)'\n",
    "        \n",
    "        _, tk_hist, xk_error_hist, rel_grad_uk_norm_hist, fk_hist = algorithm_hist\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        # Error history subplot\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.semilogy(xk_error_hist, label='Error History'+acc_string, color='blue')\n",
    "        if non_acc_algorithm_hist is not None:\n",
    "            plt.semilogy(non_acc_xk_error_hist, label='Error History (Non-Accelerated)', color='orange', linestyle='--')\n",
    "            if not self.hj_mad.trap_integration:\n",
    "                plt.semilogy(adam_xk_error_hist, label='Error History (Non-Accelerated-Adam)', color='red', linestyle=':')\n",
    "                plt.semilogy(acc_adam_xk_error_hist, label='Error History (Accelerated-Adam)', color='green', linestyle='-.')\n",
    "        plt.title('Error History')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Error')\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # f(k) history subplot\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(fk_hist, label='f(k) History'+acc_string, color='blue')\n",
    "        if non_acc_algorithm_hist is not None:\n",
    "            plt.plot(non_acc_fk_hist, label='f(k) History (Non-Accelerated)', color='orange', linestyle='--')\n",
    "\n",
    "            if not self.hj_mad.trap_integration:\n",
    "                plt.plot(adam_fk_hist, label='f(k) History (Non-Accelerated-Adam)', color='red', linestyle=':')\n",
    "                plt.plot(acc_adam_fk_hist, label='f(k) History (Accelerated-Adam)', color='green', linestyle='-.')\n",
    "        plt.title('f(k) History')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('f(k)')\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # t(k) history subplot\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.semilogy(tk_hist, label='t(k) History'+acc_string, color='blue')\n",
    "        if non_acc_algorithm_hist is not None:\n",
    "            plt.semilogy(non_acc_tk_hist, label='t(k) History (Non-Accelerated)', color='orange', linestyle='--')\n",
    "\n",
    "            if not self.hj_mad.trap_integration:\n",
    "                plt.semilogy(adam_tk_hist, label='t(k) History (Non-Accelerated-Adam)', color='red', linestyle=':')\n",
    "                plt.semilogy(acc_adam_tk_hist, label='t(k) History (Accelerated-Adam)', color='green', linestyle='-.')\n",
    "        plt.title('t(k) History')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('t(k)')\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        # Relative gradient norm history subplot\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.semilogy(rel_grad_uk_norm_hist, label='Rel. Grad Norm'+acc_string, color='blue')\n",
    "        if non_acc_algorithm_hist is not None:\n",
    "            plt.semilogy(non_acc_rel_grad_uk_norm_hist, label='Rel. Grad Norm (Non-Accelerated)', color='orange', linestyle='--')\n",
    "\n",
    "            if not self.hj_mad.trap_integration:\n",
    "                plt.semilogy(adam_rel_grad_uk_norm_hist, label='Rel. Grad Norm (Non-Accelerated-Adam)', color='red', linestyle=':')\n",
    "                plt.semilogy(acc_adam_rel_grad_uk_norm_hist, label='Rel. Grad Norm (Accelerated-Adam)', color='green', linestyle='-.')\n",
    "        plt.title('Relative Gradient Norm History')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Relative Gradient Norm')\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.tight_layout()  # Adjust the layout\n",
    "        plt.savefig(\"MAD_combined_history_plot.png\", format='png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()  # Free up memory\n",
    "\n",
    "        # Display the latest saved plot\n",
    "        with self.plot_results_output:\n",
    "            clear_output(wait=True)  # Clear previous plot\n",
    "            display(Image(filename=\"MAD_combined_history_plot.png\"))\n",
    "\n",
    "    def reset_plot(self):\n",
    "        \"\"\"Reset the plot and hide the plot results.\"\"\"\n",
    "        self.plot_hj_mad()\n",
    "        self.plot_results_output.clear_output()  # Clear previous results\n",
    "        #self.plot_results_output = None  # Reset the plot results\n",
    "    \n",
    "    def tol_input_update(self, _):\n",
    "        self.hj_mad.tol = self.tol_input.value\n",
    "\n",
    "    def print_slider_values(self):\n",
    "        \"\"\"Print the values of the sliders and selected function in a rotated table format.\"\"\"\n",
    "        function_name = self.function_dropdown.value\n",
    "        slider_values = {\n",
    "            'x_0': self.x_0_slider.value,\n",
    "            't_init': self.t_init_slider.value,\n",
    "            't_max': self.t_max_slider.value,\n",
    "            'delta': self.delta_slider.value,\n",
    "            'int_samples': self.int_samples_slider.value,\n",
    "            'max_iters': self.max_iters_slider.value,\n",
    "            'alpha': self.alpha_slider.value,\n",
    "            't_threshold': self.t_threshold\n",
    "        }\n",
    "\n",
    "        # Create a smaller HTML table in rotated format\n",
    "        html_content = \"<h3>Slider Values and Function Name</h3><table style='border-collapse: collapse; width: 50%; font-size: 14px;'><tr><th style='border: 1px solid black;'><strong>Parameter</strong></th><th style='border: 1px solid black;'><strong>Value</strong></th></tr>\"\n",
    "        \n",
    "        # Add each parameter and its value as a new row\n",
    "        for param, value in slider_values.items():\n",
    "            html_content += f\"<tr><td style='border: 1px solid black;'>{param}</td><td style='border: 1px solid black;'>{value:.2f}</td></tr>\"\n",
    "        \n",
    "        # Add the function name at the end\n",
    "        html_content += f\"<tr><td style='border: 1px solid black;'>Function</td><td style='border: 1px solid black;'>{function_name}</td></tr>\"\n",
    "        html_content += \"</table>\"\n",
    "\n",
    "        # Display the table\n",
    "        display(HTML(html_content))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rescale=0.5\n",
      "iterations=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rescale=0.25\n",
      "iterations=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beccca92fd9546c3b6f1bde18cb1c731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>HJ Moreau Adaptive Descent Visualization (in 1D)</h2>'), HTML(value='<h3>Intern…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "UI= HJ_MAD_UI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-34.538776394910684"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1e-15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
