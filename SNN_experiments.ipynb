{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from hj_mad_ls import HJ_MD_LS\n",
    "\n",
    "# Hyperparameters\n",
    "n_neurons = 50  # Number of neurons in the shallow network\n",
    "num_epochs = 10\n",
    "batch_size = 4\n",
    "\n",
    "# Generate data\n",
    "no_of_samples = 500\n",
    "noise_level = 1e-3\n",
    "x = np.linspace(0, 2 * np.pi, no_of_samples)\n",
    "y = np.sin(x) + noise_level * np.random.randn(*x.shape)  # Add noise to the samples\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the shallow neural network\n",
    "class ShallowNet(nn.Module):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(ShallowNet, self).__init__()\n",
    "        self.linear = nn.Linear(1, n_neurons)\n",
    "        self.coeffs = nn.Parameter(torch.randn(n_neurons, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)      # Compute w_j * x + b_j\n",
    "        x = self.relu(x)        # Apply ReLU activation\n",
    "        x = x @ self.coeffs     # Compute sum of c_j * ReLU(...)\n",
    "        return x\n",
    "\n",
    "    def set_parameters(self, new_parameters):\n",
    "        \"\"\"\n",
    "        Set model parameters from the flattened parameter vector.\n",
    "        This method updates the model's parameters from a single flattened tensor.\n",
    "\n",
    "        Args:\n",
    "            new_parameters: A flattened tensor containing all parameters.\n",
    "        \"\"\"\n",
    "        offset = 0\n",
    "        for param in self.parameters():\n",
    "            param_size = param.numel()  # Total number of elements in the parameter\n",
    "            # Extract the relevant slice and reshape it to the parameter's original shape\n",
    "            if len(new_parameters.shape) == 1:\n",
    "                new_param_values = new_parameters[offset:offset + param_size]\n",
    "            else:\n",
    "                new_param_values = new_parameters[0, offset:offset + param_size]  # Access batch dimension\n",
    "            new_param_values = new_param_values.view_as(param)  # Match the original shape\n",
    "            param.data.copy_(new_param_values)  # Update parameter\n",
    "            offset += param_size\n",
    "\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the function f (e.g., a loss function based on the model's predictions)\n",
    "def loss_function(x, model, inputs, targets):\n",
    "    \"\"\"\n",
    "    Computes the loss based on the given model parameters.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape (n_samples, n_features) representing flattened parameters.\n",
    "        model: The neural network model.\n",
    "        inputs: Input batch.\n",
    "        targets: Ground truth values.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (n_samples,) containing the loss for each sample.\n",
    "    \"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    \n",
    "    # Initialize list to store losses\n",
    "    sample_losses = []\n",
    "\n",
    "    # Compute losses for each sample in the batch\n",
    "    for i in range(n_samples):\n",
    "        # Extract parameters for the current sample\n",
    "        model_params = x[i, :]  # Take the i-th sample\n",
    "        model.set_parameters(model_params)  # Update model parameters\n",
    "\n",
    "        # Forward pass and loss computation\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        sample_losses.append(loss.item())\n",
    "\n",
    "    # Convert the list of losses to a tensor\n",
    "    return torch.tensor(sample_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = ShallowNet(n_neurons)\n",
    "\n",
    "# # HJ_MAD hyperparameters\n",
    "# delta = 5e-11#4.87e-11          # Small delta -> Better Approximation of the Moreau Envelope Provided Large T or Close to Global Minimum\n",
    "# t =   5e+7 #3.69            # Large T -> Better Exploration (Moreau Envelope closer to approximate quadratic)\n",
    "# int_samples = int(500)  # Large N -> Better Expectations for Larger T -> Better Exploration (But more expensive)\n",
    "# max_iters = int(2)   # Maximum number of iterations\n",
    "# loss_tol = 9e-5         # Stopping criterion for the loss\n",
    "# sat_tol = 1e-3          # Stopping criterion for the saturation\n",
    "# beta=0.0                # Set beta to 0.0 to turn off \n",
    "# momentum=0.0            # Set momentum to 0.0 to turn off\n",
    "# distribution=\"Gaussian\" # Set sampling distribution \n",
    "\n",
    "# # Use HJ_MAD to optimize the model parameters\n",
    "# HJ_MAD_alg = HJ_MD_LS(delta=delta, t=t,distribution=distribution,momentum=momentum,\n",
    "#                       int_samples=int_samples, max_iters=max_iters, f_tol=loss_tol,\n",
    "#                       verbose=False, adaptive_time=False,adaptive_delta=True,\n",
    "#                       line_search=False)\n",
    "\n",
    "\n",
    "# # Training loop: Iterate through the epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Iterate through the dataset in batches\n",
    "#     for inputs, targets in dataloader:\n",
    "#         # Define f\n",
    "#         def f(x):\n",
    "#             return loss_function(x, model, inputs, targets)\n",
    "        \n",
    "#         # Get the model parameters as a flattened tensor\n",
    "#         parameters = list(model.parameters())\n",
    "#         parameter_vector = torch.cat([param.flatten() for param in parameters])\n",
    "#         parameter_vector = parameter_vector.unsqueeze(0) # shape (1, num_features)\n",
    "\n",
    "#         # Compute Current Loss\n",
    "#         loss_old = f(parameter_vector)  # Compute the loss for the current parameters\n",
    "\n",
    "#         # Run HJ_MAD to optimize the model parameters\n",
    "#         new_parameters, loss, loss_history, delta_hist, tk_hist, iterations = HJ_MAD_alg.run(f, parameter_vector)  # Run HJ_MAD to optimize the model parameters\n",
    "\n",
    "#         HJ_MAD_alg.delta = delta_hist[-1]  # Update delta for the next iteration\n",
    "#         HJ_MAD_alg.t = tk_hist[-1]\n",
    "\n",
    "#         # Update the model parameters with the optimized values\n",
    "#         model.set_parameters(new_parameters)\n",
    "\n",
    "#         # Print the loss before and after optimization\n",
    "#         print(f'Loss before: {loss_old.item():.4e} | Loss after: {loss.item():.4e} | Delta: {delta_hist[-1]}')\n",
    "        \n",
    "#     # Print the loss every 100 epochs for monitoring\n",
    "#     if (epoch + 1) % 100 == 0:\n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# # Visualise the learned function\n",
    "# plt.figure()\n",
    "# plt.plot(x, y, label='True function')\n",
    "# plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- RUNNING HJ-MAD-LS Algorithm ---------------------------\n",
      "dimension =  150 n_samples =  1000\n",
      "[  0]: fk = 3.96e+02 | deltak = 5.00e-11 | tk = 5.00e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[  1]: fk = 2.83e+02 | deltak = 5.00e-11 | tk = 5.00e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[  2]: fk = 1.71e+02 | deltak = 5.00e-11 | tk = 4.95e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[  3]: fk = 9.27e+01 | deltak = 5.00e-11 | tk = 4.90e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[  4]: fk = 3.61e+01 | deltak = 5.00e-11 | tk = 4.85e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[  5]: fk = 2.60e+00 | deltak = 5.00e-11 | tk = 4.80e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[  6]: fk = 1.66e-01 | deltak = 5.00e-11 | tk = 4.75e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[  7]: fk = 1.38e-01 | deltak = 5.00e-11 | tk = 4.71e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[  8]: fk = 1.17e-01 | deltak = 5.00e-11 | tk = 4.66e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[  9]: fk = 1.11e-01 | deltak = 5.00e-11 | tk = 4.61e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 10]: fk = 1.01e-01 | deltak = 5.00e-11 | tk = 4.57e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 11]: fk = 9.12e-02 | deltak = 5.00e-11 | tk = 4.52e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 12]: fk = 8.32e-02 | deltak = 5.00e-11 | tk = 4.48e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 13]: fk = 7.94e-02 | deltak = 5.00e-11 | tk = 4.43e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 14]: fk = 7.71e-02 | deltak = 5.00e-11 | tk = 4.39e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 15]: fk = 7.24e-02 | deltak = 5.00e-11 | tk = 4.34e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 16]: fk = 6.90e-02 | deltak = 5.00e-11 | tk = 4.30e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 17]: fk = 7.21e-02 | deltak = 5.00e-11 | tk = 4.34e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 18]: fk = 6.73e-02 | deltak = 5.00e-11 | tk = 4.30e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 19]: fk = 5.92e-02 | deltak = 5.00e-11 | tk = 4.26e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 20]: fk = 5.77e-02 | deltak = 5.00e-11 | tk = 4.21e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 21]: fk = 5.44e-02 | deltak = 5.00e-11 | tk = 4.17e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 22]: fk = 5.15e-02 | deltak = 5.00e-11 | tk = 4.13e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 23]: fk = 5.18e-02 | deltak = 5.00e-11 | tk = 4.09e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 24]: fk = 4.71e-02 | deltak = 5.00e-11 | tk = 4.05e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    f(xk): 0.04714495316147804 | f(prox): 0.05308090150356293\n",
      "[ 25]: fk = 5.31e-02 | deltak = 4.50e-11 | tk = 4.01e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 26]: fk = 5.33e-02 | deltak = 4.50e-11 | tk = 3.97e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 27]: fk = 4.84e-02 | deltak = 4.50e-11 | tk = 3.93e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 28]: fk = 4.55e-02 | deltak = 4.50e-11 | tk = 3.89e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 29]: fk = 4.14e-02 | deltak = 4.50e-11 | tk = 3.85e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 30]: fk = 3.46e-02 | deltak = 4.50e-11 | tk = 3.81e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 31]: fk = 3.63e-02 | deltak = 4.50e-11 | tk = 3.77e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    f(xk): 0.03629680722951889 | f(prox): 0.04049742966890335\n",
      "[ 32]: fk = 4.05e-02 | deltak = 4.05e-11 | tk = 3.74e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 33]: fk = 4.10e-02 | deltak = 4.05e-11 | tk = 3.70e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 34]: fk = 3.38e-02 | deltak = 4.05e-11 | tk = 3.66e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    f(xk): 0.03376172110438347 | f(prox): 0.037604399025440216\n",
      "[ 35]: fk = 3.76e-02 | deltak = 3.65e-11 | tk = 3.62e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 36]: fk = 3.01e-02 | deltak = 3.65e-11 | tk = 3.59e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 37]: fk = 2.95e-02 | deltak = 3.65e-11 | tk = 3.55e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 38]: fk = 2.98e-02 | deltak = 3.65e-11 | tk = 3.59e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 39]: fk = 3.05e-02 | deltak = 3.65e-11 | tk = 3.55e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    f(xk): 0.030512306839227676 | f(prox): 0.036392148584127426\n",
      "[ 40]: fk = 3.64e-02 | deltak = 3.28e-11 | tk = 3.59e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 41]: fk = 2.41e-02 | deltak = 3.28e-11 | tk = 3.55e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    f(xk): 0.0241368617862463 | f(prox): 0.03037177212536335\n",
      "[ 42]: fk = 3.04e-02 | deltak = 2.95e-11 | tk = 3.52e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 43]: fk = 2.42e-02 | deltak = 2.95e-11 | tk = 3.48e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 44]: fk = 2.50e-02 | deltak = 2.95e-11 | tk = 3.45e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 45]: fk = 2.74e-02 | deltak = 2.95e-11 | tk = 3.41e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 46]: fk = 2.75e-02 | deltak = 2.95e-11 | tk = 3.38e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 47]: fk = 2.03e-02 | deltak = 2.95e-11 | tk = 3.34e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 48]: fk = 1.85e-02 | deltak = 2.95e-11 | tk = 3.31e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    f(xk): 0.018509313464164734 | f(prox): 0.021151993423700333\n",
      "[ 49]: fk = 2.12e-02 | deltak = 2.66e-11 | tk = 3.28e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 50]: fk = 2.06e-02 | deltak = 2.66e-11 | tk = 3.24e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 51]: fk = 1.73e-02 | deltak = 2.66e-11 | tk = 3.21e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "    f(xk): 0.01733485423028469 | f(prox): 0.0218774676322937\n",
      "[ 52]: fk = 2.19e-02 | deltak = 2.39e-11 | tk = 3.18e+07\n",
      "    Number of non-zero softmax weights on samples: 1\n",
      "[ 53]: fk = 2.02e-02 | deltak = 2.39e-11 | tk = 3.15e+07\n"
     ]
    }
   ],
   "source": [
    "# Define f\n",
    "def f(x):\n",
    "    return loss_function(x, model, x_tensor, y_tensor)\n",
    "\n",
    "model = ShallowNet(n_neurons)\n",
    "\n",
    "# HJ_MAD hyperparameters\n",
    "delta = 5e-11#4.87e-11          # Small delta -> Better Approximation of the Moreau Envelope Provided Large T or Close to Global Minimum\n",
    "t =   5e+7 #3.69            # Large T -> Better Exploration (Moreau Envelope closer to approximate quadratic)\n",
    "int_samples = int(1000)  # Large N -> Better Expectations for Larger T -> Better Exploration (But more expensive)\n",
    "max_iters = int(2000)   # Maximum number of iterations\n",
    "loss_tol = 9e-5         # Stopping criterion for the loss\n",
    "sat_tol = 1e-3          # Stopping criterion for the saturation\n",
    "delta_dampener=0.99#95     # Dampening factor for delta (0.8 <= delta_dampener < 1.0 ish)\n",
    "beta=0.9                # Set beta to 0.0 to turn off \n",
    "momentum=0.0            # Set momentum to 0.0 to turn off\n",
    "distribution=\"Gaussian\" # Set sampling distribution \n",
    "\n",
    "# Use HJ_MAD to optimize the model parameters\n",
    "HJ_MAD_alg = HJ_MD_LS(delta=delta, t=t,distribution=distribution,momentum=momentum,\n",
    "                      int_samples=int_samples, max_iters=max_iters, f_tol=loss_tol,\n",
    "                      verbose=True, adaptive_time=True,adaptive_delta=True,\n",
    "                      line_search=False, stepsize=1.0)\n",
    "\n",
    "# Get the model parameters as a flattened tensor\n",
    "parameters = list(model.parameters())\n",
    "parameter_vector = torch.cat([param.flatten() for param in parameters])\n",
    "parameter_vector = parameter_vector.unsqueeze(0) # shape (1, num_features)\n",
    "\n",
    "# Compute Current Loss\n",
    "loss_old = f(parameter_vector)  # Compute the loss for the current parameters\n",
    "\n",
    "# Run HJ_MAD to optimize the model parameters\n",
    "start_time = time.time()\n",
    "new_parameters, loss, loss_history, delta_hist, tk_hist, iterations = HJ_MAD_alg.run(f, parameter_vector)  # Run HJ_MAD to optimize the model parameters\n",
    "elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "\n",
    "func_evals = 2*iterations * (int_samples)\n",
    "print(\"\\n=== Optimization Results ===\")\n",
    "print(f\"Function Evaluations: {func_evals}\")\n",
    "print(f\"Iterations          : {iterations} (Max: {max_iters})\")\n",
    "print(f\"Elapsed Time        : {elapsed_time:.2f} seconds\")\n",
    "print(f\"Final Loss          : {f(new_parameters).item():.5e}\")\n",
    "print(\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(loss_history, label='Loss History', color='blue')\n",
    "plt.title('Loss History during Optimization')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the loss history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(delta_hist, label='delta History', color='blue')\n",
    "plt.title('Delta History during Optimization')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Delta')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(tk_hist, label='t History', color='blue')\n",
    "plt.title('T History during Optimization')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('t')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualise the learned function\n",
    "plt.figure()\n",
    "plt.plot(x, y, label='True function')\n",
    "plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# from hj_prox_ls import HJ_PROX_LS\n",
    "\n",
    "# # Hyperparameters\n",
    "# n_neurons = 50  # Number of neurons in the shallow network\n",
    "# learning_rate = 0.01\n",
    "# num_epochs = 1000\n",
    "# batch_size = 100\n",
    "\n",
    "# # Generate data\n",
    "# no_of_samples = 500\n",
    "# noise_level = 1e-3\n",
    "# x = np.linspace(0, 2 * np.pi, no_of_samples)\n",
    "# y = np.sin(x) + noise_level * np.random.randn(*x.shape)  # Add noise to the samples\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "# y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Create dataset and dataloader\n",
    "# dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Define the shallow neural network\n",
    "# class ShallowNet(nn.Module):\n",
    "#     def __init__(self, n_neurons):\n",
    "#         super(ShallowNet, self).__init__()\n",
    "#         self.linear = nn.Linear(1, n_neurons)\n",
    "#         self.coeffs = nn.Parameter(torch.randn(n_neurons, 1))\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear(x)      # Compute w_j * x + b_j\n",
    "#         x = self.relu(x)        # Apply ReLU activation\n",
    "#         x = x @ self.coeffs     # Compute sum of c_j * ReLU(...)\n",
    "#         return x\n",
    "\n",
    "#     # def set_parameters(self, new_parameters):\n",
    "#     #     \"\"\"\n",
    "#     #     Set model parameters from the flattened parameter vector.\n",
    "#     #     This method updates the model's parameters from a single flattened tensor.\n",
    "#     #     \"\"\"\n",
    "#     #     offset = 0\n",
    "#     #     for param in self.parameters():\n",
    "#     #         param_size = param.numel() \n",
    "#     #         new_param_values = new_parameters[offset:offset + param_size]\n",
    "#     #         new_param_values = new_param_values.view(param.shape)  \n",
    "#     #         param.data.copy_(new_param_values)  \n",
    "#     #         offset += param_size  \n",
    "\n",
    "#     def set_parameters(self, new_parameters):\n",
    "#         \"\"\"\n",
    "#         Set model parameters from the flattened parameter vector.\n",
    "#         This method updates the model's parameters from a single flattened tensor.\n",
    "\n",
    "#         Args:\n",
    "#             new_parameters: A flattened tensor containing all parameters.\n",
    "#         \"\"\"\n",
    "#         offset = 0\n",
    "#         for param in self.parameters():\n",
    "#             param_size = param.numel()  # Total number of elements in the parameter\n",
    "#             # Extract the relevant slice and reshape it to the parameter's original shape\n",
    "#             if len(new_parameters.shape) == 1:\n",
    "#                 new_param_values = new_parameters[offset:offset + param_size]\n",
    "#             else:\n",
    "#                 new_param_values = new_parameters[0, offset:offset + param_size]  # Access batch dimension\n",
    "#             new_param_values = new_param_values.view_as(param)  # Match the original shape\n",
    "#             param.data.copy_(new_param_values)  # Update parameter\n",
    "#             offset += param_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize model, loss function, and optimizer\n",
    "# model = ShallowNet(n_neurons)\n",
    "# criterion = nn.MSELoss()\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # HJ_MAD hyperparameters\n",
    "# delta0 = 1\n",
    "# t = 1\n",
    "# int_samples = int(100)\n",
    "# delta_dampener=0.8\n",
    "# beta=0.0\n",
    "# first_moment = None\n",
    "\n",
    "# # Use HJ_MAD to optimize the model parameters\n",
    "# HJ_PROX_LS_alg = HJ_PROX_LS(delta=delta0, t=t, int_samples=int_samples,delta_dampener=delta_dampener,beta=beta,verbose=True)\n",
    "\n",
    "# # Define the function f (e.g., a loss function based on the model's predictions)\n",
    "# def Objective_function(x, model, inputs, targets):\n",
    "#     \"\"\"\n",
    "#     Computes the loss based on the given model parameters.\n",
    "\n",
    "#     Args:\n",
    "#         x: Tensor of shape (n_samples, n_features) representing flattened parameters.\n",
    "#         model: The neural network model.\n",
    "#         inputs: Input batch.\n",
    "#         targets: Ground truth values.\n",
    "\n",
    "#     Returns:\n",
    "#         Tensor of shape (n_samples,) containing the loss for each sample.\n",
    "#     \"\"\"\n",
    "#     n_samples = x.shape[0]\n",
    "    \n",
    "#     # Initialize list to store losses\n",
    "#     sample_losses = []\n",
    "\n",
    "#     # Compute losses for each sample in the batch\n",
    "#     for i in range(n_samples):\n",
    "#         # Extract parameters for the current sample\n",
    "#         model_params = x[i, :]  # Take the i-th sample\n",
    "#         model.set_parameters(model_params)  # Update model parameters\n",
    "\n",
    "#         # Forward pass and loss computation\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         sample_losses.append(loss.item())\n",
    "\n",
    "#     # Convert the list of losses to a tensor\n",
    "#     return torch.tensor(sample_losses)\n",
    "\n",
    "\n",
    "# # Training loop: Iterate through the epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Iterate through the dataset in batches\n",
    "#     for inputs, targets in dataloader:\n",
    "#         # Define f\n",
    "#         def f(x):\n",
    "#             return Objective_function(x, model, inputs, targets)\n",
    "        \n",
    "#         # Get the model parameters as a flattened tensor\n",
    "#         parameters = list(model.parameters())\n",
    "#         parameter_vector = torch.cat([param.flatten() for param in parameters])\n",
    "#         parameter_vector = parameter_vector.unsqueeze(0) # shape (1, num_features)\n",
    "\n",
    "#         # Compute Current Loss\n",
    "#         loss_old = f(parameter_vector)  # Compute the loss for the current parameters\n",
    "\n",
    "#         # Run HJ_MAD to optimize the model parameters\n",
    "#         new_parameters, loss, first_moment = HJ_PROX_LS_alg.run(f, parameter_vector)  # Run HJ_MAD to optimize the model parameters\n",
    "\n",
    "#         # Update the model parameters with the optimized values\n",
    "#         model.set_parameters(new_parameters)\n",
    "        \n",
    "#     # Print the loss every 100 epochs for monitoring\n",
    "#     if (epoch + 1) % 100 == 0:\n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Delta: {HJ_PROX_LS_alg.delta}')\n",
    "\n",
    "# # Visualise the learned function\n",
    "# plt.figure()\n",
    "# plt.plot(x, y, label='True function')\n",
    "# plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "n_neurons = 50  # Number of neurons in the shallow network\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "# Generate data\n",
    "no_of_samples = 500\n",
    "noise_level = 1e-3\n",
    "x = np.linspace(0, 2 * np.pi, no_of_samples)\n",
    "y = np.sin(x) + noise_level * np.random.randn(*x.shape)  # Add noise to the samples\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the shallow neural network\n",
    "class ShallowNet(nn.Module):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(ShallowNet, self).__init__()\n",
    "        self.linear = nn.Linear(1, n_neurons)\n",
    "        self.coeffs = nn.Parameter(torch.randn(n_neurons, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)      # Compute w_j * x + b_j\n",
    "        x = self.relu(x)        # Apply ReLU activation\n",
    "        x = x @ self.coeffs     # Compute sum of c_j * ReLU(...)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ShallowNet(n_neurons)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Visualise the learned function\n",
    "plt.figure()\n",
    "plt.plot(x, y, label='True function')\n",
    "plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
