{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPOilvdqgxGu"
   },
   "source": [
    "# This notebook is based on the paper: \"Global-Convergence-Nonconvex-Optimization\". \n",
    "\n",
    "The aim of this project is to find the global solution to  \n",
    "\\begin{equation}\n",
    "  \\min_{x \\in \\mathbb{R}^n} f(x).\n",
    "\\end{equation}\n",
    "\n",
    "To obtain a global minimizer, the main idea is to minimize the Moreau Envelop instead, which \"convexifies\" the original function. To make the Moreau envelope tractable, we use connections to Hamilton-Jacobi Equations via the Cole-Hopf and Hopf-Lax formulas to efficiently compute the gradients of the Moreau envelope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdiGOuXQA8qO",
    "outputId": "9d09c67a-aa8a-4138-9582-f52a98dc3a32"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from scipy.integrate import quad\n",
    "from scipy.special import roots_hermite\n",
    "from typing import Optional \n",
    "\n",
    "epsilon_double = np.finfo(np.float64).eps\n",
    "\n",
    "from test_functions import Griewank, AlpineN1, Drop_Wave, Levy, Rastrigin, Ackley\n",
    "from test_functions import Griewank_numpy, AlpineN1_numpy, Drop_Wave_numpy, Levy_numpy, Rastrigin_numpy, Ackley_numpy\n",
    "\n",
    "from test_functions import MultiMinimaFunc, MultiMinimaAbsFunc\n",
    "from test_functions import MultiMinimaFunc_numpy, MultiMinimaAbsFunc_numpy\n",
    "\n",
    "seed   = 30\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq1zNUTPg1JJ"
   },
   "source": [
    "### Create Class for Hamilton-Jacobi Moreau Adaptive Descent (HJ_MAD)\n",
    "\n",
    "The Moreau envelop of $f$ is given by\n",
    "\\begin{equation}\n",
    "  u(x,t) \\triangleq \\inf_{z\\in \\mathbb{R}^n} f(z) + \\dfrac{1}{2t}\\|z-x\\|^2.\n",
    "\\end{equation}\n",
    "\n",
    "We leverage the fact that the solution to the Moreau envelope above satisfies the Hamilton-Jacobi Equation\n",
    "\\begin{equation}\n",
    "  \\begin{split}\n",
    "    u_t^\\delta  + \\frac{1}{2}\\|Du^\\delta  \\|^2 \\ = \\frac{\\delta}{2} \\Delta u^\\delta \\qquad &\\text{ in }  \\mathbb{R}^n\\times (0,T]\n",
    "    \\\\\n",
    "    u = f \\qquad &\\text{ in } \\mathbb{R}^n\\times \\{t = 0\\}\n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "when $\\delta = 0$. \n",
    "\n",
    "By adding a viscous term ($\\delta > 0$), we are able to approximate the solution to the HJ equation using the Cole-Hopf formula to obtain\n",
    "\\begin{equation}\n",
    "  u^\\delta(x,t) = - \\delta \\ln\\Big(\\Phi_t * \\exp(-f/\\delta)\\Big)(x) = - \\delta \\ln \\int_{\\mathbb{R}^n} \\Phi(x-y,t)  \\exp\\left(\\frac{-f(y)}{\\delta}\\right) dy \n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{equation}\n",
    "  \\Phi(x,t) = \\frac{1}{{(4\\pi \\delta t)}^{n/2}} \\exp{\\frac{-\\|x\\|^2}{4\\delta t}}. \n",
    "\\end{equation}\n",
    "This allows us to write the Moreau Envelope (and its gradient) explicitly as an expectation. In this case, we compute the gradient as\n",
    "\\begin{equation}\n",
    "  \\nabla u^\\delta(x,t) = \\dfrac{1}{t}\\cdot  \\dfrac{\\mathbb{E}_{y\\sim  \\mathbb{P}_{x,t}}\\left[(x-y) \\exp\\left(-\\delta^{-1}\\tilde{f}(y)\\right) \\right]}\n",
    "    {\\mathbb{E}_{y\\sim  \\mathbb{P}_{x,t}}\\left[ \\exp\\left(-\\delta^{-1} \\tilde{f}(y)\\right) \\right]}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLVImqaPe-ph"
   },
   "source": [
    "### Define classes for HJ-MAD, Pure Random Search, and Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aI9QUu3vA9Yj"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# HJ Moreau Adaptive Descent\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class HJ_MAD:\n",
    "    ''' \n",
    "        Hamilton-Jacobi Moreau Adaptive Descent (HJ_MAD) is used to solve nonconvex minimization\n",
    "        problems via a zeroth-order sampling scheme.\n",
    "        \n",
    "        Inputs:\n",
    "          1)  f            = function to be minimized. Inputs have size (n_samples x n_features). Outputs have size n_samples\n",
    "          2)  x_true       = true global minimizer\n",
    "          3)  delta        = coefficient of viscous term in the HJ equation\n",
    "          4)  int_samples  = number of samples used to approximate expectation in heat equation solution\n",
    "          5)  x_true       = true global minimizer\n",
    "          6)  t_vec        = time vector containig [initial time, minimum time allowed, maximum time]\n",
    "          7)  max_iters    = max number of iterations\n",
    "          8)  tol          = stopping tolerance\n",
    "          9)  theta        = parameter used to update tk\n",
    "          10) beta         = exponential averaging term for gradient beta (beta multiplies history, 1-beta multiplies current grad)\n",
    "          11) eta_vec      = vector containing [eta_minus, eta_plus], where eta_minus < 1 and eta_plus > 1 (part of time update)\n",
    "          11) alpha        = step size. has to be in between (1-sqrt(eta_minus), 1+sqrt(eta_plus))\n",
    "          12) fixed_time   = boolean for using adaptive time\n",
    "          13) verbose      = boolean for printing\n",
    "          14) momentum     = For acceleration.\n",
    "          15) accelerated  = boolean for using Accelerated Gradient Descent\n",
    "\n",
    "        Outputs:\n",
    "          1) x_opt                    = optimal x_value approximation\n",
    "          2) xk_hist                  = update history\n",
    "          3) tk_hist                  = time history\n",
    "          4) fk_hist                  = function value history\n",
    "          5) xk_error_hist            = error to true solution history \n",
    "          6) rel_grad_uk_norm_hist    = relative grad norm history of Moreau envelope\n",
    "    '''\n",
    "    def __init__(self, f, x_true, delta=0.1, int_samples=100, t_vec = [1.0, 1e-3, 1e1], max_iters=5e4, \n",
    "                 tol=5e-2, theta=0.9, beta=[0.9], eta_vec = [0.9, 1.1], alpha=1.0, fixed_time=False, \n",
    "                 verbose=True,rescale0=1e-1, momentum=None,saturate_tol=1e-9, integration_method='MC',\n",
    "                 rootsGHQ=None):\n",
    "      \n",
    "      self.delta            = delta\n",
    "      self.f                = f\n",
    "      self.int_samples      = int_samples\n",
    "      self.max_iters        = max_iters\n",
    "      self.tol              = tol\n",
    "      self.t_vec            = t_vec\n",
    "      self.theta            = theta\n",
    "      self.x_true           = x_true\n",
    "      self.beta             = beta \n",
    "      self.alpha            = alpha \n",
    "      self.eta_vec          = eta_vec\n",
    "      self.fixed_time       = fixed_time\n",
    "      self.verbose          = verbose\n",
    "      self.momentum         = momentum\n",
    "      self.rescale0         = rescale0\n",
    "      self.saturate_tol     = saturate_tol\n",
    "      self.integration_method = integration_method\n",
    "      if rootsGHQ is not None:\n",
    "        self.rootsGHQ = rootsGHQ\n",
    "\n",
    "      \n",
    "      # check that alpha is in right interval\n",
    "      assert(alpha >= 1-np.sqrt(eta_vec[0]))\n",
    "      assert(alpha <= 1+np.sqrt(eta_vec[1]))\n",
    "\n",
    "    def compute_grad_uk_NMC(self,x, t, dim=slice(None)):\n",
    "      \"\"\"\n",
    "        Compute the gradient of Moreau Envelope of f using Monte Carlo Integration.\n",
    "        \n",
    "        Parameters:\n",
    "            x (float): Point at which to compute the gradient.\n",
    "            t (float): Time parameter.\n",
    "            dim (int): Dimension to compute the gradient.\n",
    "        \n",
    "        Returns:\n",
    "            grad (float): Computed gradient.\n",
    "        \"\"\"\n",
    "      \n",
    "      underflow = 1e-15\n",
    "      overflow = 1e15\n",
    "      rescale_factor = self.rescale0\n",
    "      min_rescale=1e-15\n",
    "      iterations = 0\n",
    "\n",
    "      n_features = x.shape[0]\n",
    "      device = x.device\n",
    "\n",
    "      y = x.expand(self.int_samples, n_features).clone()\n",
    "      while True:\n",
    "        t_rescaled = t/rescale_factor\n",
    "\n",
    "\n",
    "        sqrt_factor = np.sqrt(self.delta * t_rescaled)\n",
    "\n",
    "        # Replace the specified column with random samples\n",
    "        if dim == slice(None):  # randomize all n_features of y\n",
    "          #z = torch.randn(self.int_samples,n_features)\n",
    "          y = x - self.z * sqrt_factor\n",
    "        else:\n",
    "          # y = x.expand(self.int_samples, n_features).clone()\n",
    "          # # Replace the specified column with random samples\n",
    "          # y[:, dim] = standard_dev * torch.randn(self.int_samples) + x[dim]\n",
    "          #z = torch.randn(self.int_samples)\n",
    "          y[:, dim] = x[dim] - self.z * sqrt_factor\n",
    "\n",
    "        f_values = self.f(y)\n",
    "\n",
    "        # Rescale the exponent to prevent overflow/underflow\n",
    "        rescaled_exponent= -rescale_factor*f_values/self.delta\n",
    "\n",
    "        # Remove the maximum exponent to prevent overflow\n",
    "        max_exponent = torch.max(rescaled_exponent) \n",
    "        shifted_exponent = rescaled_exponent - max_exponent\n",
    "\n",
    "        # Find the maximum absolute exponent\n",
    "        # max_abs_exponent = torch.max(torch.abs(rescaled_exponent))\n",
    "        # max_exponent = torch.max(rescaled_exponent)\n",
    "\n",
    "        # # If max_exponent is negative, add the absolute value to prevent underflow\n",
    "        # # If max_exponent is positive, subtract it to prevent overflow\n",
    "        # shifted_exponent = rescaled_exponent + max_abs_exponent if max_exponent < 0 else rescaled_exponent - max_exponent\n",
    "\n",
    "        # Compute the exponential term\n",
    "        exp_term = torch.exp(shifted_exponent)\n",
    "\n",
    "        denominator       = torch.mean(exp_term)\n",
    "\n",
    "        # Check if the denominator is within the underflow/overflow bounds\n",
    "        if (denominator >= underflow and denominator <= overflow) or rescale_factor < min_rescale:\n",
    "          break\n",
    "\n",
    "        # Adjust rescale factor and increment iteration count\n",
    "        rescale_factor /= 2\n",
    "        iterations += 1\n",
    "\n",
    "      # Increase intial rescale factor if it is too small to better utilize samples\n",
    "      # if iterations == 0:\n",
    "      #   self.rescale0 = self.rescale0*2\n",
    "      # else:\n",
    "      # self.rescale0 = rescale_factor\n",
    "\n",
    "      # print(f\"{iterations=}\")\n",
    "\n",
    "      # THIS IS IMPORTANT FOR TUNING INITIAL RESCALE FACTOR\n",
    "      # print(f'Loops to find rescale factor: {loop_iterations}')\n",
    "\n",
    "      # Replace the specified column with random samples\n",
    "      if dim == slice(None):  # randomize all n_features of y\n",
    "        numerator = self.z*exp_term.view(self.int_samples, 1)\n",
    "      else:\n",
    "        numerator = self.z*exp_term\n",
    "\n",
    "      numerator = torch.mean(numerator)\n",
    "        \n",
    "      # Compute Grad U\n",
    "      grad_uk_1D = sqrt_factor*numerator/denominator # the t gets canceled with the update formula\n",
    "      grad_uk = torch.zeros(n_features, dtype=torch.float64, device=device)\n",
    "      grad_uk[dim] = grad_uk_1D\n",
    "\n",
    "      # Compute Estimated prox_xk\n",
    "      prox_xk = x - grad_uk\n",
    "\n",
    "      #print(f\"{grad_uk[dim]=}\")\n",
    "\n",
    "      return grad_uk, prox_xk\n",
    "\n",
    "    def compute_grad_uk_MC(self,x, t, dim=slice(None)):\n",
    "      \"\"\"\n",
    "        Compute the gradient of Moreau Envelope of f using Monte Carlo Integration.\n",
    "        \n",
    "        Parameters:\n",
    "            x (float): Point at which to compute the gradient.\n",
    "            t (float): Time parameter.\n",
    "            dim (int): Dimension to compute the gradient.\n",
    "        \n",
    "        Returns:\n",
    "            grad (float): Computed gradient.\n",
    "        \"\"\"\n",
    "      \n",
    "      underflow = 1e-15\n",
    "      overflow = 1e15\n",
    "      rescale_factor = self.rescale0\n",
    "      min_rescale=1e-10\n",
    "      iterations = 0\n",
    "\n",
    "      while True:\n",
    "        n_features = x.shape[0]\n",
    "        standard_dev = np.sqrt(self.delta*t/rescale_factor)\n",
    "\n",
    "        #samples = self.int_samples\n",
    "        \n",
    "        #y = standard_dev * torch.randn(samples, n_features) + x\n",
    "        y = x.expand(self.int_samples, n_features).clone()\n",
    "\n",
    "        if dim == slice(None):  # randomize all n_features of y\n",
    "          y = standard_dev * self.z + x#torch.randn(self.int_samples, n_features) + x\n",
    "        else:\n",
    "          y = x.expand(self.int_samples, n_features).clone()\n",
    "          # Replace the specified column with random samples\n",
    "          y[:, dim] = standard_dev * self.z + x[dim] # torch.randn(self.int_samples) + x[dim]\n",
    "\n",
    "        f_values = self.f(y)\n",
    "\n",
    "        rescaled_exponent = -rescale_factor*f_values/self.delta\n",
    "        # Remove the maximum exponent to prevent overflow\n",
    "        max_exponent = torch.max(rescaled_exponent) \n",
    "        shifted_exponent = rescaled_exponent - max_exponent\n",
    "\n",
    "        exp_term = torch.exp(shifted_exponent)\n",
    "        v_delta       = torch.mean(exp_term)\n",
    "\n",
    "        # print(f'v_delta = {v_delta}')\n",
    "\n",
    "        if (v_delta >= underflow and v_delta <= overflow) or rescale_factor < min_rescale:\n",
    "          break\n",
    "\n",
    "        # Adjust rescale factor and increment iteration count\n",
    "        rescale_factor /= 2\n",
    "        iterations += 1\n",
    "\n",
    "      # Increase intial rescale factor if it is too small to better utilize samples\n",
    "      if iterations == 0:\n",
    "        self.rescale0 = self.rescale0*2\n",
    "      else:\n",
    "        self.rescale0 = rescale_factor\n",
    "\n",
    "      # THIS IS IMPORTANT FOR TUNING INITIAL RESCALE FACTOR\n",
    "      # print(f'Loops to find rescale factor: {loop_iterations}')\n",
    "      numerator = y*exp_term.view(self.int_samples, 1)\n",
    "      numerator = torch.mean(numerator, dim=0)\n",
    "        \n",
    "      # Compute Grad U\n",
    "      # TODO: Check if this is correct\n",
    "      grad_uk = (x -  numerator/(v_delta)) # the t gets canceled with the update formula\n",
    "\n",
    "      # Compute Moreau envelope\n",
    "      #uk = -self.delta * torch.log(v_delta)\n",
    "\n",
    "      # Compute Estimated prox_xk\n",
    "      prox_xk = numerator / (v_delta)\n",
    "\n",
    "      return grad_uk, prox_xk\n",
    "  \n",
    "\n",
    "    def compute_grad_uk_GHQ(self, x, t, dim):\n",
    "        \"\"\"\n",
    "        Compute the gradient of Moreau Envelope of f using Gauss-Hermite quadrature.\n",
    "        \n",
    "        Parameters:\n",
    "            x (float): Point at which to compute the gradient.\n",
    "            t (float): Time parameter.\n",
    "            dim (int): Dimension to compute the gradient.\n",
    "        \n",
    "        Returns:\n",
    "            grad (float): Computed gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        n_features = x.shape[0]\n",
    "        min_rescale=1e-15\n",
    "        underflow = 1e-15\n",
    "        overflow = 1e15\n",
    "\n",
    "        z, weights = self.rootsGHQ\n",
    "\n",
    "        # Define rescale_factor for preventing overflow/underflow\n",
    "        rescale_factor = self.rescale0\n",
    "        rescale_counter = 0\n",
    "\n",
    "        # The device on which the computation is done\n",
    "        device = x.device\n",
    "\n",
    "        # Repeat x for all samples\n",
    "        y = x.clone().expand(self.int_samples,n_features)\n",
    "        y = y.clone().contiguous()\n",
    "\n",
    "        while True:\n",
    "          # Rescale t\n",
    "          t_rescaled = t/rescale_factor\n",
    "\n",
    "          # Compute the roots and weights for the Hermite quadrature\n",
    "\n",
    "          # Compute the integral of the exponential for f in 1 Dimension\n",
    "          y_1D = x[dim] - z*np.sqrt(2*self.delta*t_rescaled) # Size (int_samples,1)\n",
    "          y[:,dim] = y_1D # Update y only in the dimension we are moving.\n",
    "\n",
    "          rescaled_f_exponent = - rescale_factor*self.f(y)/ self.delta\n",
    "          max_exponent = torch.max(rescaled_f_exponent)  # Find the maximum exponent\n",
    "          shifted_exponent = rescaled_f_exponent - max_exponent\n",
    "          F_exp = torch.exp(shifted_exponent)\n",
    "\n",
    "\n",
    "          # Compute the Denominator Integral\n",
    "          v_delta = - torch.sum(weights * F_exp) # * (1 / np.sqrt(np.pi)) \n",
    "\n",
    "          # Make sure over/underflow does not occur in the Denominator\n",
    "          if (v_delta >= underflow and v_delta <= overflow) or rescale_factor < min_rescale:\n",
    "            break\n",
    "          \n",
    "          # Adjust rescale factor and increment iteration count\n",
    "          rescale_factor /= 2\n",
    "          rescale_counter += 1\n",
    "        \n",
    "        # Increase intial rescale factor if it is too small to better utilize samples\n",
    "        if rescale_counter == 0:\n",
    "          self.rescale0 = self.rescale0*2\n",
    "        else:\n",
    "          self.rescale0 = rescale_factor\n",
    "\n",
    "        #print(f\"{rescale_counter=}\")\n",
    "\n",
    "        # Compute Numerator Integral\n",
    "        grad_v_delta_F = z * F_exp\n",
    "          \n",
    "        # numerator = - np.sqrt(2/(self.delta*t_rescaled)) * torch.sum(weights * grad_v_delta_F) # * (1 / np.sqrt(np.pi)) \n",
    "        numerator = np.sqrt(2/(self.delta*t_rescaled)) * torch.sum(weights * grad_v_delta_F) # * (1 / np.sqrt(np.pi)) \n",
    "\n",
    "        # Compute Gradient in 1D\n",
    "        grad_uk_1D = - self.delta * numerator / v_delta\n",
    "        grad_uk = torch.zeros(n_features, dtype=torch.float64, device=device)\n",
    "        grad_uk[dim] = grad_uk_1D\n",
    "\n",
    "        # Compute Prox_xk\n",
    "        prox_xk = x - t_rescaled*grad_uk\n",
    "        \n",
    "        return t_rescaled*grad_uk, prox_xk\n",
    "    \n",
    "    def compute_grad_uk_quad(self, x, t, dim):\n",
    "      \"\"\"\n",
    "      Compute the gradient of Moreau Envelope of f using SciPy's quad adaptive integration.\n",
    "      \n",
    "      Parameters:\n",
    "          x (torch.Tensor): Point at which to compute the gradient.\n",
    "          t (float): Time parameter.\n",
    "          dim (int): Dimension to compute the gradient.\n",
    "      \n",
    "      Returns:\n",
    "          grad (torch.Tensor): Computed gradient.\n",
    "          prox_xk (torch.Tensor): Proximal operator result.\n",
    "      \"\"\"\n",
    "\n",
    "      n_features = x.shape[0]\n",
    "      min_rescale = 1e-15\n",
    "      underflow = 1e-15\n",
    "      overflow = 1e15\n",
    "\n",
    "      # Define rescale_factor for preventing overflow/underflow\n",
    "      rescale_factor = self.rescale0\n",
    "      rescale_counter = 0\n",
    "\n",
    "      # The device on which the computation is done\n",
    "      device = x.device\n",
    "\n",
    "      while True:\n",
    "          # Rescale t\n",
    "          t_rescaled = t / rescale_factor\n",
    "          sqrt_factor = np.sqrt(2 * self.delta * t_rescaled)\n",
    "\n",
    "\n",
    "          # Define the rescaled function for numerator and denominator\n",
    "          def integrand(z, for_numerator=False):\n",
    "              y = x.clone()\n",
    "              y[dim] = x[dim] - z * sqrt_factor\n",
    "              rescaled_f_exponent = -rescale_factor * self.f(y.view(1,-1)) / self.delta\n",
    "              exp_term = torch.exp(rescaled_f_exponent - z**2)\n",
    "              \n",
    "              if for_numerator:\n",
    "                  return z * exp_term.item()\n",
    "              else:\n",
    "                  return exp_term.item()\n",
    "\n",
    "          # Compute denominator integral (v_delta)\n",
    "          denominator_result, _ = quad(\n",
    "              integrand, \n",
    "              -10, \n",
    "              10, \n",
    "              args=(False,)\n",
    "          )\n",
    "\n",
    "          # Ensure no over/underflow occurs\n",
    "          if (underflow <= denominator_result <= overflow) or rescale_factor < min_rescale:\n",
    "              print(f\"{denominator_result=}\")\n",
    "              break\n",
    "\n",
    "          # Adjust rescale factor and increment iteration count\n",
    "          rescale_factor /= 2\n",
    "          rescale_counter += 1\n",
    "\n",
    "      # Update initial rescale factor if too small to better utilize samples\n",
    "      # if rescale_counter == 0:\n",
    "      #     self.rescale0 *= 2\n",
    "      # else:\n",
    "      #     self.rescale0 = rescale_factor\n",
    "\n",
    "      # Compute numerator integral\n",
    "      numerator_result, _ = quad(\n",
    "          integrand, \n",
    "          10, \n",
    "          -10, \n",
    "          args=(True,)\n",
    "      )\n",
    "\n",
    "      # Gradient computation\n",
    "      grad_uk_1D = -self.delta * (np.sqrt(2 / (self.delta * t_rescaled)) * numerator_result) / denominator_result\n",
    "      grad_uk = torch.zeros(n_features, dtype=torch.float64, device=device)\n",
    "      grad_uk[dim] = grad_uk_1D\n",
    "\n",
    "      # Compute proximal operator result\n",
    "      prox_xk = x - t_rescaled * grad_uk\n",
    "\n",
    "      return t_rescaled * grad_uk, prox_xk\n",
    "\n",
    "\n",
    "    def gradient_descent(self, xk, tk, update_dim=slice(None)):\n",
    "        # Compute prox and gradient\n",
    "        if self.integration_method == 'MC':\n",
    "          grad_uk, prox_xk = self.compute_grad_uk_MC(xk, tk, update_dim)\n",
    "\n",
    "          # Perform gradient descent update\n",
    "          xk_plus1 = xk.clone()\n",
    "          xk_plus1[update_dim] = xk[update_dim] - self.alpha * (xk[update_dim] - prox_xk[update_dim])\n",
    "\n",
    "        elif self.integration_method == 'GHQ':\n",
    "          grad_uk, _ =self.compute_grad_uk_GHQ(xk, tk, update_dim)\n",
    "        \n",
    "          # Perform gradient descent update\n",
    "          xk_plus1 = xk - self.alpha * grad_uk\n",
    "\n",
    "        elif self.integration_method == 'quad':\n",
    "          grad_uk, _ = self.compute_grad_uk_quad(xk, tk, update_dim)\n",
    "\n",
    "          # Perform gradient descent update\n",
    "          xk_plus1 = xk - self.alpha * grad_uk\n",
    "        \n",
    "        elif self.integration_method == 'NMC':\n",
    "          grad_uk, _ = self.compute_grad_uk_NMC(xk, tk, update_dim)\n",
    "\n",
    "          # Perform gradient descent update\n",
    "          xk_plus1 = xk - self.alpha * grad_uk\n",
    "\n",
    "        return xk_plus1, grad_uk\n",
    "\n",
    "    def update_time(self, tk, rel_grad_uk_norm):\n",
    "      '''\n",
    "        time step rule\n",
    "\n",
    "        if ‖gk_plus‖≤ theta (‖gk‖+ eps):\n",
    "          min (eta_plus t,T)\n",
    "        else\n",
    "          max (eta_minus t,t_min) otherwise\n",
    "\n",
    "        OR:\n",
    "        \n",
    "        if rel grad norm too small, increase tk (with maximum T).\n",
    "        else if rel grad norm is too \"big\", decrease tk with minimum (t_min)\n",
    "      '''\n",
    "\n",
    "      eta_minus = self.eta_vec[0]\n",
    "      eta_plus = self.eta_vec[1]\n",
    "      T = self.t_vec[2]\n",
    "      t_min = self.t_vec[1]\n",
    "\n",
    "      if rel_grad_uk_norm <= self.theta:\n",
    "        # increase t when relative gradient norm is smaller than theta\n",
    "        tk = min(eta_plus*tk , T) \n",
    "      else:\n",
    "        # decrease otherwise t when relative gradient norm is smaller than theta\n",
    "        tk = max(eta_minus*tk, t_min)\n",
    "\n",
    "      return tk\n",
    "    \n",
    "    def stopping_criteria(self,k,cd,history):\n",
    "      '''\n",
    "        Stopping Criteria for HJ-MAD and HJ-MAD-CD\n",
    "      '''\n",
    "      xk_hist, xk_error_hist, rel_grad_uk_norm_hist, fk_hist, tk_hist = history\n",
    "\n",
    "      if xk_error_hist[k] < self.tol:\n",
    "          if self.verbose:\n",
    "            print('HJ-MAD converged with rel grad norm {:6.2e}'.format(rel_grad_uk_norm_hist[k]))\n",
    "            print('iter = ', k, ', number of function evaluations = ', len(xk_error_hist)*self.int_samples)\n",
    "          return True\n",
    "      elif k==self.max_iters:\n",
    "        if self.verbose:\n",
    "          print('HJ-MAD failed to converge with rel grad norm {:6.2e}'.format(rel_grad_uk_norm_hist[k]))\n",
    "          print('iter = ', k, ', number of function evaluations = ', len(xk_error_hist)*self.int_samples)\n",
    "          print('Used fixed time = ', self.fixed_time)\n",
    "          return True\n",
    "      if cd:\n",
    "        if k > 0 and np.abs(torch.norm(xk_hist[k] - xk_hist[k-1])) < self.saturate_tol*torch.norm(xk_hist[k-1]): \n",
    "          if self.verbose:\n",
    "            print('HJ-MAD converged due to error saturation with rel grad norm {:6.2e}'.format(rel_grad_uk_norm_hist[k]))\n",
    "            print('iter = ', k, ', number of function evaluations = ', len(xk_error_hist)*self.int_samples)\n",
    "          return True\n",
    "        elif k > 10 and np.sum(np.diff(xk_error_hist[k-10:k+1]) > 0) > 3: # TODO: Needs to be Removed and Replaced with stopping criterion below\n",
    "          if self.verbose:\n",
    "            print('HJ-MAD stopped due to non-monotonic error decrease with rel grad norm {:6.2e}'.format(rel_grad_uk_norm_hist[k]))\n",
    "            print('iter = ', k, ', number of function evaluations = ', len(xk_error_hist)*self.int_samples)\n",
    "          return True\n",
    "        # elif k > 20 and torch.std(fk_hist[k-20:k+1]) < self.tol:\n",
    "        #   print('HJ-MAD converged due to oscillating fk with rel grad norm {:6.2e}'.format(rel_grad_uk_norm_hist[k]))\n",
    "        #   print('iter = ', k, ', number of function evaluations = ', len(xk_error_hist)*int_samples)\n",
    "        #   return True\n",
    "\n",
    "    \n",
    "    def run(self, x0, cd=False, update_dim=slice(None)):\n",
    "      \"\"\"\n",
    "      Run the HJ-MAD algorithm to minimize the function.\n",
    "\n",
    "      Parameters:\n",
    "      x0 (torch.Tensor): Initial guess for the minimizer.\n",
    "      cd (bool): Coordinate descent flag.\n",
    "      update_dim (slice): Dimension to update.\n",
    "\n",
    "      Returns:\n",
    "      x_opt (torch.Tensor): Optimal x value approximation.\n",
    "      xk_hist (torch.Tensor): Update history.\n",
    "      tk_hist (torch.Tensor): Time history.\n",
    "      xk_error_hist (torch.Tensor): Error to true solution history.\n",
    "      rel_grad_uk_norm_hist (torch.Tensor): Relative grad norm history of Moreau envelope.\n",
    "      fk_hist (torch.Tensor): Function value history.\n",
    "      \"\"\"\n",
    "      # Dimensions of x0\n",
    "      n_features = x0.shape[0]\n",
    "\n",
    "      # Initialize history tensors\n",
    "      xk_hist = torch.zeros(self.max_iters, n_features)\n",
    "      xk_error_hist = torch.zeros(self.max_iters)\n",
    "      rel_grad_uk_norm_hist = torch.zeros(self.max_iters)\n",
    "      fk_hist = torch.zeros(self.max_iters)\n",
    "      tk_hist = torch.zeros(self.max_iters)\n",
    "\n",
    "      # Initialize iteration variables x and t\n",
    "      xk = x0\n",
    "      x_opt = xk\n",
    "      tk = self.t_vec[0]\n",
    "\n",
    "      # Set up Momentum\n",
    "      if self.momentum is not None:\n",
    "        xk_minus_1 = xk\n",
    "\n",
    "      rel_grad_uk_norm = 1.0\n",
    "\n",
    "      if self.integration_method == 'NMC' or self.integration_method == 'MC':\n",
    "        if update_dim == slice(None):  # randomize all n_features of y\n",
    "          self.z = torch.randn(self.int_samples,n_features)\n",
    "        else:\n",
    "          self.z = torch.randn(self.int_samples)\n",
    "\n",
    "      fmt = '[{:3d}]: fk = {:6.2e} | xk_err = {:6.2e} | |grad_uk| = {:6.2e} | tk = {:6.2e}'\n",
    "      if self.verbose:\n",
    "        print('-------------------------- RUNNING HJ-MAD ---------------------------')\n",
    "        print('dimension = ', n_features, 'n_samples = ', self.int_samples)\n",
    "\n",
    "      # Compute initial gradient\n",
    "      _ , grad_uk = self.gradient_descent(xk, tk, update_dim)\n",
    "\n",
    "\n",
    "      for k in range(self.max_iters):\n",
    "        # Store current state in history\n",
    "        xk_hist[k, :] = xk\n",
    "        rel_grad_uk_norm_hist[k] = rel_grad_uk_norm\n",
    "        xk_error_hist[k] = torch.norm(xk - self.x_true)\n",
    "        tk_hist[k] = tk\n",
    "        fk_hist[k] = self.f(xk.view(1, n_features))\n",
    "\n",
    "        if self.verbose:\n",
    "          print(fmt.format(k + 1, fk_hist[k], xk_error_hist[k], rel_grad_uk_norm_hist[k], tk))\n",
    "\n",
    "        # Check for convergence\n",
    "        if self.stopping_criteria(k, cd, [xk_hist, xk_error_hist, rel_grad_uk_norm_hist, fk_hist, tk_hist]):\n",
    "          break\n",
    "\n",
    "        if k > 0 and fk_hist[k] < fk_hist[k - 1]:\n",
    "          x_opt = xk\n",
    "\n",
    "        grad_uk_norm_old = torch.norm(grad_uk)\n",
    "\n",
    "        # Accelerate gradient descent if momentum is not None\n",
    "        if self.momentum is not None and k > 0:\n",
    "          yk = xk.clone()\n",
    "          yk[update_dim] = xk[update_dim] + self.momentum * (xk[update_dim] - xk_minus_1[update_dim])\n",
    "          xk_minus_1[update_dim] = xk[update_dim]\n",
    "        else:\n",
    "          yk = xk.clone()\n",
    "\n",
    "        # Perform gradient descent\n",
    "        xk, grad_uk = self.gradient_descent(yk, tk, update_dim)\n",
    "\n",
    "        # Compute relative gradients\n",
    "        grad_uk_norm = torch.norm(grad_uk)\n",
    "        rel_grad_uk_norm = grad_uk_norm / (grad_uk_norm_old + 1e-12)\n",
    "\n",
    "        # Update tk\n",
    "        if not self.fixed_time:\n",
    "          tk = self.update_time(tk, rel_grad_uk_norm)\n",
    "\n",
    "  \n",
    "      return x_opt, xk_hist[0:k+1,:], tk_hist[0:k+1], xk_error_hist[0:k+1], rel_grad_uk_norm_hist[0:k+1], fk_hist[0:k+1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HJ_MAD_CoordinateDescent(HJ_MAD):\n",
    "    \"\"\"\n",
    "    Hamilton-Jacobi Moreau Adaptive Descent (HJ_MAD) Coordinate Descent for 2D functions.\n",
    "\n",
    "    This class extends the HJ_MAD algorithm to perform coordinate descent for 2D functions. It alternates between \n",
    "    optimizing each coordinate while keeping the other fixed, treating the function as a 1D function for each run \n",
    "    and using the previous solution in the next run.\n",
    "\n",
    "    Attributes:\n",
    "        f (callable): The function to be minimized.\n",
    "        x_true (torch.Tensor): The true global minimizer.\n",
    "        delta (float): Coefficient of the viscous term in the HJ equation.\n",
    "        int_samples (int): Number of samples used to approximate expectation in the heat equation solution.\n",
    "        t_vec (list): Time vector containing [initial time, minimum time allowed, maximum time].\n",
    "        max_iters (int): Maximum number of iterations.\n",
    "        tol (float): Stopping tolerance.\n",
    "        alpha (float): Step size.\n",
    "        beta (float): Exponential averaging term for gradient beta.\n",
    "        eta_vec (list): Vector containing [eta_minus, eta_plus].\n",
    "        theta (float): Parameter used to update tk.\n",
    "        fixed_time (bool): Whether to use adaptive time.\n",
    "        verbose (bool): Whether to print progress.\n",
    "        rescale0 (float): Initial rescale factor.\n",
    "        momentum (float): Momentum term for acceleration.\n",
    "\n",
    "    Methods:\n",
    "        run(x0, num_cycles): Runs the coordinate descent optimization process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f, x_true, delta=0.1, int_samples=100, t_vec=[1.0, 1e-3, 1e1], max_iters=5e4,\n",
    "                 tol=5e-2, theta=0.9, beta=[0.9], eta_vec=[0.9, 1.1], alpha=1.0, fixed_time=False,\n",
    "                 plot=False, verbose=True, rescale0=1e-1, momentum=None,saturate_tol=1e-9,integration_method='MC'):\n",
    "        self.tol = tol\n",
    "        self.plot = plot\n",
    "\n",
    "        if integration_method == 'GHQ':\n",
    "            device = x_true.device\n",
    "            z, weights = roots_hermite(int_samples)\n",
    "            z = torch.tensor(z, dtype=torch.float64, device=device)\n",
    "            weights = torch.tensor(weights, dtype=torch.float64, device=device)\n",
    "            rootsGHQ = (z, weights)\n",
    "        else:\n",
    "            rootsGHQ = None\n",
    "\n",
    "        super().__init__(f=f, x_true=x_true, delta=delta, int_samples=int_samples, t_vec=t_vec, max_iters=max_iters,\n",
    "                         tol=self.tol, alpha=alpha, beta=beta, eta_vec=eta_vec, theta=theta, fixed_time=fixed_time,\n",
    "                         verbose=verbose, rescale0=rescale0, momentum=momentum,saturate_tol=saturate_tol,integration_method=integration_method,\n",
    "                         rootsGHQ=rootsGHQ)\n",
    "\n",
    "    def plot_1d_descent(self, xk, xk_new, dim, domain=(-15, 15), num_points=1000):\n",
    "        \"\"\"\n",
    "        Plots the 1D descent for the current dimension.\n",
    "\n",
    "        Args:\n",
    "            xk (torch.Tensor): Current position.\n",
    "            dim (int): The current dimension being optimized.\n",
    "            domain (tuple): The range over which to vary the current dimension.\n",
    "            num_points (int): Number of points to sample in the domain.\n",
    "        \"\"\"\n",
    "        x_vals = np.linspace(domain[0], domain[1], num_points)\n",
    "        f_vals = []\n",
    "        h_vals = []\n",
    "\n",
    "        for x in x_vals:\n",
    "            xk_varied = xk.clone()\n",
    "            xk_varied[dim] = x\n",
    "            f_vals.append(self.f(xk_varied.unsqueeze(0)).item())\n",
    "            h_vals.append(self.f(xk_varied.unsqueeze(0)).item() + 1/(2*self.t_vec[0])*torch.norm(xk_varied-xk)**2)\n",
    "\n",
    "        std_dev = np.sqrt(self.delta * self.t_vec[0]/self.rescale0)\n",
    "        std_dev_minus = xk.clone()\n",
    "        std_dev_plus = xk.clone()\n",
    "        std_dev_minus[dim] -= std_dev\n",
    "        std_dev_plus[dim] += std_dev\n",
    "        # print(f\"{self.delta} * {self.t_vec[0]} = {self.delta * self.t_vec[0]}\")\n",
    "        # print(f'Std Dev: {std_dev}, Std Dev Minus: {std_dev_minus}, Std Dev Plus: {std_dev_plus}')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(x_vals, f_vals, '-', color='black', label=f'f(x) Dimension {dim + 1}')\n",
    "        plt.plot(x_vals, h_vals, '-', color='blue', label=r'$f(x) + \\frac{1}{2t_0} ||x - x_k||^2$')\n",
    "        plt.plot(xk[dim], self.f(xk.unsqueeze(0)).item(), '*', color='red', label=f'xk Dimension {dim + 1}')\n",
    "        plt.plot(xk_new[dim], self.f(xk_new.unsqueeze(0)).item(), '*', color='green', label=f'New xk Dimension {dim + 1}')\n",
    "        plt.plot(std_dev_minus[dim].item(), self.f(std_dev_minus.unsqueeze(0)).item(), 'x', color='purple', label='Std Devs')\n",
    "        plt.plot(std_dev_plus[dim].item(), self.f(std_dev_plus.unsqueeze(0)).item(), 'x', color='purple')\n",
    "        plt.xlabel(f'Dimension {dim + 1}')\n",
    "        plt.ylabel('Function Value')\n",
    "        plt.title(f'1D Descent for Dimension {dim + 1}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def run(self, x0, num_cycles=10):\n",
    "        \"\"\"\n",
    "        Runs the coordinate descent optimization process.\n",
    "\n",
    "        Args:\n",
    "            x0 (torch.Tensor): Initial guess for the minimizer.\n",
    "            num_cycles (int): Number of cycles to run the coordinate descent.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Optimal solution found by the coordinate descent.\n",
    "            list: History of solutions for each cycle.\n",
    "            list: History of the entire optimization process.\n",
    "            list: Error history for each cycle.\n",
    "        \"\"\"\n",
    "        xk = x0.clone()\n",
    "        n_features = x0.shape[0]\n",
    "        CD_xk_hist = torch.zeros(n_features*num_cycles+1, n_features)\n",
    "        CD_xk_hist[0,:]    = xk\n",
    "        full_xk_hist = []\n",
    "        full_fk_hist = []\n",
    "        full_xk_error_hist = []\n",
    "\n",
    "        # x_opt, xk_hist, _, xk_error_hist, _, _ = super().run(xk,cd=True)\n",
    "        # xk = x_opt.clone()\n",
    "        # full_history.extend(xk_hist)\n",
    "        # xk_error_hist_MAD.extend(xk_error_hist)\n",
    "\n",
    "        for cycle in range(num_cycles):\n",
    "            dims = list(range(n_features))\n",
    "            #dims = [1, 0]\n",
    "\n",
    "            # # Randomly select 50% of the dimensions\n",
    "            # dims = random.sample(range(n_features), k=n_features // 2)\n",
    "            dim_count=0\n",
    "            for dim in dims:\n",
    "                # if dim == 1:\n",
    "                #     self.t_vec[0] = 800\n",
    "                #     self.t_vec[1] = 0.1\n",
    "                #     self.t_vec[2] = 1000\n",
    "                #     self.delta = 0.1\n",
    "                #     self.rescale0 = 1\n",
    "\n",
    "                # Plot the 1D descent for the current dimension\n",
    "                xk_prev = xk.clone()\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"Cycle {cycle + 1}/{num_cycles} and Dimension {dim + 1}/{n_features}\")\n",
    "\n",
    "                # Optimize with respect to the first coordinate\n",
    "                xk, xk_hist, tk_hist, xk_error_hist, rel_grad_uk_norm_hist, fk_hist = super().run(xk,cd=True, update_dim=dim)\n",
    "\n",
    "                if self.plot:\n",
    "                    self.plot_1d_descent(xk_prev,xk, dim)\n",
    "     \n",
    "                full_xk_hist.extend(xk_hist.numpy())\n",
    "                full_xk_error_hist.extend(xk_error_hist.numpy())\n",
    "                full_fk_hist.extend(fk_hist.numpy())\n",
    "\n",
    "                if  xk_error_hist[-1] < self.tol:\n",
    "                    print(f'HJ-MAD-CD converged. Error: {xk_error_hist[-1]:.3f}, tolerance: {self.tol}.')\n",
    "                    CD_xk_hist[cycle+1,:]    = xk\n",
    "                    X_opt = xk\n",
    "                    return X_opt, CD_xk_hist,full_xk_hist, full_xk_error_hist, full_fk_hist\n",
    "                \n",
    "                CD_xk_hist[cycle+dim_count+1,:]    = xk\n",
    "                dim_count+=1\n",
    "\n",
    "            # if cycle > 0 and cycle % 3 == 0:\n",
    "            #     self.int_samples *= 2\n",
    "\n",
    "            # CD_xk_hist[cycle+1,:]    = xk\n",
    "\n",
    "        X_opt = xk\n",
    "        return X_opt, CD_xk_hist,full_xk_hist, full_xk_error_hist, full_fk_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "class HJ_MAD_CoordinateDescent_parallel(HJ_MAD):\n",
    "    \"\"\"\n",
    "    Hamilton-Jacobi Moreau Adaptive Descent (HJ_MAD) Coordinate Descent for 2D functions.\n",
    "\n",
    "    This class extends the HJ_MAD algorithm to perform coordinate descent for 2D functions. It alternates between \n",
    "    optimizing each coordinate while keeping the other fixed, treating the function as a 1D function for each run \n",
    "    and using the previous solution in the next run.\n",
    "\n",
    "    Attributes:\n",
    "        f (callable): The function to be minimized.\n",
    "        x_true (torch.Tensor): The true global minimizer.\n",
    "        delta (float): Coefficient of the viscous term in the HJ equation.\n",
    "        int_samples (int): Number of samples used to approximate expectation in the heat equation solution.\n",
    "        t_vec (list): Time vector containing [initial time, minimum time allowed, maximum time].\n",
    "        max_iters (int): Maximum number of iterations.\n",
    "        tol (float): Stopping tolerance.\n",
    "        alpha (float): Step size.\n",
    "        beta (float): Exponential averaging term for gradient beta.\n",
    "        eta_vec (list): Vector containing [eta_minus, eta_plus].\n",
    "        theta (float): Parameter used to update tk.\n",
    "        fixed_time (bool): Whether to use adaptive time.\n",
    "        verbose (bool): Whether to print progress.\n",
    "        rescale0 (float): Initial rescale factor.\n",
    "        momentum (float): Momentum term for acceleration.\n",
    "\n",
    "    Methods:\n",
    "        run(x0, num_cycles): Runs the coordinate descent optimization process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f, x_true, delta=0.1, int_samples=100, t_vec=[1.0, 1e-3, 1e1], max_iters=5e4,\n",
    "                 tol=5e-2, theta=0.9, beta=[0.9], eta_vec=[0.9, 1.1], alpha=1.0, fixed_time=False,\n",
    "                 verbose=True,plot=False, rescale0=1e-1, momentum=None, saturate_tol=1e-9):\n",
    "        self.tol = tol\n",
    "        self.plot = plot\n",
    "        super().__init__(f=f, x_true=x_true, delta=delta, int_samples=int_samples, t_vec=t_vec, max_iters=max_iters,\n",
    "                         tol=self.tol, alpha=alpha, beta=beta, eta_vec=eta_vec, theta=theta, fixed_time=fixed_time,\n",
    "                         verbose=verbose, rescale0=rescale0, momentum=momentum, saturate_tol=saturate_tol)\n",
    "\n",
    "    def plot_1d_descent(self, xk, dim, domain=(-10, 10), num_points=100):\n",
    "        \"\"\"\n",
    "        Plots the 1D descent for the current dimension.\n",
    "\n",
    "        Args:\n",
    "            xk (torch.Tensor): Current position.\n",
    "            dim (int): The current dimension being optimized.\n",
    "            domain (tuple): The range over which to vary the current dimension.\n",
    "            num_points (int): Number of points to sample in the domain.\n",
    "        \"\"\"\n",
    "        x_vals = np.linspace(domain[0], domain[1], num_points)\n",
    "        f_vals = []\n",
    "\n",
    "        for x in x_vals:\n",
    "            xk_varied = xk.clone()\n",
    "            xk_varied[dim] = x\n",
    "            f_vals.append(self.f(xk_varied.unsqueeze(0)).item())\n",
    "\n",
    "        std_dev = np.sqrt(self.delta * self.t_vec[0])\n",
    "        std_dev_minus = xk.clone()\n",
    "        std_dev_plus = xk.clone()\n",
    "        std_dev_minus[dim] -= std_dev\n",
    "        std_dev_plus[dim] += std_dev\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(x_vals, f_vals, '-', color='black', label=f'f(x) Dimension {dim + 1}')\n",
    "        plt.plot(xk[dim].item(), self.f(xk.unsqueeze(0)).item(), '*', color='red', label=f'xk Dimension {dim + 1}')\n",
    "        plt.plot(std_dev_minus[dim].item(), self.f(std_dev_minus.unsqueeze(0)).item(), 'x', color='purple', label='Std Dev Minus')\n",
    "        plt.plot(std_dev_plus[dim].item(), self.f(std_dev_plus.unsqueeze(0)).item(), '*', color='purple', label='Std Dev Plus')\n",
    "        plt.xlabel(f'Dimension {dim + 1}')\n",
    "        plt.ylabel('Function Value')\n",
    "        plt.title(f'1D Descent for Dimension {dim + 1}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def optimize_dimension(self, xk, dim):\n",
    "        \"\"\"\n",
    "        Optimize with respect to a single dimension.\n",
    "\n",
    "        Args:\n",
    "            xk (torch.Tensor): Current position.\n",
    "            dim (int): The dimension to optimize.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Updated position for the dimension.\n",
    "            list: History of positions for the dimension.\n",
    "            list: History of errors for the dimension.\n",
    "            list: History of function values for the dimension.\n",
    "        \"\"\"\n",
    "        xk, xk_hist, tk_hist, xk_error_hist, rel_grad_uk_norm_hist, fk_hist = super().run(xk, cd=True, update_dim=dim)\n",
    "        return xk, xk_hist, xk_error_hist, fk_hist\n",
    "\n",
    "    def run(self, x0, num_cycles=10):\n",
    "        \"\"\"\n",
    "        Runs the coordinate descent optimization process.\n",
    "\n",
    "        Args:\n",
    "            x0 (torch.Tensor): Initial guess for the minimizer.\n",
    "            num_cycles (int): Number of cycles to run the coordinate descent.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Optimal solution found by the coordinate descent.\n",
    "            list: History of solutions for each cycle.\n",
    "            list: History of the entire optimization process.\n",
    "            list: Error history for each cycle.\n",
    "        \"\"\"\n",
    "        xk = x0.clone()\n",
    "        n_features = x0.shape[0]\n",
    "        CD_xk_hist = torch.zeros(n_features * num_cycles + 1, n_features)\n",
    "        CD_xk_hist[0, :] = xk\n",
    "        full_xk_hist = []\n",
    "        full_fk_hist = []\n",
    "        full_xk_error_hist = []\n",
    "\n",
    "        for cycle in range(num_cycles):\n",
    "            dims = list(range(n_features))\n",
    "            dim_count = 0\n",
    "            print(\"Cycle\", cycle + 1)\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                futures = {executor.submit(self.optimize_dimension, xk.clone(), dim): dim for dim in dims}\n",
    "                results = {dim: future.result() for future, dim in futures.items()}\n",
    "\n",
    "            for dim, (xk_dim, xk_hist, xk_error_hist, fk_hist) in results.items():\n",
    "                if self.verbose:\n",
    "                    print(f\"Cycle {cycle + 1}/{num_cycles} and Dimension {dim + 1}/{n_features}\")\n",
    "\n",
    "                if self.plot:\n",
    "                    self.plot_1d_descent(xk, dim)\n",
    "\n",
    "                xk[dim] = xk_dim[dim]\n",
    "                full_xk_hist.extend(xk_hist.numpy())\n",
    "                full_xk_error_hist.extend(xk_error_hist.numpy())\n",
    "                full_fk_hist.extend(fk_hist.numpy())\n",
    "\n",
    "                if xk_error_hist[-1] < self.tol:\n",
    "                    if self.verbose:\n",
    "                        print(f'HJ-MAD-CD converged. Error: {xk_error_hist[-1]:.3f}, tolerance: {self.tol}.')\n",
    "                    CD_xk_hist[cycle + 1, :] = xk\n",
    "                    X_opt = xk\n",
    "                    return X_opt, CD_xk_hist, full_xk_hist, full_xk_error_hist, full_fk_hist\n",
    "\n",
    "                CD_xk_hist[cycle + dim_count + 1, :] = xk\n",
    "                dim_count += 1\n",
    "            print(f\"Error = {torch.norm(xk - self.x_true)}\")\n",
    "            \n",
    "            if xk_error_hist[-1] < self.tol:\n",
    "                if self.verbose:\n",
    "                    print(f'HJ-MAD-CD converged. Error: {xk_error_hist[-1]:.3f}, tolerance: {self.tol}.')\n",
    "                CD_xk_hist[cycle + 1, :] = xk\n",
    "                X_opt = xk\n",
    "                return X_opt, CD_xk_hist, full_xk_hist, full_xk_error_hist, full_fk_hist\n",
    "        X_opt = xk\n",
    "        return X_opt, CD_xk_hist, full_xk_hist, full_xk_error_hist, full_fk_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb--RgyjfPAq"
   },
   "source": [
    "### Set up hyperparameters for HJ-MAD for different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbHcFJ3e6Qfa"
   },
   "outputs": [],
   "source": [
    "# Default values\n",
    "delta         = 5e-3\n",
    "max_iters     = 1000 #int(1e5)\n",
    "tol           = 5e-2#7e-4\n",
    "momentum      = 0.64\n",
    "rescale0      = 0.5\n",
    "# Set the number of trials to run\n",
    "avg_trials = 1\n",
    "sat_tol = 1e-10\n",
    "\n",
    "# # def f(x):\n",
    "# #   return MultiMinimaFunc(x)\n",
    "# # def f_numpy(x):\n",
    "# #   return MultiMinimaFunc_numpy(x)\n",
    "# # ax_bry  = 30\n",
    "# # f_name  = 'MultiMinimaFunc'\n",
    "# # dim = 1; int_samples = int(100);\n",
    "# # x0      = -30*torch.ones(dim, dtype=torch.double)\n",
    "# # x_true  = -1.51034569*torch.ones(dim, dtype=torch.double)\n",
    "\n",
    "# # delta         = 0.1\n",
    "# # max_iters     = int(100)\n",
    "# # tol           = 1e-3\n",
    "# # momentum = 0.5\n",
    "\n",
    "# # theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# # beta          = 0.0\n",
    "# # t_min     = 1e-1\n",
    "# # t_max     = 300\n",
    "# # t_init    = 220\n",
    "# # alpha     = 0.1\n",
    "# # eta_min = 0.99\n",
    "# # eta_plus = 5.0\n",
    "# # eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "\n",
    "# # # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def f(x):\n",
    "#   return Griewank(x)\n",
    "# def f_numpy(x):\n",
    "#   return Griewank_numpy(x)\n",
    "# ax_bry  = 20\n",
    "# f_name  = 'Griewank'\n",
    "# dim = 2; int_samples = int(10000);\n",
    "# x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "# rescale0      = 1\n",
    "# delta         = 1e-6\n",
    "# max_iters     = int(1e4)\n",
    "# tol           = 1e-4\n",
    "# momentum = 0.0\n",
    "\n",
    "# theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# beta          = 0.9\n",
    "# t_min     = 1e-2/delta\n",
    "# t_max     = int(2)/delta\n",
    "# t_init    = 1e-2/delta\n",
    "# alpha     = 5e-2\n",
    "# eta_min = 0.99\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# # # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # def f(x):\n",
    "# #   return Griewank(x)\n",
    "# # def f_numpy(x):\n",
    "# #   return Griewank_numpy(x)\n",
    "# # ax_bry  = 20\n",
    "# # f_name  = 'Griewank'\n",
    "# # dim = 20; int_samples = int(1000);#int(1000000);\n",
    "# # x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# # x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "# # rescale0      = 2**(-15)\n",
    "# # delta         = 1e-6\n",
    "# # max_iters     = int(1e5)\n",
    "# # tol           = 5e-2\n",
    "# # momentum = 0.64\n",
    "\n",
    "# # theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# # beta          = 0.9\n",
    "# # t_min     = 1e-1/delta\n",
    "# # t_max     = int(2e1)/delta\n",
    "# # t_init    = 1e-1/delta\n",
    "# # alpha     = 5e-2\n",
    "# # eta_min = 0.99\n",
    "# # eta_plus = 5.0\n",
    "# # eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------\n",
    "# # def f(x):\n",
    "# #   return Griewank(x)\n",
    "# # def f_numpy(x):\n",
    "# #   return Griewank_numpy(x)\n",
    "# # f_name  = 'Griewank'\n",
    "# # dim = 200; int_samples = int(100); # this one has higher dimension\n",
    "# # x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# # x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "# # rescale0      = 2**(-5)\n",
    "# # sat_tol = 1e-9 #7e-7 or 7e-10 (not sure) for 100 dims, 7e-8 for less than 100 dims\n",
    "# # theta     = 1.0 # note: larger theta => easier to increase time\n",
    "# # beta      = 0.9\n",
    "# # # momentum  = 0.5\n",
    "# # # beta      = 0.0\n",
    "# # momentum  = 0.0\n",
    "# # t_min     = 2e1\n",
    "# # t_max     = 1e5\n",
    "# # t_init    = 2e1\n",
    "# # alpha     = 1.2\n",
    "# # eta_min = 0.5\n",
    "# # eta_plus = 5.0\n",
    "# # eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "def f(x):\n",
    "  return Griewank(x)\n",
    "def f_numpy(x):\n",
    "  return Griewank_numpy(x)\n",
    "f_name  = 'Griewank'\n",
    "dim = 500; int_samples = int(100); \n",
    "x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "rescale0      = 2**(-6)#256\n",
    "sat_tol = 1e-9 #7e-7 or 7e-10 (not sure) for 100 dims, 7e-8 for less than 100 dims\n",
    "theta     = 1.0 # note: larger theta => easier to increase time\n",
    "beta      = 0.9\n",
    "momentum  = 0.0\n",
    "t_min     = 2e1\n",
    "t_max     = 1e5\n",
    "t_init    = 2e1\n",
    "alpha     = 1.2\n",
    "eta_min = 0.5\n",
    "eta_plus = 5.0\n",
    "eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def f(x):\n",
    "#   return Drop_Wave(x)\n",
    "# def f_numpy(x):\n",
    "#   return Drop_Wave_numpy(x)\n",
    "# ax_bry  = 20\n",
    "# max_iters     = 1000\n",
    "# f_name  = 'Drop_Wave'\n",
    "# rescale0      = 1\n",
    "# dim = 2; int_samples = int(10000)\n",
    "# x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "\n",
    "# momentum      = 0.5\n",
    "# delta         = 1e-4\n",
    "# theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# beta          = 0.8\n",
    "# t_min     = 1e-6\n",
    "# t_max     = int(2e1)/delta\n",
    "# t_init    = 1e3\n",
    "# alpha     = 0.5\n",
    "# eta_min = 0.5\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# # # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def f(x):\n",
    "#   return AlpineN1(x)\n",
    "# def f_numpy(x):\n",
    "#   return AlpineN1_numpy(x)\n",
    "# ax_bry  = 20\n",
    "# f_name  = 'AlpineN1'\n",
    "\n",
    "# dim = 2; int_samples = int(100000)# int(10000);\n",
    "# x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "\n",
    "# momentum      = 0.45\n",
    "\n",
    "# theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# beta          = 0.0\n",
    "# # t_max     = int(2e1)/delta\n",
    "# # t_init    = 1e-3\n",
    "# # t_min     = t_init\n",
    "# t_max     = int(2e3)/delta\n",
    "# t_init    = 1e-3\n",
    "# t_min     = 1e-4\n",
    "# alpha     = 0.25\n",
    "# eta_min = 0.6\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def f(x):\n",
    "#   return Levy(x)\n",
    "# def f_numpy(x):\n",
    "#   return Levy_numpy(x)\n",
    "# ax_bry  = 20\n",
    "# f_name  = 'Levy'\n",
    "\n",
    "# # Set the number of trials to run\n",
    "# rescale0 = 2**(-7)\n",
    "# tol           = 5e-2\n",
    "# sat_tol = 1e-12\n",
    "# max_iters     = 1000\n",
    "\n",
    "# dim = 2; int_samples = int(500000)\n",
    "# x0      = -15*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.ones(dim, dtype=torch.double)\n",
    "\n",
    "# theta         = 0.9 # note: larger theta => easier to increase time\n",
    "# beta          = 0.5\n",
    "\n",
    "# t_max     = int(2e5)/delta\n",
    "# t_init    = 1e6\n",
    "# t_min     = 1e2\n",
    "# alpha     = 0.25\n",
    "# eta_min = 0.6\n",
    "# eta_plus = 1.5\n",
    "# eta_vec = [eta_min, eta_plus]\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def f(x):\n",
    "#   return Rastrigin(x)\n",
    "# def f_numpy(x):\n",
    "#   return Rastrigin_numpy(x)\n",
    "# ax_bry  = 20\n",
    "# f_name  = 'Rastrigin'\n",
    "# delta=5e-3\n",
    "# dim = 2; int_samples = int(10000);\n",
    "# x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "# momentum      = 0.25\n",
    "# theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# beta          = 0.5\n",
    "# t_max     = int(2e1)/delta\n",
    "# t_init    = 5.0\n",
    "# t_min     = t_init\n",
    "# alpha     = 0.5\n",
    "# eta_min = 0.5\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]\n",
    "# tol=2e-10\n",
    "\n",
    "# # # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def f(x):\n",
    "#   return Ackley(x)\n",
    "# def f_numpy(x):\n",
    "#   return Ackley_numpy(x)\n",
    "# ax_bry  = 20\n",
    "# f_name  = 'Ackley'\n",
    "\n",
    "# dim = 2; int_samples = int(100000);\n",
    "# x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "# x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "# momentum      = 0.25\n",
    "# theta         = 1.0 # note: larger theta => easier to increase time\n",
    "# beta          = 0.9\n",
    "# t_max     = int(2e1)/delta\n",
    "# t_init    = 1e-3\n",
    "# t_min     = t_init\n",
    "# alpha     = 5e-1\n",
    "# eta_min = 0.5\n",
    "# eta_plus = 5.0\n",
    "# eta_vec = [eta_min, eta_plus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-AcVUFHfahc"
   },
   "source": [
    "Run HJ-MAD and average its results over avg_trials trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Under this transformation the standard deviation of the Gaussian is 1, hence we have more control over t and delta\n",
    "\n",
    "# Create an instance of HJ_MAD_CoordinateDescent\n",
    "if f_name == 'Ackley':\n",
    "    hj_mad_cd_GHQ = HJ_MAD_CoordinateDescent(f, x_true, delta=delta*1e-10,\n",
    "                    int_samples=int(1000), t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=0.0,saturate_tol=sat_tol,integration_method='GHQ')\n",
    "elif f_name == 'Griewank' and dim == 500:\n",
    "    tol           = 5e-2\n",
    "    hj_mad_cd_GHQ = HJ_MAD_CoordinateDescent(f, x_true, delta= 1e-8,\n",
    "                    int_samples=int(20), t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=0.0,saturate_tol=1e-10,integration_method='GHQ')\n",
    "\n",
    "    hj_mad_cd_MC = HJ_MAD_CoordinateDescent(f, x_true, delta=delta,\n",
    "                    int_samples=int(1000), t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=momentum,saturate_tol=sat_tol,integration_method=\"MC\")\n",
    "\n",
    "    hj_mad_cd_NMC = HJ_MAD_CoordinateDescent(f, x_true, delta=1e-6,\n",
    "                    int_samples=int(1000), t_vec=[t_init*1e4, t_min, t_max*1e5], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=momentum,saturate_tol=sat_tol,integration_method=\"NMC\")\n",
    "    \n",
    "\n",
    "elif f_name == 'Rastrigin':\n",
    "    tol = 1e-4\n",
    "    hj_mad_cd_GHQ = HJ_MAD_CoordinateDescent(f, x_true, delta=delta,\n",
    "                    int_samples=int(10000), t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=0.0,saturate_tol=sat_tol,integration_method='GHQ')\n",
    "\n",
    "    hj_mad_cd_MC = HJ_MAD_CoordinateDescent(f, x_true, delta=delta,\n",
    "                    int_samples=int(5000), t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=0.0,saturate_tol=sat_tol*1e5,integration_method=\"MC\")\n",
    "\n",
    "    hj_mad_cd_NMC = HJ_MAD_CoordinateDescent(f, x_true, delta=5e-5,\n",
    "                    int_samples=int(5000), t_vec=[20, 1e-5, 1e5], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=0.1,saturate_tol=sat_tol*1e5,integration_method=\"NMC\")\n",
    "elif f_name == 'Levy': # All have been tuned\n",
    "    hj_mad_cd_GHQ = HJ_MAD_CoordinateDescent(f, x_true, delta=1e-13,\n",
    "                    int_samples=int(80), t_vec=[0.5, 0.5, 1], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=0.6,saturate_tol=sat_tol,integration_method='GHQ')\n",
    "    \n",
    "    hj_mad_cd_MC = HJ_MAD_CoordinateDescent(f, x_true, delta=delta,\n",
    "                    int_samples=int(100000), t_vec=[t_init, t_min, t_max], max_iters=10, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=momentum,saturate_tol=sat_tol,integration_method=\"MC\")\n",
    "\n",
    "    hj_mad_cd_NMC = HJ_MAD_CoordinateDescent(f, x_true, delta=1e-2,\n",
    "                        int_samples=int(10), t_vec=[10, 1e-2, 1000], max_iters=10, tol=tol, alpha=alpha,\n",
    "                        beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                        momentum=0.5,saturate_tol=sat_tol,integration_method=\"NMC\")\n",
    "elif f_name == 'AlpineN1': # None are tuned\n",
    "    hj_mad_cd_GHQ = HJ_MAD_CoordinateDescent(f, x_true, delta=5e-15,\n",
    "                    int_samples=int(1000), t_vec=[50, 10, 60], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=0.0,saturate_tol=sat_tol,integration_method='GHQ')\n",
    "    \n",
    "    hj_mad_cd_MC = HJ_MAD_CoordinateDescent(f, x_true, delta=delta,\n",
    "                    int_samples=int_samples, t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=0.0,saturate_tol=sat_tol,integration_method=\"MC\")\n",
    "\n",
    "    hj_mad_cd_NMC = HJ_MAD_CoordinateDescent(f, x_true, delta=delta,\n",
    "                    int_samples=int(50), t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=0.0,saturate_tol=sat_tol,integration_method=\"NMC\")\n",
    "    \n",
    "elif f_name == 'Drop_Wave':\n",
    "    tol = 5e-2\n",
    "    hj_mad_cd_GHQ = HJ_MAD_CoordinateDescent(f, x_true, delta=1e-21,\n",
    "                    int_samples=int(1000), t_vec=[1e6, 1e4, 1e6], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=0.4,saturate_tol=sat_tol,integration_method='GHQ')\n",
    "else:\n",
    "    hj_mad_cd_GHQ = HJ_MAD_CoordinateDescent(f, x_true, delta=delta,\n",
    "                    int_samples=int_samples, t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=False, verbose=True,rescale0=rescale0,\n",
    "                    momentum=0.0,saturate_tol=sat_tol,integration_method='GHQ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize accumulators for averages\n",
    "# avg_func_evals = 0\n",
    "# sum_elapsed_time = 0\n",
    "# total_iterations = 0  # To store total iterations across trials\n",
    "\n",
    "# # Run the specified number of trials\n",
    "# for _ in range(avg_trials):\n",
    "#     start_time = time.time()  # Record the start time\n",
    "\n",
    "#     # Execute the HJ_MAD_CD algorithm and retrieve results\n",
    "#     x_opt_cd_GHQ, coordinate_wise_xk_hist_GHQ, xk_hist_cd_GHQ, xk_error_hist_cd_GHQ, fk_hist_cd_GHQ = hj_mad_cd_GHQ.run(x0, num_cycles=20)\n",
    "\n",
    "#     elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "#     sum_elapsed_time += elapsed_time  # Accumulate elapsed time\n",
    "\n",
    "#     total_iterations += len(xk_error_hist_cd_GHQ)  # Add iterations used in this trial\n",
    "#     avg_func_evals += len(xk_error_hist_cd_GHQ) * int_samples  # Update average function evaluations\n",
    "\n",
    "#     print(f\"Elapsed time: {elapsed_time:.4f} seconds\")  # Print elapsed time for the current trial\n",
    "\n",
    "\n",
    "# # Compute averages after all trials\n",
    "# avg_func_evals /= avg_trials  # Average function evaluations per trial\n",
    "# average_iterations = total_iterations / avg_trials  # Average number of iterations per trial\n",
    "\n",
    "# # Output results\n",
    "# # print('\\n\\n avg_func_evals = ', avg_func_evals)\n",
    "# print(f\"Average iterations before convergence/stopping: {average_iterations:.2f}\")\n",
    "# print(f\"Average elapsed time: {sum_elapsed_time / avg_trials:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Initialize accumulators for averages\n",
    "# avg_func_evals = 0\n",
    "# sum_elapsed_time = 0\n",
    "# total_iterations = 0  # To store total iterations across trials\n",
    "\n",
    "# # Run the specified number of trials\n",
    "# for _ in range(avg_trials):\n",
    "#     start_time = time.time()  # Record the start time\n",
    "\n",
    "#     # Execute the HJ_MAD_CD algorithm and retrieve results\n",
    "#     x_opt_cd_MC, coordinate_wise_xk_hist_MC, xk_hist_cd_MC, xk_error_hist_cd_MC, fk_hist_cd_MC = hj_mad_cd_MC.run(x0, num_cycles=20)\n",
    "\n",
    "#     elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "#     sum_elapsed_time += elapsed_time  # Accumulate elapsed time\n",
    "\n",
    "#     total_iterations += len(xk_error_hist_cd_MC)  # Add iterations used in this trial\n",
    "#     avg_func_evals += len(xk_error_hist_cd_MC) * int_samples  # Update average function evaluations\n",
    "\n",
    "#     print(f\"Elapsed time: {elapsed_time:.4f} seconds\")  # Print elapsed time for the current trial\n",
    "\n",
    "\n",
    "# # Compute averages after all trials\n",
    "# avg_func_evals /= avg_trials  # Average function evaluations per trial\n",
    "# average_iterations = total_iterations / avg_trials  # Average number of iterations per trial\n",
    "\n",
    "# # Output results\n",
    "# # print('\\n\\n avg_func_evals = ', avg_func_evals)\n",
    "# print(f\"Average iterations before convergence/stopping: {average_iterations:.2f}\")\n",
    "# print(f\"Average elapsed time: {sum_elapsed_time / avg_trials:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Initialize accumulators for averages\n",
    "# avg_func_evals = 0\n",
    "# sum_elapsed_time = 0\n",
    "# total_iterations = 0  # To store total iterations across trials\n",
    "\n",
    "# # Run the specified number of trials\n",
    "# for _ in range(avg_trials):\n",
    "#     start_time = time.time()  # Record the start time\n",
    "\n",
    "#     # Execute the HJ_MAD_CD algorithm and retrieve results\n",
    "#     x_opt_cd_NMC, coordinate_wise_xk_hist_NMC, xk_hist_cd_NMC, xk_error_hist_cd_NMC, fk_hist_cd_NMC = hj_mad_cd_NMC.run(x0, num_cycles=20)\n",
    "\n",
    "#     elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "#     sum_elapsed_time += elapsed_time  # Accumulate elapsed time\n",
    "\n",
    "#     total_iterations += len(xk_error_hist_cd_NMC)  # Add iterations used in this trial\n",
    "#     avg_func_evals += len(xk_error_hist_cd_NMC) * int_samples  # Update average function evaluations\n",
    "\n",
    "#     print(f\"Elapsed time: {elapsed_time:.4f} seconds\")  # Print elapsed time for the current trial\n",
    "\n",
    "\n",
    "# # Compute averages after all trials\n",
    "# avg_func_evals /= avg_trials  # Average function evaluations per trial\n",
    "# average_iterations = total_iterations / avg_trials  # Average number of iterations per trial\n",
    "\n",
    "# # Output results\n",
    "# # print('\\n\\n avg_func_evals = ', avg_func_evals)\n",
    "# print(f\"Average iterations before convergence/stopping: {average_iterations:.2f}\")\n",
    "# print(f\"Average elapsed time: {sum_elapsed_time / avg_trials:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an instance of HJ_MAD_CoordinateDescent\n",
    "# hj_mad_cd = HJ_MAD_CoordinateDescent_parallel(f, x_true, delta=delta,\n",
    "#                     int_samples=int_samples, t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol, alpha=alpha,\n",
    "#                     beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False,plot=True, verbose=False,rescale0=rescale0,\n",
    "#                     momentum=momentum,saturate_tol=sat_tol,integration_method='MC')\n",
    "\n",
    "# # Initialize accumulators for averages\n",
    "# avg_func_evals = 0\n",
    "# sum_elapsed_time = 0\n",
    "# total_iterations = 0  # To store total iterations across trials\n",
    "\n",
    "# # Run the specified number of trials\n",
    "# for _ in range(avg_trials):\n",
    "#     start_time = time.time()  # Record the start time\n",
    "\n",
    "#     # Execute the HJ_MAD_CD algorithm and retrieve results\n",
    "#     x_opt_cd_para, coordinate_wise_xk_hist_para, xk_hist_cd_para, xk_error_hist_cd_para, fk_hist_cd_para = hj_mad_cd.run(x0, num_cycles=20)\n",
    "\n",
    "#     elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "#     sum_elapsed_time += elapsed_time  # Accumulate elapsed time\n",
    "\n",
    "#     total_iterations += len(xk_error_hist_cd_para)  # Add iterations used in this trial\n",
    "#     avg_func_evals += len(xk_error_hist_cd_para) * int_samples  # Update average function evaluations\n",
    "\n",
    "#     print(f\"Elapsed time: {elapsed_time:.4f} seconds\")  # Print elapsed time for the current trial\n",
    "\n",
    "\n",
    "# # Compute averages after all trials\n",
    "# avg_func_evals /= avg_trials  # Average function evaluations per trial\n",
    "# average_iterations = total_iterations / avg_trials  # Average number of iterations per trial\n",
    "\n",
    "# # Output results\n",
    "# print('\\n\\n avg_func_evals = ', avg_func_evals)\n",
    "# print(f\"Average iterations before convergence/stopping: {average_iterations:.2f}\")\n",
    "# print(f\"Average elapsed time: {sum_elapsed_time / avg_trials:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73eSi37bA9ak",
    "outputId": "8649873a-41e6-4860-b70e-762d57c05cdb"
   },
   "outputs": [],
   "source": [
    "int_samples = int(100000);\n",
    "rescale0=1\n",
    "# delta         = 5e-7\n",
    "# t_min     = 2e1\n",
    "# t_max     = 1e6\n",
    "# t_init    = 2e3#2e1\n",
    "delta         = 5e-7\n",
    "t_min     = 2e1\n",
    "t_max     = 1e5\n",
    "t_init    = 2e3#2e1\n",
    "HJ_MAD_alg = HJ_MAD(f, x_true, delta=delta,\n",
    "                    int_samples=int_samples, t_vec=[t_init, t_min, t_max], max_iters=max_iters, tol=tol*1e-1, alpha=alpha,\n",
    "                    beta=beta, eta_vec = eta_vec, theta=theta, fixed_time=False, verbose=True,rescale0=rescale0,momentum=0.5,\n",
    "                    integration_method='NMC')\n",
    "# Initialize accumulators for averages\n",
    "avg_func_evals = 0\n",
    "sum_elapsed_time = 0\n",
    "total_iterations = 0  # To store total iterations across trials\n",
    "\n",
    "# Run the specified number of trials\n",
    "for _ in range(avg_trials):\n",
    "    #x0 = 10*torch.ones(dim, dtype=torch.double)\n",
    "    start_time = time.time()  # Record the start time\n",
    "\n",
    "    # Execute the HJ_MAD algorithm and retrieve results\n",
    "    x_opt_MAD, xk_hist_MAD, tk_hist_MAD, xk_error_hist_MAD, rel_grad_uk_norm_hist_MAD, fk_hist_MAD = HJ_MAD_alg.run(x0)\n",
    "\n",
    "    elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "    sum_elapsed_time += elapsed_time  # Accumulate elapsed time\n",
    "    \n",
    "    total_iterations += len(xk_error_hist_MAD)  # Add iterations used in this trial\n",
    "    avg_func_evals += len(xk_error_hist_MAD) * int_samples  # Update average function evaluations\n",
    "\n",
    "    print(f\"Elapsed time: {elapsed_time:.4f} seconds\")  # Print elapsed time for the current trial\n",
    "\n",
    "# Compute averages after all trials\n",
    "avg_func_evals /= avg_trials  # Average function evaluations per trial\n",
    "average_iterations = total_iterations / avg_trials  # Average number of iterations per trial\n",
    "\n",
    "# Output results\n",
    "print('\\n\\n avg_func_evals = ', avg_func_evals)\n",
    "print(f\"Average iterations before convergence: {average_iterations:.2f}\")\n",
    "print(f\"Average elapsed time: {sum_elapsed_time / avg_trials:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBT0oupCFD_u"
   },
   "source": [
    "### Generate Convergence Histories and Optimization Path Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "gHiwldOKLkKT",
    "outputId": "e50a8767-a37d-4e8a-965d-9cf9349bf15e"
   },
   "outputs": [],
   "source": [
    "\n",
    "title_fontsize = 22\n",
    "fontsize       = 18\n",
    "fig1 = plt.figure()\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.semilogy(xk_error_hist_MAD, color='purple', linewidth=3,label='HJ-MAD(NMC)');\n",
    "#ax.semilogy(xk_error_hist_cd_NMC, color='red', linewidth=3,label='HJ-MAD-NMC');\n",
    "#ax.semilogy(xk_error_hist_cd_GHQ, color='blue', linewidth=3,label='HJ-MAD-CD-GHQ');\n",
    "#ax.semilogy(xk_error_hist_cd_MC, color='green', linewidth=3,label='HJ-MAD-CD-MC');\n",
    "# ax.semilogy(xk_error_hist_EGD[0:len(xk_error_hist_GD)], 'm-', linewidth=3)\n",
    "#ax.semilogy(xk_error_hist_GD[0:len(xk_error_hist_GD)], 'g-', linewidth=3)\n",
    "ax.set_title(f'Dims={dim},Func={f_name},\\n Adaptive Rescale Factor', fontsize=title_fontsize)\n",
    "ax.set_xlabel(\"Iterations\", fontsize=title_fontsize)\n",
    "ax.set_ylabel(\"Errors\", fontsize=title_fontsize)\n",
    "ax.legend(fontsize=fontsize)\n",
    "# title_str = 'Relative Errors'\n",
    "# ax.set_title(title_str, fontsize=title_fontsize)\n",
    "ax.tick_params(labelsize=fontsize, which='both', direction='in')\n",
    "\n",
    "# save_str = 'griewank_error_hist.png'\n",
    "# fig1.savefig(save_str, dpi=300 , bbox_inches=\"tight\", pad_inches=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "3cYDR5eYFNFm",
    "outputId": "2d3f6069-c74a-4ae7-f9a4-77c1bc19578e"
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "ax = plt.axes()\n",
    "ax.semilogy(fk_hist_MAD, color='red', linewidth=3,label='HJ-MAD');\n",
    "ax.semilogy(fk_hist_cd_NMC, color='red', linewidth=3,label='HJ-MAD-NMC');\n",
    "ax.semilogy(fk_hist_cd_GHQ, color='blue', linewidth=3,label='HJ-MAD-CD-GHQ');\n",
    "ax.semilogy(fk_hist_cd_MC, color='green', linewidth=3,label='HJ-MAD-CD-MC');\n",
    "\n",
    "ax.set_xlabel(\"Iterations\", fontsize=title_fontsize)\n",
    "ax.set_ylabel(\"fk\", fontsize=title_fontsize)\n",
    "ax.legend(fontsize=fontsize)\n",
    "title_str = 'Objective Function Values'\n",
    "ax.set_title(title_str, fontsize=title_fontsize)\n",
    "ax.tick_params(labelsize=fontsize, which='both', direction='in')\n",
    "\n",
    "# save_str = 'griewank_func_hist.png'\n",
    "# fig1.savefig(save_str, dpi=300 , bbox_inches=\"tight\", pad_inches=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnWYfPeIR15H"
   },
   "source": [
    "## 2D Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "OGEzn9rLTAbV",
    "outputId": "6293e921-9c2b-42fb-cf53-d3f728623162"
   },
   "outputs": [],
   "source": [
    "if dim == 2:\n",
    "\n",
    "  if f_name == 'Levy':\n",
    "    x0      = -15*torch.ones(dim, dtype=torch.double)\n",
    "    x_true  = torch.ones(dim, dtype=torch.double)\n",
    "  else:\n",
    "    x0      = 10*torch.ones(dim, dtype=torch.double)\n",
    "    x_true  = torch.zeros(dim, dtype=torch.double)\n",
    "\n",
    "  surface_plot_resolution = 50\n",
    "  x = np.linspace(-ax_bry, ax_bry, surface_plot_resolution)\n",
    "  y = np.linspace(-ax_bry, ax_bry, surface_plot_resolution)\n",
    "\n",
    "  X, Y = np.meshgrid(x, y)\n",
    "  n_features = 2\n",
    "\n",
    "  t_final = t_max\n",
    "\n",
    "  Z                 = np.zeros(X.shape)\n",
    "  Z_MAD             = np.zeros(X.shape)\n",
    "\n",
    "  for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "      Z[i,j] = f(torch.FloatTensor([X[i,j],Y[i,j]]).view(1,n_features))  \n",
    "     \n",
    "\n",
    "  fig, ax = plt.subplots(1, 1)\n",
    "  im = ax.contourf(X, Y, Z, 20, cmap=plt.get_cmap('gray'))\n",
    "  plt.style.use('default')\n",
    "\n",
    "  title_fontsize = 22\n",
    "  fontsize       = 15\n",
    "\n",
    "  ax.plot(np.vstack(xk_hist_cd_MC)[:,0], np.vstack(xk_hist_cd_MC)[:,1], '-o', color='blue',label='HJ-MAD-CD-MC')\n",
    "  ax.plot(np.vstack(xk_hist_cd_GHQ)[:,0], np.vstack(xk_hist_cd_GHQ)[:,1], 'm-o', label='HJ-MAD-CD-GHQ')\n",
    "  ax.plot(np.vstack(xk_hist_cd_NMC)[:,0], np.vstack(xk_hist_cd_NMC)[:,1], 'm-o',label='HJ-MAD-CD-NMC')\n",
    "\n",
    "\n",
    "  ax.plot(x_true[0], x_true[1], 'rx', markeredgewidth=3, markersize=12,label='global min')\n",
    "  ax.plot(x0[0], x0[1], 'kx', markeredgewidth=3, markersize=12,label='initial guess')\n",
    "\n",
    "  ax.legend(fontsize=12, facecolor='white', markerfirst=False, loc='lower right')\n",
    "\n",
    "  ax.set_xlim(-ax_bry,ax_bry)\n",
    "  cb = plt.colorbar(im)\n",
    "\n",
    "  # save_loc = 'optimization_paths.png'\n",
    "  # plt.savefig(save_loc,bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive 2D Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dim == 2:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    ax_bry_3D_plot = 20\n",
    "    surface_plot_resolution = 50\n",
    "    x = np.linspace(-ax_bry_3D_plot, ax_bry_3D_plot, surface_plot_resolution)\n",
    "    y = np.linspace(-ax_bry_3D_plot, ax_bry_3D_plot, surface_plot_resolution)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    # Convert PyTorch tensors to NumPy\n",
    "    # xk_hist_MAD_np = xk_hist_MAD.numpy()\n",
    "    # coordinate_wise_xk_hist_np = np.vstack(xk_hist_cd)\n",
    "\n",
    "    # # Ensure z_values are scalars\n",
    "    # HJ_MAD_f_values = np.array([\n",
    "    #     f(torch.FloatTensor([[xk_hist_MAD_np[i, 0], xk_hist_MAD_np[i, 1]]])).item()\n",
    "    #     for i in range(len(xk_hist_MAD_np))\n",
    "    # ])\n",
    "\n",
    "    HJ_MAD_CD_f_values = np.array([\n",
    "        f(torch.FloatTensor([[np.vstack(xk_hist_cd_MC)[i, 0], np.vstack(xk_hist_cd_MC)[i, 1]]])).item()\n",
    "        for i in range(len(np.vstack(xk_hist_cd_MC)))\n",
    "    ])\n",
    "\n",
    "    # Global minimum and initial guess\n",
    "    if x_true.dim() == 1:\n",
    "        x_true = x_true.unsqueeze(0)\n",
    "    global_min_f = f(x_true).item()\n",
    "\n",
    "    # Initial guess point\n",
    "    if x0.dim() == 1:\n",
    "        x0 = x0.unsqueeze(0)\n",
    "    f_initial = f(x0).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dim == 2:\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # Create surface trace\n",
    "    surface_trace = go.Surface(\n",
    "        z=Z, x=X, y=Y, colorscale='Viridis', showscale=True, name='Surface'\n",
    "    )\n",
    "\n",
    "    # Create optimization paths\n",
    "    # HJ_MAD_trace = go.Scatter3d(\n",
    "    #     x=xk_hist_MAD_np[:, 0],\n",
    "    #     y=xk_hist_MAD_np[:, 1],\n",
    "    #     z=HJ_MAD_f_values,\n",
    "    #     mode='lines+markers',\n",
    "    #     marker=dict(size=5, color='red'),\n",
    "    #     line=dict(color='red', width=3),\n",
    "    #     name='HJ-MAD'\n",
    "    # )\n",
    "\n",
    "    HJ_MAD_CD_trace = go.Scatter3d(\n",
    "        x=np.vstack(xk_hist_cd_MC)[:, 0],\n",
    "        y=np.vstack(xk_hist_cd_MC)[:, 1],\n",
    "        z=HJ_MAD_CD_f_values,\n",
    "        mode='lines+markers',\n",
    "        marker=dict(size=5, color='blue'),\n",
    "        line=dict(color='blue', width=3),\n",
    "        name='HJ-MAD-CD'\n",
    "    )\n",
    "\n",
    "    # Global minimum point\n",
    "    global_min_trace = go.Scatter3d(\n",
    "        x=[x_true[0, 0].item()],\n",
    "        y=[x_true[0, 1].item()],\n",
    "        z=[global_min_f],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8, color='black', symbol='x'),\n",
    "        name='Global min'\n",
    "    )\n",
    "\n",
    "    # Initial guess point\n",
    "    initial_guess_trace = go.Scatter3d(\n",
    "        x=[x0[0, 0].item()],\n",
    "        y=[x0[0, 1].item()],\n",
    "        z=[f_initial],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8, color='green', symbol='x'),\n",
    "        name='Initial guess'\n",
    "    )\n",
    "\n",
    "    # Combine traces\n",
    "    fig = go.Figure(data=[surface_trace, HJ_MAD_CD_trace, global_min_trace, initial_guess_trace])\n",
    "\n",
    "    # Set layout details\n",
    "    fig.update_layout(\n",
    "        title=\"Interactive 3D Optimization Path\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"X-axis\",\n",
    "            yaxis_title=\"Y-axis\",\n",
    "            zaxis_title=\"f-axis\",\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, t=40, b=0),\n",
    "        legend=dict(\n",
    "            x=0.02,  # Adjust the x position of the legend\n",
    "            y=0.98,  # Adjust the y position of the legend\n",
    "            bgcolor='rgba(255, 255, 255, 0.5)',  # Set background color with transparency\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Show interactive plot\n",
    "    fig.show(renderer=\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dim == 2:\n",
    "\n",
    "    # Create the 3D plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot the surface\n",
    "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none', zorder=1)\n",
    "\n",
    "    # Plot the HJ-MAD optimization path\n",
    "    # ax.plot(xk_hist_MAD_np[:, 0], xk_hist_MAD_np[:, 1], HJ_MAD_f_values, '-o', color='red', label=\"HJ-MAD\", zorder=2)\n",
    "\n",
    "    # Plot the HJ-MAD-CD optimization path\n",
    "    ax.plot(np.vstack(xk_hist_cd_MC)[:, 0], np.vstack(xk_hist_cd_MC)[:, 1], HJ_MAD_CD_f_values, '-o', color='blue', label=\"HJ-MAD-CD\", zorder=2)\n",
    "\n",
    "    ax.plot(\n",
    "        [x_true[0, 0].item()],  # Wrap in list\n",
    "        [x_true[0, 1].item()],  # Wrap in list\n",
    "        [global_min_f],  # Wrap in list\n",
    "        'x', color='black', label=\"Global min\", zorder=3\n",
    "    )\n",
    "    ax.plot(\n",
    "        [x0[0, 0].item()],  # Wrap in list\n",
    "        [x0[0, 1].item()],  # Wrap in list\n",
    "        [f_initial],  # Wrap in list\n",
    "        'x', color='green', label=\"Initial guess\", zorder=3\n",
    "    )\n",
    "\n",
    "    # Set view angle\n",
    "    ax.view_init(elev=50, azim=30)  # Increase the elevation angle to 90 degrees\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel('X-axis')\n",
    "    ax.set_ylabel('Y-axis')\n",
    "    ax.set_zlabel('f-axis')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
