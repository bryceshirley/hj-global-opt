{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from hj_mad_ls import HJ_MD_LS\n",
    "\n",
    "# Hyperparameters\n",
    "n_neurons = 50  # Number of neurons in the shallow network\n",
    "num_epochs = 10\n",
    "batch_size = 4\n",
    "\n",
    "# Generate data\n",
    "no_of_samples = 500\n",
    "noise_level = 1e-3\n",
    "x = np.linspace(0, 2 * np.pi, no_of_samples)\n",
    "y = np.sin(x) + noise_level * np.random.randn(*x.shape)  # Add noise to the samples\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the shallow neural network\n",
    "class ShallowNet(nn.Module):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(ShallowNet, self).__init__()\n",
    "        self.linear = nn.Linear(1, n_neurons)\n",
    "        self.coeffs = nn.Parameter(torch.randn(n_neurons, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)      # Compute w_j * x + b_j\n",
    "        x = self.relu(x)        # Apply ReLU activation\n",
    "        x = x @ self.coeffs     # Compute sum of c_j * ReLU(...)\n",
    "        return x\n",
    "\n",
    "    def set_parameters(self, new_parameters):\n",
    "        \"\"\"\n",
    "        Set model parameters from the flattened parameter vector.\n",
    "        This method updates the model's parameters from a single flattened tensor.\n",
    "\n",
    "        Args:\n",
    "            new_parameters: A flattened tensor containing all parameters.\n",
    "        \"\"\"\n",
    "        offset = 0\n",
    "        for param in self.parameters():\n",
    "            param_size = param.numel()  # Total number of elements in the parameter\n",
    "            # Extract the relevant slice and reshape it to the parameter's original shape\n",
    "            if len(new_parameters.shape) == 1:\n",
    "                new_param_values = new_parameters[offset:offset + param_size]\n",
    "            else:\n",
    "                new_param_values = new_parameters[0, offset:offset + param_size]  # Access batch dimension\n",
    "            new_param_values = new_param_values.view_as(param)  # Match the original shape\n",
    "            param.data.copy_(new_param_values)  # Update parameter\n",
    "            offset += param_size\n",
    "\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the function f (e.g., a loss function based on the model's predictions)\n",
    "def loss_function(x, model, inputs, targets):\n",
    "    \"\"\"\n",
    "    Computes the loss based on the given model parameters.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape (n_samples, n_features) representing flattened parameters.\n",
    "        model: The neural network model.\n",
    "        inputs: Input batch.\n",
    "        targets: Ground truth values.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (n_samples,) containing the loss for each sample.\n",
    "    \"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    \n",
    "    # Initialize list to store losses\n",
    "    sample_losses = []\n",
    "\n",
    "    # Compute losses for each sample in the batch\n",
    "    for i in range(n_samples):\n",
    "        # Extract parameters for the current sample\n",
    "        model_params = x[i, :]  # Take the i-th sample\n",
    "        model.set_parameters(model_params)  # Update model parameters\n",
    "\n",
    "        # Forward pass and loss computation\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        sample_losses.append(loss.item())\n",
    "\n",
    "    # Convert the list of losses to a tensor\n",
    "    return torch.tensor(sample_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ShallowNet(n_neurons)\n",
    "\n",
    "# HJ_MAD hyperparameters\n",
    "delta = 5e-11#4.87e-11          # Small delta -> Better Approximation of the Moreau Envelope Provided Large T or Close to Global Minimum\n",
    "t =   5e+7 #3.69            # Large T -> Better Exploration (Moreau Envelope closer to approximate quadratic)\n",
    "int_samples = int(500)  # Large N -> Better Expectations for Larger T -> Better Exploration (But more expensive)\n",
    "max_iters = int(2)   # Maximum number of iterations\n",
    "loss_tol = 9e-5         # Stopping criterion for the loss\n",
    "sat_tol = 1e-3          # Stopping criterion for the saturation\n",
    "beta=0.0                # Set beta to 0.0 to turn off \n",
    "momentum=0.0            # Set momentum to 0.0 to turn off\n",
    "distribution=\"Gaussian\" # Set sampling distribution \n",
    "\n",
    "# Use HJ_MAD to optimize the model parameters\n",
    "HJ_MAD_alg = HJ_MD_LS(delta=delta, t=t,distribution=distribution,momentum=momentum,\n",
    "                      int_samples=int_samples, max_iters=max_iters, f_tol=loss_tol,\n",
    "                      verbose=False, adaptive_time=False,adaptive_delta=True,\n",
    "                      line_search=False)\n",
    "\n",
    "\n",
    "# Training loop: Iterate through the epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Iterate through the dataset in batches\n",
    "    for inputs, targets in dataloader:\n",
    "        # Define f\n",
    "        def f(x):\n",
    "            return loss_function(x, model, inputs, targets)\n",
    "        \n",
    "        # Get the model parameters as a flattened tensor\n",
    "        parameters = list(model.parameters())\n",
    "        parameter_vector = torch.cat([param.flatten() for param in parameters])\n",
    "        parameter_vector = parameter_vector.unsqueeze(0) # shape (1, num_features)\n",
    "\n",
    "        # Compute Current Loss\n",
    "        loss_old = f(parameter_vector)  # Compute the loss for the current parameters\n",
    "\n",
    "        # Run HJ_MAD to optimize the model parameters\n",
    "        new_parameters, loss, loss_history, delta_hist, tk_hist, iterations = HJ_MAD_alg.run(f, parameter_vector)  # Run HJ_MAD to optimize the model parameters\n",
    "\n",
    "        HJ_MAD_alg.delta = delta_hist[-1]  # Update delta for the next iteration\n",
    "        HJ_MAD_alg.t = tk_hist[-1]\n",
    "\n",
    "        # Update the model parameters with the optimized values\n",
    "        model.set_parameters(new_parameters)\n",
    "\n",
    "        # Print the loss before and after optimization\n",
    "        print(f'Loss before: {loss_old.item():.4e} | Loss after: {loss.item():.4e} | Delta: {delta_hist[-1]}')\n",
    "        \n",
    "    # Print the loss every 100 epochs for monitoring\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Visualise the learned function\n",
    "plt.figure()\n",
    "plt.plot(x, y, label='True function')\n",
    "plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define f\n",
    "def f(x):\n",
    "    return loss_function(x, model, x_tensor, y_tensor)\n",
    "\n",
    "model = ShallowNet(n_neurons)\n",
    "\n",
    "# HJ_MAD hyperparameters\n",
    "delta = 5e-11#4.87e-11          # Small delta -> Better Approximation of the Moreau Envelope Provided Large T or Close to Global Minimum\n",
    "t =   5e+7 #3.69            # Large T -> Better Exploration (Moreau Envelope closer to approximate quadratic)\n",
    "int_samples = int(500)  # Large N -> Better Expectations for Larger T -> Better Exploration (But more expensive)\n",
    "max_iters = int(2000)   # Maximum number of iterations\n",
    "loss_tol = 9e-5         # Stopping criterion for the loss\n",
    "sat_tol = 1e-3          # Stopping criterion for the saturation\n",
    "beta=0.9                # Set beta to 0.0 to turn off \n",
    "momentum=0.0            # Set momentum to 0.0 to turn off\n",
    "distribution=\"Gaussian\" # Set sampling distribution \n",
    "\n",
    "# Use HJ_MAD to optimize the model parameters\n",
    "HJ_MAD_alg = HJ_MD_LS(delta=delta, t=t,distribution=distribution,momentum=momentum,\n",
    "                      int_samples=int_samples, max_iters=max_iters, f_tol=loss_tol,\n",
    "                      verbose=True, adaptive_time=False,adaptive_delta=True,\n",
    "                      line_search=False)\n",
    "\n",
    "# Get the model parameters as a flattened tensor\n",
    "parameters = list(model.parameters())\n",
    "parameter_vector = torch.cat([param.flatten() for param in parameters])\n",
    "parameter_vector = parameter_vector.unsqueeze(0) # shape (1, num_features)\n",
    "\n",
    "# Compute Current Loss\n",
    "loss_old = f(parameter_vector)  # Compute the loss for the current parameters\n",
    "\n",
    "# Run HJ_MAD to optimize the model parameters\n",
    "start_time = time.time()\n",
    "new_parameters, loss, loss_history, delta_hist, tk_hist, iterations = HJ_MAD_alg.run(f, parameter_vector)  # Run HJ_MAD to optimize the model parameters\n",
    "elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "\n",
    "func_evals = 2*iterations * (int_samples)\n",
    "print(\"\\n=== Optimization Results ===\")\n",
    "print(f\"Function Evaluations: {func_evals}\")\n",
    "print(f\"Iterations          : {iterations} (Max: {max_iters})\")\n",
    "print(f\"Elapsed Time        : {elapsed_time:.2f} seconds\")\n",
    "print(f\"Final Loss          : {f(new_parameters).item():.5e}\")\n",
    "print(\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(loss_history, label='Loss History', color='blue')\n",
    "plt.title('Loss History during Optimization')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the loss history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(delta_hist, label='delta History', color='blue')\n",
    "plt.title('Delta History during Optimization')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Delta')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(tk_hist, label='t History', color='blue')\n",
    "plt.title('T History during Optimization')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('t')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualise the learned function\n",
    "plt.figure()\n",
    "plt.plot(x, y, label='True function')\n",
    "plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# from hj_prox_ls import HJ_PROX_LS\n",
    "\n",
    "# # Hyperparameters\n",
    "# n_neurons = 50  # Number of neurons in the shallow network\n",
    "# learning_rate = 0.01\n",
    "# num_epochs = 1000\n",
    "# batch_size = 100\n",
    "\n",
    "# # Generate data\n",
    "# no_of_samples = 500\n",
    "# noise_level = 1e-3\n",
    "# x = np.linspace(0, 2 * np.pi, no_of_samples)\n",
    "# y = np.sin(x) + noise_level * np.random.randn(*x.shape)  # Add noise to the samples\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "# y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Create dataset and dataloader\n",
    "# dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Define the shallow neural network\n",
    "# class ShallowNet(nn.Module):\n",
    "#     def __init__(self, n_neurons):\n",
    "#         super(ShallowNet, self).__init__()\n",
    "#         self.linear = nn.Linear(1, n_neurons)\n",
    "#         self.coeffs = nn.Parameter(torch.randn(n_neurons, 1))\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear(x)      # Compute w_j * x + b_j\n",
    "#         x = self.relu(x)        # Apply ReLU activation\n",
    "#         x = x @ self.coeffs     # Compute sum of c_j * ReLU(...)\n",
    "#         return x\n",
    "\n",
    "#     # def set_parameters(self, new_parameters):\n",
    "#     #     \"\"\"\n",
    "#     #     Set model parameters from the flattened parameter vector.\n",
    "#     #     This method updates the model's parameters from a single flattened tensor.\n",
    "#     #     \"\"\"\n",
    "#     #     offset = 0\n",
    "#     #     for param in self.parameters():\n",
    "#     #         param_size = param.numel() \n",
    "#     #         new_param_values = new_parameters[offset:offset + param_size]\n",
    "#     #         new_param_values = new_param_values.view(param.shape)  \n",
    "#     #         param.data.copy_(new_param_values)  \n",
    "#     #         offset += param_size  \n",
    "\n",
    "#     def set_parameters(self, new_parameters):\n",
    "#         \"\"\"\n",
    "#         Set model parameters from the flattened parameter vector.\n",
    "#         This method updates the model's parameters from a single flattened tensor.\n",
    "\n",
    "#         Args:\n",
    "#             new_parameters: A flattened tensor containing all parameters.\n",
    "#         \"\"\"\n",
    "#         offset = 0\n",
    "#         for param in self.parameters():\n",
    "#             param_size = param.numel()  # Total number of elements in the parameter\n",
    "#             # Extract the relevant slice and reshape it to the parameter's original shape\n",
    "#             if len(new_parameters.shape) == 1:\n",
    "#                 new_param_values = new_parameters[offset:offset + param_size]\n",
    "#             else:\n",
    "#                 new_param_values = new_parameters[0, offset:offset + param_size]  # Access batch dimension\n",
    "#             new_param_values = new_param_values.view_as(param)  # Match the original shape\n",
    "#             param.data.copy_(new_param_values)  # Update parameter\n",
    "#             offset += param_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize model, loss function, and optimizer\n",
    "# model = ShallowNet(n_neurons)\n",
    "# criterion = nn.MSELoss()\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # HJ_MAD hyperparameters\n",
    "# delta0 = 1\n",
    "# t = 1\n",
    "# int_samples = int(100)\n",
    "# delta_dampener=0.8\n",
    "# beta=0.0\n",
    "# first_moment = None\n",
    "\n",
    "# # Use HJ_MAD to optimize the model parameters\n",
    "# HJ_PROX_LS_alg = HJ_PROX_LS(delta=delta0, t=t, int_samples=int_samples,delta_dampener=delta_dampener,beta=beta,verbose=True)\n",
    "\n",
    "# # Define the function f (e.g., a loss function based on the model's predictions)\n",
    "# def Objective_function(x, model, inputs, targets):\n",
    "#     \"\"\"\n",
    "#     Computes the loss based on the given model parameters.\n",
    "\n",
    "#     Args:\n",
    "#         x: Tensor of shape (n_samples, n_features) representing flattened parameters.\n",
    "#         model: The neural network model.\n",
    "#         inputs: Input batch.\n",
    "#         targets: Ground truth values.\n",
    "\n",
    "#     Returns:\n",
    "#         Tensor of shape (n_samples,) containing the loss for each sample.\n",
    "#     \"\"\"\n",
    "#     n_samples = x.shape[0]\n",
    "    \n",
    "#     # Initialize list to store losses\n",
    "#     sample_losses = []\n",
    "\n",
    "#     # Compute losses for each sample in the batch\n",
    "#     for i in range(n_samples):\n",
    "#         # Extract parameters for the current sample\n",
    "#         model_params = x[i, :]  # Take the i-th sample\n",
    "#         model.set_parameters(model_params)  # Update model parameters\n",
    "\n",
    "#         # Forward pass and loss computation\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         sample_losses.append(loss.item())\n",
    "\n",
    "#     # Convert the list of losses to a tensor\n",
    "#     return torch.tensor(sample_losses)\n",
    "\n",
    "\n",
    "# # Training loop: Iterate through the epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Iterate through the dataset in batches\n",
    "#     for inputs, targets in dataloader:\n",
    "#         # Define f\n",
    "#         def f(x):\n",
    "#             return Objective_function(x, model, inputs, targets)\n",
    "        \n",
    "#         # Get the model parameters as a flattened tensor\n",
    "#         parameters = list(model.parameters())\n",
    "#         parameter_vector = torch.cat([param.flatten() for param in parameters])\n",
    "#         parameter_vector = parameter_vector.unsqueeze(0) # shape (1, num_features)\n",
    "\n",
    "#         # Compute Current Loss\n",
    "#         loss_old = f(parameter_vector)  # Compute the loss for the current parameters\n",
    "\n",
    "#         # Run HJ_MAD to optimize the model parameters\n",
    "#         new_parameters, loss, first_moment = HJ_PROX_LS_alg.run(f, parameter_vector)  # Run HJ_MAD to optimize the model parameters\n",
    "\n",
    "#         # Update the model parameters with the optimized values\n",
    "#         model.set_parameters(new_parameters)\n",
    "        \n",
    "#     # Print the loss every 100 epochs for monitoring\n",
    "#     if (epoch + 1) % 100 == 0:\n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Delta: {HJ_PROX_LS_alg.delta}')\n",
    "\n",
    "# # Visualise the learned function\n",
    "# plt.figure()\n",
    "# plt.plot(x, y, label='True function')\n",
    "# plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "n_neurons = 50  # Number of neurons in the shallow network\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "# Generate data\n",
    "no_of_samples = 500\n",
    "noise_level = 1e-3\n",
    "x = np.linspace(0, 2 * np.pi, no_of_samples)\n",
    "y = np.sin(x) + noise_level * np.random.randn(*x.shape)  # Add noise to the samples\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the shallow neural network\n",
    "class ShallowNet(nn.Module):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(ShallowNet, self).__init__()\n",
    "        self.linear = nn.Linear(1, n_neurons)\n",
    "        self.coeffs = nn.Parameter(torch.randn(n_neurons, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)      # Compute w_j * x + b_j\n",
    "        x = self.relu(x)        # Apply ReLU activation\n",
    "        x = x @ self.coeffs     # Compute sum of c_j * ReLU(...)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ShallowNet(n_neurons)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Visualise the learned function\n",
    "plt.figure()\n",
    "plt.plot(x, y, label='True function')\n",
    "plt.plot(x, model(x_tensor).detach().numpy(), '--', label='Learned function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
